{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99777474-4e12-4c65-82c8-97313f21d22e",
   "metadata": {},
   "source": [
    "# User Story Breakdown Model Development Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c1c2e-7f10-4904-a098-578a42acae2f",
   "metadata": {},
   "source": [
    "**Project:** Automated User Story Task Breakdown with Dependency Analysis and Skill Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a31cb-ad8f-4132-931b-703d335acc89",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Project Overview](#1-project-overview)\n",
    "2. [Input Form Strategy](#2-input-form-strategy)\n",
    "3. [Data Structure](#3-data-structure)\n",
    "4. [Prompt Engineering Architecture & Implementation](#4-Prompt-engineering-architecture--implementation)\n",
    "5. [Fine-tuning](#5-fine-tuning-results)\n",
    "6. [Evaluation & Performance Analysis](#6-evaluation--performance-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236fe40-7495-41d1-ba7f-0cbc1c515ef7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "The goal of this project is to develop a model that can automatically break down user stories into tasks, identify dependencies between tasks, and extract the required skills needed for each task. \n",
    "\n",
    "### 1.2 Key Objectives\n",
    "- **Task Decomposition**: Break user stories into granular, actionable tasks\n",
    "- **Dependency Identification**: Detect relationships and dependencies between tasks\n",
    "- **Skill Extraction**: Identify required technical and non-technical skills\n",
    "- **Effort Estimation**: Provide realistic time estimates for each task\n",
    "\n",
    "### 1.3 Success Metrics\n",
    "- Task breakdown accuracy\n",
    "- Dependency detection precision\n",
    "- Skill extraction completeness\n",
    "- Processing time and cost efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990aa736-c3fb-4379-b39b-eb006daccf8a",
   "metadata": {},
   "source": [
    "## 2. Input Form Strategy\n",
    "\n",
    "### 2.1 Input Structure Design\n",
    "\n",
    "The input strategy focuses on handling various formats and quantities of user stories to optimize processing efficiency and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8324ae-b601-48dd-9ce8-fea30155783a",
   "metadata": {},
   "source": [
    "#### 2.1.1 Strategy 1: Individual Processing\n",
    "\n",
    "**Approach**: Process each user story separately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3193a7e-e207-447d-a3b0-4bce291e7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "scripts_dir = os.path.join(os.getcwd(), 'scripts')\n",
    "sys.path.append(scripts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534f6a8c-e0b0-4d4e-897d-338315fec076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168b2ae-7d4c-4da1-add2-28dc54a9aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_USER_STORIES = [\n",
    "    \"As a user, I want to create an account so that I can access personalized features\",\n",
    "    \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
    "    \"As a customer, I want to search for products so that I can find what I need quickly\",\n",
    "    \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\",\n",
    "    \"As a user, I want to reset my password so that I can regain access to my account\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d173c5d1-d200-43ec-ba0b-97d7bd1e37d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Enhanced Individual Processor for User Stories\n",
      "============================================================\n",
      "ðŸ¢ PARTY A: INDIVIDUAL PROCESSING WITH FULL DETAILS\n",
      "============================================================\n",
      "Philosophy: 'One story at a time, done right'\n",
      "\n",
      "Processing 5 sample user stories...\n",
      "ðŸ¢ TRADITIONALISTS: Processing 5 stories individually...\n",
      "\n",
      "ðŸš€ Processing 5 user stories...\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to create an account so that I can access ...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 341, Output: 54, Total: 395\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 379, Output: 49, Total: 428\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 398, Output: 20, Total: 418\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 23, Total: 422\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 19, Total: 418\n",
      "[REQUIRED_SKILLS] Tokens - Input: 398, Output: 21, Total: 419\n",
      "[REQUIRED_SKILLS] Tokens - Input: 397, Output: 25, Total: 422\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 452, Output: 228, Total: 680\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to view analytics dashboard so that I ca...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 341, Output: 60, Total: 401\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 385, Output: 49, Total: 434\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 22, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 27, Total: 426\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 28, Total: 427\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 21, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 24, Total: 424\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 458, Output: 98, Total: 556\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As a customer, I want to search for products so that I can f...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 343, Output: 72, Total: 415\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 399, Output: 49, Total: 448\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 401, Output: 24, Total: 425\n",
      "[REQUIRED_SKILLS] Tokens - Input: 401, Output: 22, Total: 423\n",
      "[REQUIRED_SKILLS] Tokens - Input: 402, Output: 20, Total: 422\n",
      "[REQUIRED_SKILLS] Tokens - Input: 402, Output: 24, Total: 426\n",
      "[REQUIRED_SKILLS] Tokens - Input: 403, Output: 31, Total: 434\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 472, Output: 94, Total: 566\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 342, Output: 60, Total: 402\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 386, Output: 49, Total: 435\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 25, Total: 424\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 20, Total: 420\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 25, Total: 425\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 21, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 398, Output: 13, Total: 411\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 459, Output: 114, Total: 573\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to reset my password so that I can regain ...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 343, Output: 74, Total: 417\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 401, Output: 49, Total: 450\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 404, Output: 20, Total: 424\n",
      "[REQUIRED_SKILLS] Tokens - Input: 402, Output: 23, Total: 425\n",
      "[REQUIRED_SKILLS] Tokens - Input: 401, Output: 23, Total: 424\n",
      "[REQUIRED_SKILLS] Tokens - Input: 403, Output: 23, Total: 426\n",
      "[REQUIRED_SKILLS] Tokens - Input: 401, Output: 21, Total: 422\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 474, Output: 214, Total: 688\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "âœ… Completed processing 5 stories\n",
      "âœ… SUCCESS: Processed 5 stories with 21 API calls\n",
      "\n",
      "================================================================================\n",
      "ðŸ¢ PARTY A: TRADITIONALISTS - COMPLETE DETAILED RESULTS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PERFORMANCE SUMMARY:\n",
      "--------------------------------------------------\n",
      "â±ï¸  Execution Time: 124.06 seconds\n",
      "ðŸ”Œ API Calls: 21\n",
      "ðŸ“‹ Total Tasks Generated: 25\n",
      "ðŸ”— Dependencies Found: 24\n",
      "ðŸŽ¯ Skills Identified: 89\n",
      "âš¡ Efficiency: 1.19 tasks per API call\n",
      "\n",
      "ðŸ’° TOKEN USAGE:\n",
      "--------------------------------------------------\n",
      "ðŸŽ« Total Tokens: 17858\n",
      "ðŸ’µ Estimated Cost: $0.197360\n",
      "   â€¢ Task Extraction: 2030 tokens (11.4%)\n",
      "   â€¢ Story Point Estimation: 2195 tokens (12.3%)\n",
      "   â€¢ Required Skills: 10570 tokens (59.2%)\n",
      "   â€¢ Dependency Analysis: 3063 tokens (17.2%)\n",
      "\n",
      "ðŸ“‹ ALL 25 DECOMPOSED TASKS:\n",
      "--------------------------------------------------\n",
      " 1. Design user registration form interface\n",
      "    â””â”€ From: As a user, I want to create an account so that I can access ...\n",
      "\n",
      " 2. Implement email validation and verification system\n",
      "    â””â”€ From: As a user, I want to create an account so that I can access ...\n",
      "\n",
      " 3. Create password strength requirements and validation\n",
      "    â””â”€ From: As a user, I want to create an account so that I can access ...\n",
      "\n",
      " 4. Build user profile creation workflow\n",
      "    â””â”€ From: As a user, I want to create an account so that I can access ...\n",
      "\n",
      " 5. Add account activation process\n",
      "    â””â”€ From: As a user, I want to create an account so that I can access ...\n",
      "\n",
      " 6. Design analytics dashboard layout and components\n",
      "    â””â”€ From: As an admin, I want to view analytics dashboard so that I ca...\n",
      "\n",
      " 7. Implement data collection and aggregation system\n",
      "    â””â”€ From: As an admin, I want to view analytics dashboard so that I ca...\n",
      "\n",
      " 8. Create real-time performance metrics display\n",
      "    â””â”€ From: As an admin, I want to view analytics dashboard so that I ca...\n",
      "\n",
      " 9. Add filtering and date range selection features\n",
      "    â””â”€ From: As an admin, I want to view analytics dashboard so that I ca...\n",
      "\n",
      "10. Develop dashboard data visualization and charting\n",
      "    â””â”€ From: As an admin, I want to view analytics dashboard so that I ca...\n",
      "\n",
      "11. Design search interface with filters and autocomplete functionality\n",
      "    â””â”€ From: As a customer, I want to search for products so that I can f...\n",
      "\n",
      "12. Implement search algorithm and indexing for product catalog\n",
      "    â””â”€ From: As a customer, I want to search for products so that I can f...\n",
      "\n",
      "13. Create search results display with pagination and sorting options\n",
      "    â””â”€ From: As a customer, I want to search for products so that I can f...\n",
      "\n",
      "14. Add search history and suggestions feature with user profiling\n",
      "    â””â”€ From: As a customer, I want to search for products so that I can f...\n",
      "\n",
      "15. Integrate search with product categorization and tagging system\n",
      "    â””â”€ From: As a customer, I want to search for products so that I can f...\n",
      "\n",
      "16. Configure automated build and testing pipeline\n",
      "    â””â”€ From: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "\n",
      "17. Set up deployment staging and production environments\n",
      "    â””â”€ From: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "\n",
      "18. Implement code quality checks and security scanning\n",
      "    â””â”€ From: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "\n",
      "19. Integrate version control system with pipeline\n",
      "    â””â”€ From: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "\n",
      "20. Define deployment triggers and scheduling\n",
      "    â””â”€ From: As a developer, I want to set up CI/CD pipeline so that depl...\n",
      "\n",
      "21. Design password reset form interface with email input and submit button\n",
      "    â””â”€ From: As a user, I want to reset my password so that I can regain ...\n",
      "\n",
      "22. Implement email validation and verification system for password reset\n",
      "    â””â”€ From: As a user, I want to reset my password so that I can regain ...\n",
      "\n",
      "23. Create password reset token generation and expiration system\n",
      "    â””â”€ From: As a user, I want to reset my password so that I can regain ...\n",
      "\n",
      "24. Build password reset workflow with new password input and confirmation\n",
      "    â””â”€ From: As a user, I want to reset my password so that I can regain ...\n",
      "\n",
      "25. Add password update functionality to user profile database\n",
      "    â””â”€ From: As a user, I want to reset my password so that I can regain ...\n",
      "\n",
      "\n",
      "ðŸ”— TASK DEPENDENCIES:\n",
      "--------------------------------------------------\n",
      "Found 20 tasks with dependencies:\n",
      "\n",
      "1. ðŸ“‹ TASK: Implement email validation and verification system\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Design user registration form interface\n",
      "\n",
      "2. ðŸ“‹ TASK: Create password strength requirements and validation\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Design user registration form interface\n",
      "\n",
      "3. ðŸ“‹ TASK: Build user profile creation workflow\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement email validation and verification system\n",
      "      2. Create password strength requirements and validation\n",
      "\n",
      "4. ðŸ“‹ TASK: Add account activation process\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Build user profile creation workflow\n",
      "\n",
      "5. ðŸ“‹ TASK: Implement data collection and aggregation system\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Design analytics dashboard layout and components\n",
      "\n",
      "6. ðŸ“‹ TASK: Create real-time performance metrics display\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement data collection and aggregation system\n",
      "\n",
      "7. ðŸ“‹ TASK: Add filtering and date range selection features\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Create real-time performance metrics display\n",
      "\n",
      "8. ðŸ“‹ TASK: Develop dashboard data visualization and charting\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement data collection and aggregation system\n",
      "\n",
      "9. ðŸ“‹ TASK: Implement search algorithm and indexing for product catalog\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Design search interface with filters and autocomplete functionality\n",
      "\n",
      "10. ðŸ“‹ TASK: Create search results display with pagination and sorting options\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement search algorithm and indexing for product catalog\n",
      "\n",
      "11. ðŸ“‹ TASK: Add search history and suggestions feature with user profiling\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement search algorithm and indexing for product catalog\n",
      "\n",
      "12. ðŸ“‹ TASK: Integrate search with product categorization and tagging system\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement search algorithm and indexing for product catalog\n",
      "      2. Create search results display with pagination and sorting options\n",
      "\n",
      "13. ðŸ“‹ TASK: Set up deployment staging and production environments\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Configure automated build and testing pipeline\n",
      "\n",
      "14. ðŸ“‹ TASK: Implement code quality checks and security scanning\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Configure automated build and testing pipeline\n",
      "\n",
      "15. ðŸ“‹ TASK: Integrate version control system with pipeline\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Configure automated build and testing pipeline\n",
      "\n",
      "16. ðŸ“‹ TASK: Define deployment triggers and scheduling\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Set up deployment staging and production environments\n",
      "      2. Implement code quality checks and security scanning\n",
      "      3. Integrate version control system with pipeline\n",
      "\n",
      "17. ðŸ“‹ TASK: Implement email validation and verification system for password reset\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Design password reset form interface with email input and submit button\n",
      "\n",
      "18. ðŸ“‹ TASK: Create password reset token generation and expiration system\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Implement email validation and verification system for password reset\n",
      "\n",
      "19. ðŸ“‹ TASK: Build password reset workflow with new password input and confirmation\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Create password reset token generation and expiration system\n",
      "\n",
      "20. ðŸ“‹ TASK: Add password update functionality to user profile database\n",
      "   â¬‡ï¸  DEPENDS ON:\n",
      "      1. Build password reset workflow with new password input and confirmation\n",
      "\n",
      "\n",
      "ðŸŽ¯ SKILLS REQUIRED FOR EACH TASK:\n",
      "--------------------------------------------------\n",
      " 1. ðŸ“‹ Design user registration form interface\n",
      "     ðŸ› ï¸  Skills: ui_design, form_design, frontend\n",
      "\n",
      " 2. ðŸ“‹ Implement email validation and verification system\n",
      "     ðŸ› ï¸  Skills: backend, email_systems, validation, security_patterns\n",
      "\n",
      " 3. ðŸ“‹ Create password strength requirements and validation\n",
      "     ðŸ› ï¸  Skills: security, password_management, validation\n",
      "\n",
      " 4. ðŸ“‹ Build user profile creation workflow\n",
      "     ðŸ› ï¸  Skills: workflow_design, user_management, backend\n",
      "\n",
      " 5. ðŸ“‹ Add account activation process\n",
      "     ðŸ› ï¸  Skills: backend, email_systems, validation, security, workflow_design\n",
      "\n",
      " 6. ðŸ“‹ Design analytics dashboard layout and components\n",
      "     ðŸ› ï¸  Skills: ui_design, dashboard_design, data_visualization\n",
      "\n",
      " 7. ðŸ“‹ Implement data collection and aggregation system\n",
      "     ðŸ› ï¸  Skills: data_engineering, data_aggregation, data_processing, distributed_systems\n",
      "\n",
      " 8. ðŸ“‹ Create real-time performance metrics display\n",
      "     ðŸ› ï¸  Skills: frontend, real_time_systems, data_visualization, charting_libraries\n",
      "\n",
      " 9. ðŸ“‹ Add filtering and date range selection features\n",
      "     ðŸ› ï¸  Skills: frontend, filtering_systems, date_handling\n",
      "\n",
      "10. ðŸ“‹ Develop dashboard data visualization and charting\n",
      "     ðŸ› ï¸  Skills: data_visualization, dashboard_design, charting_libraries\n",
      "\n",
      "11. ðŸ“‹ Design search interface with filters and autocomplete functionality\n",
      "     ðŸ› ï¸  Skills: ui_design, search_interface, autocomplete, filtering_systems\n",
      "\n",
      "12. ðŸ“‹ Implement search algorithm and indexing for product catalog\n",
      "     ðŸ› ï¸  Skills: search_algorithms, indexing, database_optimization\n",
      "\n",
      "13. ðŸ“‹ Create search results display with pagination and sorting options\n",
      "     ðŸ› ï¸  Skills: frontend, pagination, sorting_algorithms\n",
      "\n",
      "14. ðŸ“‹ Add search history and suggestions feature with user profiling\n",
      "     ðŸ› ï¸  Skills: frontend, search_interface, user_profiling, data_storage\n",
      "\n",
      "15. ðŸ“‹ Integrate search with product categorization and tagging system\n",
      "     ðŸ› ï¸  Skills: backend, search_algorithms, data_modeling, categorization_systems, tagging_systems\n",
      "\n",
      "16. ðŸ“‹ Configure automated build and testing pipeline\n",
      "     ðŸ› ï¸  Skills: devops, ci_cd, automated_testing, build_systems\n",
      "\n",
      "17. ðŸ“‹ Set up deployment staging and production environments\n",
      "     ðŸ› ï¸  Skills: devops, infrastructure, environment_configuration\n",
      "\n",
      "18. ðŸ“‹ Implement code quality checks and security scanning\n",
      "     ðŸ› ï¸  Skills: devops, code_quality, security_scanning, static_analysis\n",
      "\n",
      "19. ðŸ“‹ Integrate version control system with pipeline\n",
      "     ðŸ› ï¸  Skills: version_control, pipeline_integration, devops\n",
      "\n",
      "20. ðŸ“‹ Define deployment triggers and scheduling\n",
      "     ðŸ› ï¸  Skills: devops, ci_cd, automation_scripting\n",
      "\n",
      "21. ðŸ“‹ Design password reset form interface with email input and submit button\n",
      "     ðŸ› ï¸  Skills: ui_design, form_design, frontend\n",
      "\n",
      "22. ðŸ“‹ Implement email validation and verification system for password reset\n",
      "     ðŸ› ï¸  Skills: backend, email_systems, validation, security_patterns\n",
      "\n",
      "23. ðŸ“‹ Create password reset token generation and expiration system\n",
      "     ðŸ› ï¸  Skills: backend, security, token_management, expiration_policies\n",
      "\n",
      "24. ðŸ“‹ Build password reset workflow with new password input and confirmation\n",
      "     ðŸ› ï¸  Skills: backend, workflow_design, password_management, security_patterns\n",
      "\n",
      "25. ðŸ“‹ Add password update functionality to user profile database\n",
      "     ðŸ› ï¸  Skills: backend, database_design, password_hashing\n",
      "\n",
      "\n",
      "ðŸ“ˆ DETAILED ANALYSIS:\n",
      "--------------------------------------------------\n",
      "ðŸ“Š Tasks Generated per Story:\n",
      "   â€¢ As a user, I want to create an account s...: 5 tasks\n",
      "   â€¢ As an admin, I want to view analytics da...: 5 tasks\n",
      "   â€¢ As a customer, I want to search for prod...: 5 tasks\n",
      "   â€¢ As a developer, I want to set up CI/CD p...: 5 tasks\n",
      "   â€¢ As a user, I want to reset my password s...: 5 tasks\n",
      "\n",
      "ðŸ”— Dependency Statistics:\n",
      "   â€¢ Tasks with dependencies: 20\n",
      "   â€¢ Maximum dependencies per task: 3\n",
      "   â€¢ Average dependencies per task: 1.0\n",
      "\n",
      "ðŸŽ¯ Skills Analysis:\n",
      "   â€¢ Unique skills identified: 49\n",
      "   â€¢ Most common skills: security_scanning, frontend, database_design, email_systems, ui_design\n",
      "\n",
      "================================================================================\n",
      "ðŸ INDIVIDUAL PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "âœ… Processing completed successfully!\n",
      "\n",
      "ðŸ“ˆ FINAL SUMMARY:\n",
      "   â€¢ Total User Stories: 5\n",
      "   â€¢ Total Tasks Generated: 25\n",
      "   â€¢ Total Dependencies: 24\n",
      "   â€¢ Total Skills Identified: 89\n",
      "   â€¢ Estimated API Cost: $0.197360\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "# Try to import tiktoken for token counting\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    print(\"Warning: tiktoken not available, using approximate token counting\")\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = None\n",
    "        if TIKTOKEN_AVAILABLE:\n",
    "            try:\n",
    "                self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not initialize tiktoken: {e}\")\n",
    "                self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            if self.tokenizer:\n",
    "                return len(self.tokenizer.encode(text))\n",
    "            else:\n",
    "                return len(text) // 4\n",
    "        except Exception as e:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks from user story\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task extraction specialist. Break down user stories into 2-7 specific, actionable tasks.\n",
    "\n",
    "Requirements:\n",
    "- Minimum 2 tasks, maximum 7 tasks\n",
    "- Each task should be concise (10-30 words)\n",
    "- Tasks must be clear and actionable\n",
    "- Focus on essential steps only\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks:\n",
    "1. Design user registration form interface\n",
    "2. Implement email validation and verification system\n",
    "3. Create password strength requirements and validation\n",
    "4. Build user profile creation workflow\n",
    "5. Add account activation process\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks:\n",
    "1. Design analytics dashboard layout and components\n",
    "2. Implement data collection and aggregation system\n",
    "3. Create real-time performance metrics display\n",
    "4. Add filtering and date range selection features\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Tasks:\n",
    "1. Design search interface with filters\n",
    "2. Implement search algorithm and indexing\n",
    "3. Create search results display with pagination\n",
    "4. Add search history and suggestions feature\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Tasks:\n",
    "1. Configure automated build and testing pipeline\n",
    "2. Set up deployment staging and production environments\n",
    "3. Implement code quality checks and security scanning\n",
    "\n",
    "Now break down this user story:\n",
    "User Story: {user_story}\n",
    "\n",
    "Return ONLY a numbered list of tasks:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "            \n",
    "            tasks = self._parse_tasks(output_text)\n",
    "            print(f\"âœ“ Extracted {len(tasks)} tasks\")\n",
    "            return tasks\n",
    "        except Exception as e:\n",
    "            print(f\"Error in task extraction: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            skip_phrases = ['user story:', 'tasks:', 'here are', 'the following', 'broken down', 'example format:', '**']\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in skip_phrases):\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "            clean_task = clean_task.strip()\n",
    "            \n",
    "            if clean_task and len(clean_task) > 10:\n",
    "                tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2: Estimate story points for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Consider complexity, time, risk, and uncertainty.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks and Estimates:\n",
    "1. Design user registration form interface (3 points)\n",
    "2. Implement email validation and verification system (5 points)\n",
    "3. Create password strength requirements and validation (3 points)\n",
    "4. Build user profile creation workflow (5 points)\n",
    "5. Add account activation process (3 points)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks and Estimates:\n",
    "1. Design analytics dashboard layout and components (5 points)\n",
    "2. Implement data collection and aggregation system (8 points)\n",
    "3. Create real-time performance metrics display (5 points)\n",
    "4. Add filtering and date range selection features (3 points)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Tasks and Estimates:\n",
    "1. Design search interface with filters (3 points)\n",
    "2. Implement search algorithm and indexing (8 points)\n",
    "3. Create search results display with pagination (3 points)\n",
    "4. Add search history and suggestions feature (5 points)\n",
    "\n",
    "Now estimate points for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "Task 3: Z points\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "            \n",
    "            points = self._parse_story_points(output_text, tasks)\n",
    "            print(f\"âœ“ Estimated story points for {len(points)} tasks\")\n",
    "            total_points = sum(points.values())\n",
    "            return {\n",
    "                'total_story_points': total_points,\n",
    "                'task_points': points,\n",
    "                'estimated_sum': total_points\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in story point estimation: {e}\")\n",
    "            return {'total_story_points': 0, 'task_points': {}, 'estimated_sum': 0}\n",
    "    \n",
    "    def _parse_story_points(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    # Extract task number and points\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    # Find closest valid point\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default points\n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3  # Default moderate complexity\n",
    "        \n",
    "        return points\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 2b: Identify required skills for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task using few-shot examples\"\"\"\n",
    "        user_story = \"General task completion\"\n",
    "        tasks = [task]\n",
    "        tasks_str = \"1. \" + task\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify specific skills required for each task.\n",
    "\n",
    "Consider programming languages, frameworks, domains, and specializations.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, form_design, frontend\n",
    "Task 2: backend, email_systems, validation, security\n",
    "Task 3: frontend, validation, security_patterns\n",
    "Task 4: backend, workflow_design, user_management\n",
    "Task 5: backend, email_systems, activation_flows\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, dashboard_design, data_visualization\n",
    "Task 2: backend, database_design, data_processing, analytics\n",
    "Task 3: frontend, real_time_systems, charting_libraries\n",
    "Task 4: frontend, filtering_systems, date_handling\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, search_interface, filtering_systems\n",
    "Task 2: backend, search_algorithms, database_optimization, indexing\n",
    "Task 3: frontend, pagination, results_display\n",
    "Task 4: backend, data_storage, recommendation_systems\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Task Skills:\n",
    "Task 1: devops, ci_cd, automated_testing, build_systems\n",
    "Task 2: devops, infrastructure, deployment_automation\n",
    "Task 3: devops, security_scanning, code_quality, static_analysis\n",
    "\n",
    "Now identify skills for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\"\"\"\n",
    "    \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "        \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "        \n",
    "            # Parse for single task\n",
    "            skills_map = self._parse_skills(output_text, tasks)\n",
    "            return skills_map.get(task, [\"general_development\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in skills mapping: {e}\")\n",
    "            return [\"general_development\"]\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "    \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "    \n",
    "        print(f\"âœ“ Identified skills for {len(skills_map)} tasks\")\n",
    "        return skills_map\n",
    "        \n",
    "    def _parse_skills(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Parse skills\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 3: Analyze dependencies between tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        tasks_with_points = []\n",
    "        for i, task in enumerate(tasks):\n",
    "            points = story_points.get(task, 3)\n",
    "            tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "        \n",
    "        tasks_str = \"\\n\".join(tasks_with_points)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "Consider logical workflow order and technical dependencies.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Dependencies:\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "Task 4 depends on Task 2 (rework_effort: 3)\n",
    "Task 5 depends on Task 2 (rework_effort: 2)\n",
    "Task 5 depends on Task 4 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 2 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Dependencies:\n",
    "Task 2 depends on Task 1 (rework_effort: 2)\n",
    "Task 3 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "rework_effort scale:\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed  \n",
    "- 3: High rework effort required\n",
    "\n",
    "Now analyze dependencies for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies. Don't create artificial ones.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "            \n",
    "            dependencies = self._parse_dependencies(output_text, tasks)\n",
    "            print(f\"âœ“ Analyzed dependencies for {len(dependencies)} tasks\")\n",
    "            return dependencies\n",
    "        except Exception as e:\n",
    "            print(f\"Error in dependency analysis: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers\n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            # Validate rework_effort\n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2  # Default\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "\n",
    "class FormatValidatorAgent:\n",
    "    \"\"\"Step 4: Validate and format final output\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def validate_and_format(self, user_story: str, tasks: List[str], \n",
    "                                 story_points: Dict[str, int], skills: Dict[str, List[str]], \n",
    "                                 dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, any]:\n",
    "        \n",
    "        # Generate task IDs\n",
    "        task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "        \n",
    "        # Build final structure\n",
    "        formatted_tasks = []\n",
    "        total_story_points = 0\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = task_ids[i]\n",
    "            task_points = story_points.get(task, 3)\n",
    "            task_skills = skills.get(task, [\"general_development\"])\n",
    "            task_dependencies = dependencies.get(task, [])\n",
    "            \n",
    "            # Convert dependencies to use task IDs\n",
    "            formatted_dependencies = []\n",
    "            for dep in task_dependencies:\n",
    "                dep_task = dep[\"task_id\"]\n",
    "                if dep_task in tasks:\n",
    "                    dep_index = tasks.index(dep_task)\n",
    "                    dep_task_id = task_ids[dep_index]\n",
    "                    formatted_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            formatted_tasks.append({\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": task_points,\n",
    "                \"depends_on\": formatted_dependencies,\n",
    "                \"required_skills\": task_skills\n",
    "            })\n",
    "            \n",
    "            total_story_points += task_points\n",
    "        \n",
    "        result = {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"story_points\": total_story_points,\n",
    "                \"tasks\": formatted_tasks\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            json.dumps(result)\n",
    "            print(\"âœ“ Format validation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Format validation failed: {e}\")\n",
    "            # Apply fixes if needed\n",
    "            result = self._fix_json_issues(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        # Extract key words from user story to create meaningful prefix\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        # Take first few significant words and create acronym\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        # Ensure prefix is exactly 3 characters\n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        \n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _fix_json_issues(self, result: Dict[str, any]) -> Dict[str, any]:\n",
    "        # Implement basic JSON fixes\n",
    "        try:\n",
    "            # Convert any non-serializable items\n",
    "            json_str = json.dumps(result, default=str)\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: JSON fix failed: {e}\")\n",
    "            return result\n",
    "\n",
    "class UserStoryPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_extractor = TaskExtractorAgent()\n",
    "        self.story_point_estimator = StoryPointEstimatorAgent()\n",
    "        self.skills_agent = RequiredSkillsAgent()\n",
    "        self.dependency_agent = DependencyAgent()\n",
    "        self.format_validator = FormatValidatorAgent()\n",
    "    \n",
    "    async def process_story(self, user_story: str) -> Dict[str, any]:\n",
    "        try:\n",
    "            print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "            \n",
    "            # Step 1: Extract tasks\n",
    "            print(\"  Step 1: Extracting tasks...\")\n",
    "            tasks = await self.task_extractor.decompose(user_story)\n",
    "            \n",
    "            if not tasks:\n",
    "                raise ValueError(\"No tasks extracted from user story\")\n",
    "            \n",
    "            # Step 2: Parallel processing of story points and skills\n",
    "            print(\"  Step 2: Estimating story points and identifying skills...\")\n",
    "            story_points_results, skills = await asyncio.gather(\n",
    "                self.story_point_estimator.estimate_story_points(user_story, tasks),\n",
    "                self.skills_agent.identify_skills(user_story, tasks)\n",
    "            )\n",
    "            story_points = story_points_results['task_points']\n",
    "            \n",
    "            # Step 3: Analyze dependencies\n",
    "            print(\"  Step 3: Analyzing dependencies...\")\n",
    "            dependencies = await self.dependency_agent.analyze_dependencies(\n",
    "                user_story, tasks, story_points\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format and validate\n",
    "            print(\"  Step 4: Formatting and validating...\")\n",
    "            result = await self.format_validator.validate_and_format(\n",
    "                user_story, tasks, story_points, skills, dependencies\n",
    "            )\n",
    "            \n",
    "            print(\"  âœ… Story processing complete!\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing story: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"error\": str(e),\n",
    "                \"output\": None\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_stories(self, user_stories: List[str]) -> List[Dict[str, any]]:\n",
    "        print(f\"\\nðŸš€ Processing {len(user_stories)} user stories...\")\n",
    "        \n",
    "        # Reset token tracker\n",
    "        global token_tracker\n",
    "        token_tracker = TokenTracker()\n",
    "        \n",
    "        # Process all stories\n",
    "        results = []\n",
    "        for story in user_stories:\n",
    "            result = await self.process_story(story)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Completed processing {len(results)} stories\")\n",
    "        return results\n",
    "\n",
    "# Create wrapper function for backward compatibility\n",
    "async def process_multiple_user_stories(user_stories: List[str]) -> Dict[str, any]:\n",
    "    \"\"\"Process multiple user stories and return consolidated results\"\"\"\n",
    "    pipeline = UserStoryPipeline()\n",
    "    individual_results = await pipeline.process_multiple_stories(user_stories)\n",
    "    \n",
    "    # Consolidate results into the format expected by EnhancedIndividualProcessor\n",
    "    all_tasks = []\n",
    "    task_origins = {}\n",
    "    dependencies = {}\n",
    "    required_skills = {}\n",
    "    \n",
    "    for result in individual_results:\n",
    "        if \"error\" not in result and result[\"output\"]:\n",
    "            story = result[\"input\"]\n",
    "            for task_data in result[\"output\"][\"tasks\"]:\n",
    "                task_desc = task_data[\"description\"]\n",
    "                all_tasks.append(task_desc)\n",
    "                \n",
    "                # Track origin\n",
    "                if task_desc not in task_origins:\n",
    "                    task_origins[task_desc] = []\n",
    "                task_origins[task_desc].append(story)\n",
    "                \n",
    "                # Dependencies - convert back to task descriptions\n",
    "                task_deps = []\n",
    "                for dep in task_data.get(\"depends_on\", []):\n",
    "                    # Find the task description for this dependency ID\n",
    "                    for other_result in individual_results:\n",
    "                        if \"error\" not in other_result and other_result[\"output\"]:\n",
    "                            for other_task in other_result[\"output\"][\"tasks\"]:\n",
    "                                if other_task[\"id\"] == dep[\"task_id\"]:\n",
    "                                    task_deps.append(other_task[\"description\"])\n",
    "                                    break\n",
    "                \n",
    "                dependencies[task_desc] = task_deps\n",
    "                required_skills[task_desc] = task_data.get(\"required_skills\", [])\n",
    "    \n",
    "    return {\n",
    "        \"tasks\": all_tasks,\n",
    "        \"task_origins\": task_origins,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"required_skills\": required_skills\n",
    "    }\n",
    "\n",
    "# Enhanced Individual Processor Classes (adapted from original)\n",
    "class TaskDecomposerAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class TaskConsolidatorAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class DependencyAnalyzerAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SkillMapperAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class EnhancedIndividualProcessor:\n",
    "    \"\"\"Enhanced version that captures and displays detailed results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.decomposer = TaskDecomposerAgent()\n",
    "        self.consolidator = TaskConsolidatorAgent()\n",
    "        self.dependency_analyzer = DependencyAnalyzerAgent()\n",
    "        self.skill_mapper = SkillMapperAgent()\n",
    "    \n",
    "    async def process_stories_with_details(self, user_stories: List[str]):\n",
    "        \"\"\"Process stories and immediately display detailed results\"\"\"\n",
    "        print(f\"ðŸ¢ TRADITIONALISTS: Processing {len(user_stories)} stories individually...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = await process_multiple_user_stories(user_stories)\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                raise Exception(result[\"error\"])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            api_calls = len(user_stories) * 4 + 1  # Approximate API calls\n",
    "            total_tasks = sum(len(tasks) for tasks in result.get(\"task_origins\", {}).values())\n",
    "            duplicate_reduction = 1 - (len(result[\"tasks\"]) / total_tasks) if total_tasks > 0 else 0\n",
    "            dependencies_found = sum(len(deps) for deps in result[\"dependencies\"].values())\n",
    "            total_skills = sum(len(skills) for skills in result[\"required_skills\"].values())\n",
    "            \n",
    "            print(f\"âœ… SUCCESS: Processed {len(user_stories)} stories with {api_calls} API calls\")\n",
    "            \n",
    "            # IMMEDIATELY DISPLAY DETAILED RESULTS\n",
    "            self.display_detailed_results(result, time.time() - start_time, api_calls)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def display_detailed_results(self, result, execution_time, api_calls):\n",
    "        \"\"\"Display comprehensive detailed results immediately\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ðŸ¢ PARTY A: TRADITIONALISTS - COMPLETE DETAILED RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # SUMMARY METRICS\n",
    "        print(\"\\nðŸ“Š PERFORMANCE SUMMARY:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"â±ï¸  Execution Time: {execution_time:.2f} seconds\")\n",
    "        print(f\"ðŸ”Œ API Calls: {api_calls}\")\n",
    "        print(f\"ðŸ“‹ Total Tasks Generated: {len(result['tasks'])}\")\n",
    "        print(f\"ðŸ”— Dependencies Found: {sum(len(deps) for deps in result['dependencies'].values())}\")\n",
    "        print(f\"ðŸŽ¯ Skills Identified: {sum(len(skills) for skills in result['required_skills'].values())}\")\n",
    "        print(f\"âš¡ Efficiency: {len(result['tasks']) / api_calls:.2f} tasks per API call\")\n",
    "        \n",
    "        # TOKEN USAGE SUMMARY\n",
    "        print(f\"\\nðŸ’° TOKEN USAGE:\")\n",
    "        print(\"-\" * 50)\n",
    "        summary = token_tracker.get_summary()\n",
    "        if summary:\n",
    "            breakdown = summary.get(\"breakdown\", {})\n",
    "            total_tokens = breakdown.get('total_consumed', 0)\n",
    "            cost_data = summary.get(\"cost_estimate\", {})\n",
    "            \n",
    "            print(f\"ðŸŽ« Total Tokens: {total_tokens}\")\n",
    "            if cost_data:\n",
    "                print(f\"ðŸ’µ Estimated Cost: ${cost_data['total_cost']:.6f}\")\n",
    "            \n",
    "            # Category breakdown\n",
    "            categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "            for cat in categories:\n",
    "                cat_data = breakdown.get(cat, {})\n",
    "                if cat_data and cat_data.get('total', 0) > 0:\n",
    "                    percentage = (cat_data['total'] / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "                    print(f\"   â€¢ {cat.replace('_', ' ').title()}: {cat_data['total']} tokens ({percentage:.1f}%)\")\n",
    "        \n",
    "        # DETAILED TASKS\n",
    "        print(f\"\\nðŸ“‹ ALL {len(result['tasks'])} DECOMPOSED TASKS:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, task in enumerate(result['tasks'], 1):\n",
    "            print(f\"{i:2d}. {task}\")\n",
    "            \n",
    "            # Show which story(ies) generated this task\n",
    "            origins = result['task_origins'].get(task, [])\n",
    "            if origins:\n",
    "                if len(origins) == 1:\n",
    "                    print(f\"    â””â”€ From: {origins[0][:60]}...\")\n",
    "                else:\n",
    "                    print(f\"    â””â”€ From {len(origins)} stories (duplicate found):\")\n",
    "                    for origin in origins:\n",
    "                        print(f\"       â€¢ {origin[:55]}...\")\n",
    "            print()\n",
    "        \n",
    "        # DEPENDENCIES\n",
    "        print(f\"\\nðŸ”— TASK DEPENDENCIES:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        tasks_with_deps = {task: deps for task, deps in result['dependencies'].items() if deps}\n",
    "        \n",
    "        if tasks_with_deps:\n",
    "            print(f\"Found {len(tasks_with_deps)} tasks with dependencies:\\n\")\n",
    "            \n",
    "            for i, (task, deps) in enumerate(tasks_with_deps.items(), 1):\n",
    "                print(f\"{i}. ðŸ“‹ TASK: {task}\")\n",
    "                print(f\"   â¬‡ï¸  DEPENDS ON:\")\n",
    "                for j, dep in enumerate(deps, 1):\n",
    "                    print(f\"      {j}. {dep}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"â„¹ï¸  No dependencies detected between tasks\")\n",
    "        \n",
    "        # SKILLS MAPPING\n",
    "        print(f\"\\nðŸŽ¯ SKILLS REQUIRED FOR EACH TASK:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (task, skills) in enumerate(result['required_skills'].items(), 1):\n",
    "            if skills:  # Only show tasks with skills\n",
    "                print(f\"{i:2d}. ðŸ“‹ {task}\")\n",
    "                print(f\"     ðŸ› ï¸  Skills: {', '.join(skills)}\")\n",
    "                print()\n",
    "        \n",
    "        # ANALYSIS\n",
    "        print(f\"\\nðŸ“ˆ DETAILED ANALYSIS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Task distribution per story\n",
    "        story_counts = {}\n",
    "        for task, origins in result['task_origins'].items():\n",
    "            for origin in origins:\n",
    "                short_origin = origin[:40] + \"...\" if len(origin) > 40 else origin\n",
    "                story_counts[short_origin] = story_counts.get(short_origin, 0) + 1\n",
    "        \n",
    "        print(\"ðŸ“Š Tasks Generated per Story:\")\n",
    "        for story, count in story_counts.items():\n",
    "            print(f\"   â€¢ {story}: {count} tasks\")\n",
    "        \n",
    "        # Dependency analysis\n",
    "        dep_counts = [len(deps) for deps in result['dependencies'].values()]\n",
    "        if dep_counts:\n",
    "            max_deps = max(dep_counts)\n",
    "            avg_deps = sum(dep_counts) / len(dep_counts)\n",
    "            tasks_with_deps_count = sum(1 for d in dep_counts if d > 0)\n",
    "            \n",
    "            print(f\"\\nðŸ”— Dependency Statistics:\")\n",
    "            print(f\"   â€¢ Tasks with dependencies: {tasks_with_deps_count}\")\n",
    "            print(f\"   â€¢ Maximum dependencies per task: {max_deps}\")\n",
    "            print(f\"   â€¢ Average dependencies per task: {avg_deps:.1f}\")\n",
    "        \n",
    "        # Skills analysis\n",
    "        all_skills = set()\n",
    "        for skills in result['required_skills'].values():\n",
    "            all_skills.update(skills)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Skills Analysis:\")\n",
    "        print(f\"   â€¢ Unique skills identified: {len(all_skills)}\")\n",
    "        if all_skills:\n",
    "            print(f\"   â€¢ Most common skills: {', '.join(list(all_skills)[:5])}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ðŸ INDIVIDUAL PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# SIMPLE FUNCTION TO RUN AND DISPLAY EVERYTHING\n",
    "async def run_with_full_details():\n",
    "    \"\"\"Run the processor and display all detailed results immediately\"\"\"\n",
    "    \n",
    "    print(\"ðŸ¢ PARTY A: INDIVIDUAL PROCESSING WITH FULL DETAILS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Philosophy: 'One story at a time, done right'\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Use sample stories\n",
    "    user_stories = SAMPLE_USER_STORIES\n",
    "    print(f\"Processing {len(user_stories)} sample user stories...\")\n",
    "    \n",
    "    # Create processor and run with detailed display\n",
    "    processor = EnhancedIndividualProcessor()\n",
    "    result = await processor.process_stories_with_details(user_stories)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"Main function to run the enhanced individual processor\"\"\"\n",
    "    print(\"ðŸš€ Enhanced Individual Processor for User Stories\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run with sample stories\n",
    "    result = await run_with_full_details()\n",
    "    \n",
    "    if result and \"error\" not in result:\n",
    "        print(\"\\nâœ… Processing completed successfully!\")\n",
    "        \n",
    "        # Display final summary\n",
    "        print(f\"\\nðŸ“ˆ FINAL SUMMARY:\")\n",
    "        print(f\"   â€¢ Total User Stories: {len(SAMPLE_USER_STORIES)}\")\n",
    "        print(f\"   â€¢ Total Tasks Generated: {len(result.get('tasks', []))}\")\n",
    "        print(f\"   â€¢ Total Dependencies: {sum(len(deps) for deps in result.get('dependencies', {}).values())}\")\n",
    "        print(f\"   â€¢ Total Skills Identified: {sum(len(skills) for skills in result.get('required_skills', {}).values())}\")\n",
    "        \n",
    "        # Token usage summary\n",
    "        summary = token_tracker.get_summary()\n",
    "        if summary:\n",
    "            cost_data = summary.get(\"cost_estimate\", {})\n",
    "            if cost_data:\n",
    "                print(f\"   â€¢ Estimated API Cost: ${cost_data['total_cost']:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Processing failed!\")\n",
    "        if result and \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "\n",
    "# For Jupyter notebook execution\n",
    "if __name__ == \"__main__\":\n",
    "    # In Jupyter, you can run this directly\n",
    "    import nest_asyncio\n",
    "    try:\n",
    "        nest_asyncio.apply()\n",
    "        result = asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        # If already in an async context, just await\n",
    "        result = await main()\n",
    "else:\n",
    "    # For direct execution in Jupyter\n",
    "    print(\"ðŸ“ Script loaded successfully!\")\n",
    "    print(\"To run, execute: await main() or asyncio.run(main())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91976800-a6ce-4ca3-8da1-f8a7261e1007",
   "metadata": {},
   "source": [
    "**Individual Processing Performance Analysis**\n",
    "\n",
    "- Processing Time\n",
    "> **Performance:** â­â­ (Slowest)\n",
    "\n",
    "- Characteristics :\n",
    "\n",
    "> Sequential processing of each story\n",
    "\n",
    "> No parallelization benefits\n",
    "\n",
    "> Processing time scales linearly: `O(n)` where n = number of stories\n",
    "\n",
    "> Overhead from multiple API round-trips\n",
    "\n",
    "- Expected Range\n",
    "> **53-106 seconds for 10 stories**\n",
    "\n",
    "---\n",
    "\n",
    "- ðŸ”¤ Token Usage\n",
    "> **Efficiency:** â­â­ (Highest Usage)\n",
    "\n",
    "- Estimated Tokens\n",
    "> **~53-106 seconds for 10 stories + analysis overhead**\n",
    "\n",
    "---\n",
    "\n",
    "- API Costs\n",
    "> **Cost Rating:** â­ (Most Expensive)\n",
    "\n",
    "- Estimated Cost \n",
    "> **~4.2 API calls per story**\n",
    "\n",
    "- Cost Factors\n",
    "  \n",
    "> Maximum API call count\n",
    "\n",
    "> Redundant context transmission\n",
    "\n",
    "> Individual skill mapping calls\n",
    "\n",
    "---\n",
    "\n",
    "- Dependency Detection Accuracy\n",
    "> **Accuracy:** â­â­â­â­â­ (Highest)\n",
    "\n",
    "- Advantages\n",
    "  \n",
    "> Each story gets individual attention\n",
    "\n",
    "> Detailed task breakdown per story\n",
    "\n",
    "> Comprehensive dependency analysis on final consolidated tasks\n",
    "\n",
    "> No context dilution\n",
    "\n",
    "- Disadvantages\n",
    "\n",
    "> Missed cross-story dependencies \n",
    "\n",
    "---\n",
    "\n",
    "- ðŸ“Š Summary\n",
    "\n",
    "- Best Performance Aspects\n",
    "\n",
    "> **âœ… Highest reliability and accuracy**\n",
    "\n",
    "> **âœ… Error isolation per story**\n",
    "\n",
    "> **âœ… Comprehensive individual analysis**\n",
    "\n",
    "- Trade-offs\n",
    "  \n",
    "> **âš ï¸ Higher API usage**\n",
    "\n",
    "> **âš ï¸ Slower execution time**\n",
    "\n",
    "> **âš ï¸ Less efficient for large batches**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20a6a8-7737-447c-b810-919bbb9bbf53",
   "metadata": {},
   "source": [
    "#### 2.1.2 Strategy 2: Batch All Processing\n",
    "\n",
    "**Approach**: Process all user stories in a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2818c74-6429-4ff1-a3ad-4b0596cf9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedBatchAllProcessor:\n",
    "    \"\"\"Enhanced version that captures and displays detailed results - conforming to individual processor format\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    async def process_stories_with_details(self, user_stories: List[str]):\n",
    "        \"\"\"Process stories and immediately display detailed results\"\"\"\n",
    "        print(f\"âš¡ SPEED DEMONS: Processing ALL {len(user_stories)} stories in ONE batch...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Process all stories and get formatted results\n",
    "            results = await process_all_user_stories(user_stories)\n",
    "            \n",
    "            if not results:\n",
    "                raise Exception(\"No results generated\")\n",
    "            \n",
    "            # Calculate metrics - now we have 3 API calls total (task extraction, dependencies, skills)\n",
    "            api_calls = 3  # task extraction, dependency analysis, skills mapping\n",
    "            \n",
    "            print(f\"âœ… SUCCESS: Processed {len(user_stories)} stories with only {api_calls} API calls\")\n",
    "            \n",
    "            # Format output similar to individual processor\n",
    "            formatted_output = self.format_output(results)\n",
    "            print(formatted_output)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def format_output(self, results: Dict[str, any]) -> str:\n",
    "        \"\"\"Format results for display - matching individual processor format\"\"\"\n",
    "        output = []\n",
    "        \n",
    "        # Token usage summary\n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(\"TOKEN USAGE SUMMARY\")\n",
    "        output.append(\"=\" * 80)\n",
    "        \n",
    "        summary = token_tracker.get_summary()\n",
    "        if summary:\n",
    "            breakdown = summary.get(\"breakdown\", {})\n",
    "            total_tokens = breakdown.get('total_consumed', 0)\n",
    "            cost_data = summary.get(\"cost_estimate\", {})\n",
    "            \n",
    "            output.append(f\"Total Tokens Used: {total_tokens}\")\n",
    "            if cost_data:\n",
    "                output.append(f\"Estimated Cost: ${cost_data['total_cost']:.6f}\")\n",
    "            \n",
    "            # Category breakdown\n",
    "            categories = ['task_extraction', 'dependency_analysis', 'skills_mapping']\n",
    "            for cat in categories:\n",
    "                cat_data = breakdown.get(cat, {})\n",
    "                if cat_data and cat_data.get('total', 0) > 0:\n",
    "                    percentage = (cat_data['total'] / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "                    output.append(f\"  {cat.replace('_', ' ').title()}: {cat_data['total']} tokens ({percentage:.1f}%)\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "        \n",
    "        # Results summary\n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(\"PROCESSING RESULTS\")\n",
    "        output.append(\"=\" * 80)\n",
    "        \n",
    "        tasks = results.get(\"tasks\", [])\n",
    "        dependencies = results.get(\"dependencies\", {})\n",
    "        required_skills = results.get(\"required_skills\", {})\n",
    "        task_origins = results.get(\"task_origins\", {})\n",
    "        \n",
    "        output.append(f\"Total Tasks Generated: {len(tasks)}\")\n",
    "        output.append(f\"Dependencies Found: {sum(len(deps) for deps in dependencies.values())}\")\n",
    "        output.append(f\"Skills Identified: {sum(len(skills) for skills in required_skills.values())}\")\n",
    "        output.append(\"\")\n",
    "        \n",
    "        # Tasks list\n",
    "        output.append(\"CONSOLIDATED TASKS:\")\n",
    "        output.append(\"-\" * 40)\n",
    "        for i, task in enumerate(tasks, 1):\n",
    "            output.append(f\"{i:2d}. {task}\")\n",
    "            \n",
    "            # Show origins\n",
    "            origins = task_origins.get(task, [])\n",
    "            if origins:\n",
    "                if len(origins) == 1:\n",
    "                    output.append(f\"    â””â”€ From: {origins[0][:60]}...\")\n",
    "                else:\n",
    "                    output.append(f\"    â””â”€ From {len(origins)} stories (duplicate found)\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "        \n",
    "        # Dependencies\n",
    "        output.append(\"TASK DEPENDENCIES:\")\n",
    "        output.append(\"-\" * 40)\n",
    "        tasks_with_deps = {task: deps for task, deps in dependencies.items() if deps}\n",
    "        \n",
    "        if tasks_with_deps:\n",
    "            for i, (task, deps) in enumerate(tasks_with_deps.items(), 1):\n",
    "                output.append(f\"{i}. TASK: {task}\")\n",
    "                output.append(f\"   DEPENDS ON:\")\n",
    "                for j, dep in enumerate(deps, 1):\n",
    "                    output.append(f\"      {j}. {dep}\")\n",
    "                output.append(\"\")\n",
    "        else:\n",
    "            output.append(\"No dependencies detected between tasks\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "        \n",
    "        # Skills\n",
    "        output.append(\"REQUIRED SKILLS:\")\n",
    "        output.append(\"-\" * 40)\n",
    "        for i, (task, skills) in enumerate(required_skills.items(), 1):\n",
    "            if skills:\n",
    "                output.append(f\"{i:2d}. {task}\")\n",
    "                output.append(f\"     Skills: {', '.join(skills)}\")\n",
    "        \n",
    "        return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2197f-7974-409f-8b69-229202d45684",
   "metadata": {},
   "source": [
    "**Batch All Processing Performance Analysis**\n",
    "- Processing Time\n",
    "> **Performance:** â­â­â­â­â­ (Fastest)\n",
    "- Characteristics :\n",
    "  \n",
    "> All stories processed in 1 API call\n",
    "\n",
    "> Maximum parallelization within LLM\n",
    "\n",
    "> Minimal API calls \n",
    "\n",
    "> Processing time: `O(1)` for decomposition step\n",
    "- Expected Range\n",
    "> **5-10 seconds for 10 stories**\n",
    "---\n",
    "- ðŸ”¤ Token Usage\n",
    "> **Efficiency:** â­â­â­â­â­ (Most Efficient)\n",
    "\n",
    "- Estimated Tokens\n",
    "> **~623.6 per story ** (82% cheaper than individual processing on output tokens no repetition of similar task structures)\n",
    "---\n",
    "- API Costs\n",
    "> **Cost Rating:** â­â­â­â­â­ (Cheapest)\n",
    "- Cost Structure\n",
    "> **4 API calls : 1 (decomposition) + 1 (dependencies) + 1 Unique_Tasks (skills) + 1 5 (story_points estimation)**\n",
    "- Cost Factors\n",
    "  \n",
    "> ~40-50% cost reduction vs. Individual\n",
    "\n",
    "> Maximum cost optimization\n",
    "\n",
    "---\n",
    "- Dependency Detection Accuracy\n",
    "> **Accuracy:** â­â­â­ (Variable)\n",
    "- Challenges\n",
    "  \n",
    "> Context dilution with large batches\n",
    "\n",
    "> Potential task detail reduction\n",
    "\n",
    "> LLM attention distribution across many stories\n",
    "\n",
    "> Risk of missing subtle dependencies\n",
    "- Mitigation\n",
    "> **Works best with <15 stories**\n",
    "---\n",
    "- Error Recovery Capability\n",
    "> **Resilience:** â­â­ (Risky)\n",
    "- Vulnerabilities\n",
    "  \n",
    "> **Single Point of Failure:** One failed call affects entire batch\n",
    "\n",
    "> **All-or-Nothing:** No partial processing capability\n",
    "\n",
    "> **Difficult Debugging:** Hard to isolate specific story issues\n",
    "\n",
    "> **Context Limits:** May hit token limits with large batches\n",
    "---\n",
    "- ðŸ“Š Summary\n",
    "- Best Performance Aspects\n",
    "  \n",
    "> **âœ… MAXIMUM efficiency - fewest API calls possible**\n",
    "\n",
    "> **âœ… FASTEST processing - single decomposition step**\n",
    "\n",
    "> **âœ… LOWEST cost - minimal API usage**\n",
    "\n",
    "> **âœ… ELEGANT simplicity - clean approach**\n",
    "- Trade-offs\n",
    "\n",
    "> **âš ï¸ Single point of failure â€“ One failed batch call breaks all stories.**\n",
    "\n",
    "> **âš ï¸ Context window limitationsâ€“ Large batches may hit token limits.**\n",
    "\n",
    "> **âš ï¸ Harder to debug issuesâ€“ Errors blend across stories, requiring manual isolation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f90bd-174c-4fb0-891a-b66fc2b60c6d",
   "metadata": {},
   "source": [
    "#### 2.1.3 Strategy 3: Grouped Processing\n",
    "\n",
    "**Approach**: Process user stories in logical groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08bc864b-3090-45af-9676-430779052dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Enhanced Grouped Processor for User Stories\n",
      "============================================================\n",
      "ðŸš€ PARTY B: GROUPED PROCESSING WITH FULL DETAILS\n",
      "============================================================\n",
      "Philosophy: 'Smart batching for balanced efficiency'\n",
      "Group size: 2\n",
      "\n",
      "Processing 5 sample user stories in groups of 2...\n",
      "ðŸš€ OPTIMIZERS: Processing 5 stories in groups of 2...\n",
      "\n",
      "ðŸ” GROUP PROCESSING ANALYSIS:\n",
      "   Stories: 5\n",
      "   Group size: 2\n",
      "   Total groups: 3\n",
      "   Estimated tokens per group: 34\n",
      "\n",
      "ðŸ“¦ Processing 3 groups...\n",
      "   Processing group 1/3 (2 stories)...\n",
      "[TASK_EXTRACTION] Tokens - Input: 363, Output: 214, Total: 577\n",
      "ðŸ“ Group extraction response preview: Here are the broken down tasks for each user story:\n",
      "\n",
      "**Tasks for Story 1:**\n",
      "1. Design user registration form\n",
      "2. Implement user authentication system (backend)\n",
      "3. Create user profile management (backen...\n",
      "   âœ… Group 1 completed with 0 tasks\n",
      "   Processing group 2/3 (2 stories)...\n",
      "[TASK_EXTRACTION] Tokens - Input: 366, Output: 177, Total: 543\n",
      "ðŸ“ Group extraction response preview: Here are the broken down tasks for each user story:\n",
      "\n",
      "**Tasks for Story 1:**\n",
      "1. Create search bar component in UI\n",
      "2. Implement search query input handling\n",
      "3. Design search result layout\n",
      "4. Implement pr...\n",
      "   âœ… Group 2 completed with 0 tasks\n",
      "   Processing group 3/3 (1 stories)...\n",
      "[TASK_EXTRACTION] Tokens - Input: 345, Output: 667, Total: 1012\n",
      "ðŸ“ Group extraction response preview: Here are the task decompositions for each user story:\n",
      "\n",
      "User Stories:\n",
      "1. As a user, I want to reset my password so that I can regain access to my account\n",
      "2. As a user, I want to view the list of recycl...\n",
      "   âœ… Group 3 completed with 51 tasks\n",
      "\n",
      "ðŸ”¢ Estimating story points for all tasks...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 492, Output: 417, Total: 909\n",
      "ðŸ”— Analyzing dependencies across all tasks...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 661, Output: 583, Total: 1244\n",
      "ðŸŽ¯ Mapping skills for all tasks...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 487, Output: 1438, Total: 1925\n",
      "ðŸ”„ Step 5: Formatting results...\n",
      "ðŸ† SUCCESS: Processed 5 stories in 3 groups with 6 API calls!\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 6210\n",
      "ESTIMATED COST: $0.097060\n",
      "\n",
      "================================================================================\n",
      "PROCESSED USER STORIES\n",
      "================================================================================\n",
      "\n",
      "--- Story 1 ---\n",
      "{\n",
      "  \"input\": \"As a user, I want to reset my password so that I can regain access to my account\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 139,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design password reset form\",\n",
      "        \"id\": \"URP_001\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design password reset form - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement password reset functionality\",\n",
      "        \"id\": \"URP_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement password reset functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"authentication framework (e.g. OAuth\",\n",
      "          \"Okta)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send password reset email with token\",\n",
      "        \"id\": \"URP_003\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_002\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Send password reset email with token - Email service integration (e.g. Sendgrid\",\n",
      "          \"Mailgun)\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"token generation and management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Handle password reset token validation\",\n",
      "        \"id\": \"URP_004\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_003\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Handle password reset token validation - Token validation\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"authentication framework (e.g. OAuth\",\n",
      "          \"Okta)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update user password in database\",\n",
      "        \"id\": \"URP_005\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_004\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update user password in database - Database management (e.g. MySQL\",\n",
      "          \"MongoDB)\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"SQL\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement password strength validation\",\n",
      "        \"id\": \"URP_006\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement password strength validation - JavaScript\",\n",
      "          \"regular expressions\",\n",
      "          \"password hashing and verification\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Display password reset success message\",\n",
      "        \"id\": \"URP_007\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_005\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Display password reset success message - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility list page layout\",\n",
      "        \"id\": \"URP_008\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility list page layout - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility search functionality\",\n",
      "        \"id\": \"URP_009\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_008\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility search functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"search algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate facility search with map view\",\n",
      "        \"id\": \"URP_010\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_009\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Integrate facility search with map view - Map integration (e.g. Google Maps\",\n",
      "          \"Leaflet)\",\n",
      "          \"JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Display facility list with basic information\",\n",
      "        \"id\": \"URP_011\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_010\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Display facility list with basic information - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement pagination for facility list\",\n",
      "        \"id\": \"URP_012\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_011\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement pagination for facility list - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"pagination algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Detect user's location via browser API or IP\",\n",
      "        \"id\": \"URP_013\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Detect user's location via browser API or IP - Geolocation API\",\n",
      "          \"JavaScript\",\n",
      "          \"IP geolocation service integration (e.g. IP2Location)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Show recycling centers within a radius of the user\",\n",
      "        \"id\": \"URP_014\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_013\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Show recycling centers within a radius of the user - Geolocation API\",\n",
      "          \"JavaScript\",\n",
      "          \"map integration (e.g. Google Maps\",\n",
      "          \"Leaflet)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility list filtering by distance\",\n",
      "        \"id\": \"URP_015\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_014\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility list filtering by distance - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"filtering algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility list sorting by distance\",\n",
      "        \"id\": \"URP_016\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_015\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility list sorting by distance - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"sorting algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility filter component\",\n",
      "        \"id\": \"URP_017\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility filter component - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility type filtering\",\n",
      "        \"id\": \"URP_018\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_017\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility type filtering - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"filtering algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility type filter to facility list page\",\n",
      "        \"id\": \"URP_019\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility type filter to facility list page - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update facility search functionality to include type filter\",\n",
      "        \"id\": \"URP_020\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_019\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update facility search functionality to include type filter - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"search algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Display facility type filter options\",\n",
      "        \"id\": \"URP_021\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Display facility type filter options - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility type filter validation\",\n",
      "        \"id\": \"URP_022\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility type filter validation - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"validation algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update facility list to reflect filtered results\",\n",
      "        \"id\": \"URP_023\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_022\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update facility list to reflect filtered results - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"data binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility details page layout\",\n",
      "        \"id\": \"URP_024\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility details page layout - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility details page functionality\",\n",
      "        \"id\": \"URP_025\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_024\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility details page functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"data binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Display facility details information\",\n",
      "        \"id\": \"URP_026\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_025\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Display facility details information - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility details page routing\",\n",
      "        \"id\": \"URP_027\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility details page routing - JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\",\n",
      "          \"routing library (e.g. React Router)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility details page SEO optimization\",\n",
      "        \"id\": \"URP_028\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_027\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility details page SEO optimization - SEO best practices\",\n",
      "          \"metadata management\",\n",
      "          \"JavaScript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Display facility address and map view\",\n",
      "        \"id\": \"URP_029\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_025\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Display facility address and map view - Map integration (e.g. Google Maps\",\n",
      "          \"Leaflet)\",\n",
      "          \"JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility details page sharing functionality\",\n",
      "        \"id\": \"URP_030\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_025\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility details page sharing functionality - JavaScript\",\n",
      "          \"social media API integration (e.g. Facebook\",\n",
      "          \"Twitter)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility creation form\",\n",
      "        \"id\": \"URP_031\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility creation form - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility creation functionality\",\n",
      "        \"id\": \"URP_032\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_031\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility creation functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"database management (e.g. MySQL\",\n",
      "          \"MongoDB)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Validate facility creation form input\",\n",
      "        \"id\": \"URP_033\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_032\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Validate facility creation form input - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"validation algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility creation form to facility list page\",\n",
      "        \"id\": \"URP_034\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_032\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility creation form to facility list page - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility creation success message\",\n",
      "        \"id\": \"URP_035\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility creation success message - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update facility list to reflect new facility\",\n",
      "        \"id\": \"URP_036\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_035\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update facility list to reflect new facility - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"data binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send facility creation notification to administrators\",\n",
      "        \"id\": \"URP_037\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_036\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Send facility creation notification to administrators - Email service integration (e.g. Sendgrid\",\n",
      "          \"Mailgun)\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility edit form\",\n",
      "        \"id\": \"URP_038\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility edit form - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility edit functionality\",\n",
      "        \"id\": \"URP_039\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_038\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility edit functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"database management (e.g. MySQL\",\n",
      "          \"MongoDB)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Validate facility edit form input\",\n",
      "        \"id\": \"URP_040\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_039\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Validate facility edit form input - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"validation algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility edit form to facility details page\",\n",
      "        \"id\": \"URP_041\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_039\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility edit form to facility details page - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility edit success message\",\n",
      "        \"id\": \"URP_042\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility edit success message - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update facility details to reflect changes\",\n",
      "        \"id\": \"URP_043\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_042\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update facility details to reflect changes - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"data binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send facility edit notification to administrators\",\n",
      "        \"id\": \"URP_044\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_043\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Send facility edit notification to administrators - Email service integration (e.g. Sendgrid\",\n",
      "          \"Mailgun)\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design facility deletion confirmation\",\n",
      "        \"id\": \"URP_045\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Design facility deletion confirmation - HTML\",\n",
      "          \"CSS\",\n",
      "          \"UI/UX design\",\n",
      "          \"responsive design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility deletion functionality\",\n",
      "        \"id\": \"URP_046\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_045\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility deletion functionality - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"database management (e.g. MySQL\",\n",
      "          \"MongoDB)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Validate facility deletion confirmation\",\n",
      "        \"id\": \"URP_047\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_046\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Validate facility deletion confirmation - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"validation algorithm implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add facility deletion link to facility details page\",\n",
      "        \"id\": \"URP_048\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_046\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add facility deletion link to facility details page - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement facility deletion success message\",\n",
      "        \"id\": \"URP_049\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"Implement facility deletion success message - HTML\",\n",
      "          \"CSS\",\n",
      "          \"JavaScript\",\n",
      "          \"frontend framework (e.g. React\",\n",
      "          \"Angular)\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update facility list to reflect deleted facility\",\n",
      "        \"id\": \"URP_050\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_049\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Update facility list to reflect deleted facility - JavaScript\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\",\n",
      "          \"data binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send facility deletion notification to administrators\",\n",
      "        \"id\": \"URP_051\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"URP_050\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Send facility deletion notification to administrators - Email service integration (e.g. Sendgrid\",\n",
      "          \"Mailgun)\",\n",
      "          \"backend programming (e.g. Node.js\",\n",
      "          \"Python)\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "âœ… Grouped processing completed successfully!\n",
      "\n",
      "ðŸ“ˆ FINAL SUMMARY:\n",
      "   â€¢ Total User Stories: 5\n",
      "   â€¢ Total Tasks Generated: 51\n",
      "   â€¢ Total Story Points: 139\n",
      "   â€¢ Total Dependencies: 36\n",
      "   â€¢ Total Skills Identified: 219\n",
      "   â€¢ API Calls: 6 (balanced efficiency!)\n",
      "   â€¢ Tasks per API call: 8.5\n",
      "   â€¢ Total Tokens Used: 6210\n",
      "   â€¢ Total Cost: $0.097060\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "# Try to import tiktoken for token counting\n",
    "try:\n",
    "    import tiktoken\n",
    "    TIKTOKEN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TIKTOKEN_AVAILABLE = False\n",
    "    print(\"Warning: tiktoken not available, using approximate token counting\")\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResults:\n",
    "    method: str\n",
    "    execution_time: float\n",
    "    api_calls: int\n",
    "    total_tasks: int\n",
    "    unique_tasks: int\n",
    "    duplicate_reduction: float\n",
    "    dependencies_found: int\n",
    "    total_skills: int\n",
    "    stories_processed: int\n",
    "    errors: List[str]\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = None\n",
    "        if TIKTOKEN_AVAILABLE:\n",
    "            try:\n",
    "                self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not initialize tiktoken: {e}\")\n",
    "                self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            if self.tokenizer:\n",
    "                return len(self.tokenizer.encode(text))\n",
    "            else:\n",
    "                return len(text) // 4\n",
    "        except Exception:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "# Sample user stories for testing\n",
    "SAMPLE_USER_STORIES = [\n",
    "    \"As a user, I want to create an account so that I can access personalized features\",\n",
    "    \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
    "    \"As a customer, I want to search for products so that I can find what I need quickly\", \n",
    "    \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\",\n",
    "    \"As a user, I want to reset my password so that I can regain access to my account\"\n",
    "]\n",
    "\n",
    "class GroupedBatchDecomposer:\n",
    "    \"\"\"Batch decomposer optimized for group processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_examples = \"\"\"\n",
    "User Stories:\n",
    "1. As a user, I want to click on the address so that it takes me to a new tab with Google Maps.\n",
    "2. As a user, I want to be able to anonymously view public information so that I know about recycling centers near me before creating an account.\n",
    "3. As a user, I want to create an account so that I can save my favorite recycling centers.\n",
    "\n",
    "Tasks for Story 1:\n",
    "1. Make address text clickable\n",
    "2. Implement click handler to format address for Google Maps URL\n",
    "3. Open Google Maps in new tab/window\n",
    "4. Add proper URL encoding for address parameters\n",
    "\n",
    "Tasks for Story 2:\n",
    "1. Design public landing page layout\n",
    "2. Create anonymous user session handling\n",
    "3. Implement facility search without authentication\n",
    "4. Display basic facility information publicly \n",
    "5. Design facility component\n",
    "6. Detect user's location via browser API or IP\n",
    "7. Show recycling centers within a radius of the user\n",
    "\n",
    "Tasks for Story 3:\n",
    "1. Design user registration form\n",
    "2. Implement user authentication system\n",
    "3. Create user profile management\n",
    "4. Add favorites functionality to UI\n",
    "5. Implement save/unsave facility feature\n",
    "\"\"\"\n",
    "    \n",
    "    async def decompose_group(self, user_stories: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Decompose a group of user stories (typically 2-3 stories)\"\"\"\n",
    "        stories_text = \"\\n\".join([f\"{i+1}. {story}\" for i, story in enumerate(user_stories)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a task decomposition expert. Break down EACH of the following user stories into specific, actionable technical tasks.\n",
    "Each task should be simple and focused on a single responsibility.\n",
    "\n",
    "For each user story, provide tasks in the format:\n",
    "Tasks for Story X:\n",
    "1. Task description\n",
    "2. Task description\n",
    "...\n",
    "\n",
    "IMPORTANT: Return tasks for ALL user stories. Do NOT skip any stories.\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "User Stories:\n",
    "{stories_text}\n",
    "\n",
    "Tasks:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "            \n",
    "            print(f\"ðŸ“ Group extraction response preview: {output_text[:200]}...\")\n",
    "            \n",
    "            return self._parse_response(output_text, user_stories)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in group decomposition: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _parse_response(self, content: str, user_stories: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Parse the LLM response into structured task data\"\"\"\n",
    "        result = {}\n",
    "        lines = content.split('\\n')\n",
    "        current_story = None\n",
    "        current_tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if line.lower().startswith('tasks for story'):\n",
    "                if current_story is not None and current_tasks:\n",
    "                    result[current_story] = current_tasks\n",
    "                \n",
    "                story_match = re.search(r'story\\s+(\\d+)', line.lower())\n",
    "                if story_match:\n",
    "                    story_num = int(story_match.group(1)) - 1\n",
    "                    if 0 <= story_num < len(user_stories):\n",
    "                        current_story = user_stories[story_num]\n",
    "                        current_tasks = []\n",
    "            \n",
    "            elif line and any(line.startswith(str(i) + '.') for i in range(1, 21)):\n",
    "                task = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "                if task and len(task) > 10:\n",
    "                    current_tasks.append(task)\n",
    "        \n",
    "        if current_story is not None and current_tasks:\n",
    "            result[current_story] = current_tasks\n",
    "        \n",
    "        return result\n",
    "\n",
    "class GroupedProcessor:\n",
    "    \"\"\"\n",
    "    Enhanced Grouped Processor that processes user stories in small groups\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, group_size: int = 2):\n",
    "        self.group_size = group_size\n",
    "        self.batch_decomposer = GroupedBatchDecomposer()\n",
    "    \n",
    "    def check_context_limits(self, user_stories: List[str]) -> Dict[str, any]:\n",
    "        \"\"\"Analyze if the group size is appropriate\"\"\"\n",
    "        total_chars = sum(len(story) for story in user_stories)\n",
    "        estimated_tokens = total_chars // 4\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_stories\": len(user_stories),\n",
    "            \"group_size\": self.group_size,\n",
    "            \"total_groups\": len(user_stories) // self.group_size + (1 if len(user_stories) % self.group_size else 0),\n",
    "            \"total_characters\": total_chars,\n",
    "            \"estimated_tokens\": estimated_tokens,\n",
    "            \"tokens_per_group\": estimated_tokens // max(1, len(user_stories) // self.group_size + (1 if len(user_stories) % self.group_size else 0)),\n",
    "            \"risk_level\": \"LOW\"\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    async def process_stories_with_details(self, user_stories: List[str]):\n",
    "        \"\"\"Process stories in groups and display detailed results\"\"\"\n",
    "        print(f\"ðŸš€ OPTIMIZERS: Processing {len(user_stories)} stories in groups of {self.group_size}...\")\n",
    "        start_time = time.time()\n",
    "        errors = []\n",
    "        api_calls = 0\n",
    "        \n",
    "        # Context analysis\n",
    "        context_analysis = self.check_context_limits(user_stories)\n",
    "        print(f\"\\nðŸ” GROUP PROCESSING ANALYSIS:\")\n",
    "        print(f\"   Stories: {context_analysis['total_stories']}\")\n",
    "        print(f\"   Group size: {context_analysis['group_size']}\")\n",
    "        print(f\"   Total groups: {context_analysis['total_groups']}\")\n",
    "        print(f\"   Estimated tokens per group: {context_analysis['tokens_per_group']}\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Group stories and decompose each group\n",
    "            groups = [user_stories[i:i + self.group_size] for i in range(0, len(user_stories), self.group_size)]\n",
    "            all_user_stories_tasks = {}\n",
    "            \n",
    "            print(f\"\\nðŸ“¦ Processing {len(groups)} groups...\")\n",
    "            \n",
    "            for i, group in enumerate(groups, 1):\n",
    "                try:\n",
    "                    print(f\"   Processing group {i}/{len(groups)} ({len(group)} stories)...\")\n",
    "                    group_tasks = await self.batch_decomposer.decompose_group(group)\n",
    "                    api_calls += 1\n",
    "                    all_user_stories_tasks.update(group_tasks)\n",
    "                    print(f\"   âœ… Group {i} completed with {sum(len(tasks) for tasks in group_tasks.values())} tasks\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to process group {i}: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    print(f\"   âŒ {error_msg}\")\n",
    "            \n",
    "            if not all_user_stories_tasks:\n",
    "                raise Exception(\"No tasks extracted from any groups\")\n",
    "            \n",
    "            # Step 2: Estimate story points for all tasks\n",
    "            print(f\"\\nðŸ”¢ Estimating story points for all tasks...\")\n",
    "            story_points = await self._estimate_story_points_batch(all_user_stories_tasks)\n",
    "            api_calls += 1\n",
    "            \n",
    "            # Step 3: Analyze dependencies\n",
    "            print(\"ðŸ”— Analyzing dependencies across all tasks...\")\n",
    "            dependencies = await self._analyze_dependencies_batch(all_user_stories_tasks, story_points)\n",
    "            api_calls += 1\n",
    "            \n",
    "            # Step 4: Map skills for each task\n",
    "            print(\"ðŸŽ¯ Mapping skills for all tasks...\")\n",
    "            skills = await self._map_skills_batch(all_user_stories_tasks)\n",
    "            api_calls += 1\n",
    "            \n",
    "            print(\"ðŸ”„ Step 5: Formatting results...\")\n",
    "            # Step 5: Format results to match individual processor output\n",
    "            results = self._format_results(all_user_stories_tasks, story_points, dependencies, skills)\n",
    "            \n",
    "            print(f\"ðŸ† SUCCESS: Processed {len(user_stories)} stories in {len(groups)} groups with {api_calls} API calls!\")\n",
    "            \n",
    "            # Format and display output\n",
    "            formatted_output = self._format_output(results)\n",
    "            print(formatted_output)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CRITICAL ERROR: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    async def _estimate_story_points_batch(self, user_stories_tasks: Dict[str, List[str]]) -> Dict[str, int]:\n",
    "        \"\"\"Estimate story points for all tasks in batch\"\"\"\n",
    "        all_tasks = []\n",
    "        for tasks in user_stories_tasks.values():\n",
    "            all_tasks.extend(tasks)\n",
    "        \n",
    "        if not all_tasks:\n",
    "            return {}\n",
    "        \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(all_tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "            \n",
    "            return self._parse_story_points_batch(output_text, all_tasks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in story points estimation: {e}\")\n",
    "            return {task: 3 for task in all_tasks}\n",
    "    \n",
    "    def _parse_story_points_batch(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Parse story points from batch response\"\"\"\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    async def _analyze_dependencies_batch(self, user_stories_tasks: Dict[str, List[str]], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Analyze dependencies for all tasks in batch with rework_effort\"\"\"\n",
    "        all_tasks = []\n",
    "        for tasks in user_stories_tasks.values():\n",
    "            all_tasks.extend(tasks)\n",
    "        \n",
    "        if not all_tasks:\n",
    "            return {}\n",
    "        \n",
    "        tasks_with_points = []\n",
    "        for i, task in enumerate(all_tasks):\n",
    "            points = story_points.get(task, 3)\n",
    "            tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "        \n",
    "        tasks_str = \"\\n\".join(tasks_with_points)\n",
    "        \n",
    "        prompt = f\"\"\"You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "rework_effort scale:\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed  \n",
    "- 3: High rework effort required\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "            \n",
    "            return self._parse_dependencies_with_effort(output_text, all_tasks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in dependencies analysis: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _parse_dependencies_with_effort(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Parse dependencies with rework_effort\"\"\"\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "    \n",
    "    async def _map_skills_batch(self, user_stories_tasks: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Map skills for all tasks in batch\"\"\"\n",
    "        all_tasks = []\n",
    "        for tasks in user_stories_tasks.values():\n",
    "            all_tasks.extend(tasks)\n",
    "        \n",
    "        if not all_tasks:\n",
    "            return {}\n",
    "        \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(all_tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"You are a technical skills analyst. Identify specific skills required for each task.\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return skills for each task in the format:\n",
    "Task 1: skill1, skill2, skill3\n",
    "Task 2: skill4, skill5, skill6\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "            \n",
    "            return self._parse_skills_batch(output_text, all_tasks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in skills mapping: {e}\")\n",
    "            return {task: [\"general_development\"] for task in all_tasks}\n",
    "    \n",
    "    def _parse_skills_batch(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Parse skills from LLM batch response\"\"\"\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        \"\"\"Generate task IDs like the individual processor\"\"\"\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _format_results(self, user_stories_tasks: Dict[str, List[str]], story_points: Dict[str, int], \n",
    "                       dependencies: Dict[str, List[Dict[str, any]]], skills: Dict[str, List[str]]) -> List[Dict[str, any]]:\n",
    "        \"\"\"Format results to match individual processor output\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for user_story, tasks in user_stories_tasks.items():\n",
    "            task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "            \n",
    "            formatted_tasks = []\n",
    "            total_story_points = 0\n",
    "            \n",
    "            for i, task in enumerate(tasks):\n",
    "                task_id = task_ids[i]\n",
    "                task_points = story_points.get(task, 3)\n",
    "                task_skills = skills.get(task, [\"general_development\"])\n",
    "                task_dependencies = dependencies.get(task, [])\n",
    "                \n",
    "                formatted_dependencies = []\n",
    "                for dep in task_dependencies:\n",
    "                    dep_task = dep[\"task_id\"]\n",
    "                    for other_story, other_tasks in user_stories_tasks.items():\n",
    "                        if dep_task in other_tasks:\n",
    "                            dep_index = other_tasks.index(dep_task)\n",
    "                            dep_task_ids = self._generate_task_ids(other_story, len(other_tasks))\n",
    "                            dep_task_id = dep_task_ids[dep_index]\n",
    "                            formatted_dependencies.append({\n",
    "                                \"task_id\": dep_task_id,\n",
    "                                \"rework_effort\": dep[\"rework_effort\"]\n",
    "                            })\n",
    "                            break\n",
    "                \n",
    "                formatted_tasks.append({\n",
    "                    \"description\": task,\n",
    "                    \"id\": task_id,\n",
    "                    \"story_points\": task_points,\n",
    "                    \"depends_on\": formatted_dependencies,\n",
    "                    \"required_skills\": task_skills\n",
    "                })\n",
    "                \n",
    "                total_story_points += task_points\n",
    "            \n",
    "            result = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": formatted_tasks\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _format_output(self, results: List[Dict[str, any]]) -> str:\n",
    "        \"\"\"Format results for display - matching individual processor format\"\"\"\n",
    "        output = []\n",
    "        \n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(\"TOKEN USAGE SUMMARY\")\n",
    "        output.append(\"=\" * 80)\n",
    "        \n",
    "        summary = token_tracker.get_summary()\n",
    "        if summary:\n",
    "            breakdown = summary.get(\"breakdown\", {})\n",
    "            output.append(f\"TOTAL TOKENS CONSUMED: {breakdown.get('total_consumed', 0)}\")\n",
    "            \n",
    "            cost_data = summary.get(\"cost_estimate\", {})\n",
    "            if cost_data:\n",
    "                output.append(f\"ESTIMATED COST: ${cost_data['total_cost']:.6f}\")\n",
    "            \n",
    "            output.append(\"\")\n",
    "        \n",
    "        output.append(\"=\" * 80)\n",
    "        output.append(\"PROCESSED USER STORIES\")\n",
    "        output.append(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            output.append(f\"\\n--- Story {i} ---\")\n",
    "            if \"error\" in result:\n",
    "                output.append(f\"âŒ Error: {result['error']}\")\n",
    "            else:\n",
    "                formatted_json = json.dumps(result, indent=2)\n",
    "                output.append(formatted_json)\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "# FUNCTION TO RUN GROUPED PROCESSING\n",
    "async def run_grouped_with_full_details(group_size: int = 2):\n",
    "    \"\"\"Run the grouped processor and display all detailed results immediately\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ PARTY B: GROUPED PROCESSING WITH FULL DETAILS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Philosophy: 'Smart batching for balanced efficiency'\")\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Reset token tracker\n",
    "    global token_tracker\n",
    "    token_tracker = TokenTracker()\n",
    "    \n",
    "    user_stories = SAMPLE_USER_STORIES\n",
    "    print(f\"Processing {len(user_stories)} sample user stories in groups of {group_size}...\")\n",
    "    \n",
    "    processor = GroupedProcessor(group_size=group_size)\n",
    "    result = await processor.process_stories_with_details(user_stories)\n",
    "    \n",
    "    if result and \"error\" not in result:\n",
    "        print(\"\\nâœ… Grouped processing completed successfully!\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ FINAL SUMMARY:\")\n",
    "        print(f\"   â€¢ Total User Stories: {len(SAMPLE_USER_STORIES)}\")\n",
    "        \n",
    "        total_tasks = sum(len(r.get('output', {}).get('tasks', [])) for r in result if 'output' in r)\n",
    "        total_story_points = sum(r.get('output', {}).get('story_points', 0) for r in result if 'output' in r)\n",
    "        total_dependencies = 0\n",
    "        total_skills = 0\n",
    "        \n",
    "        for r in result:\n",
    "            if 'output' in r:\n",
    "                for task in r['output'].get('tasks', []):\n",
    "                    total_dependencies += len(task.get('depends_on', []))\n",
    "                    total_skills += len(task.get('required_skills', []))\n",
    "        \n",
    "        # Calculate API calls based on group processing\n",
    "        groups_count = len(SAMPLE_USER_STORIES) // group_size + (1 if len(SAMPLE_USER_STORIES) % group_size else 0)\n",
    "        api_calls = groups_count + 3  # groups + story points + dependencies + skills\n",
    "        \n",
    "        print(f\"   â€¢ Total Tasks Generated: {total_tasks}\")\n",
    "        print(f\"   â€¢ Total Story Points: {total_story_points}\")\n",
    "        print(f\"   â€¢ Total Dependencies: {total_dependencies}\")\n",
    "        print(f\"   â€¢ Total Skills Identified: {total_skills}\")\n",
    "        print(f\"   â€¢ API Calls: {api_calls} (balanced efficiency!)\")\n",
    "        print(f\"   â€¢ Tasks per API call: {total_tasks / api_calls:.1f}\")\n",
    "        \n",
    "        summary = token_tracker.get_summary()\n",
    "        if summary:\n",
    "            breakdown = summary.get(\"breakdown\", {})\n",
    "            total_tokens = breakdown.get('total_consumed', 0)\n",
    "            cost_data = summary.get(\"cost_estimate\", {})\n",
    "            print(f\"   â€¢ Total Tokens Used: {total_tokens}\")\n",
    "            if cost_data:\n",
    "                print(f\"   â€¢ Total Cost: ${cost_data['total_cost']:.6f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nâŒ Grouped processing failed!\")\n",
    "        if result and \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Main execution function for grouped processing\n",
    "async def main():\n",
    "    \"\"\"Main function to run the grouped processor\"\"\"\n",
    "    print(\"ðŸš€ Enhanced Grouped Processor for User Stories\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run grouped processing with sample stories\n",
    "    result = await run_grouped_with_full_details(group_size=2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# For Jupyter notebook execution\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    try:\n",
    "        nest_asyncio.apply()\n",
    "        result = asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        result = await main()\n",
    "else:\n",
    "    print(\"ðŸ“ Grouped Processing Script loaded successfully!\")\n",
    "    print(\"ðŸš€ To run grouped processing: await main() or await run_grouped_with_full_details(group_size=2)\")\n",
    "    print(\"ðŸ“Š This will process user stories in small groups for balanced efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a121200-0cd3-45e8-9433-fa05e6f70429",
   "metadata": {},
   "source": [
    "**Grouped Processing Performance Analysis**\n",
    "- Processing Time\n",
    "> **Performance:** â­â­â­ (Balanced)\n",
    "- Characteristics :\n",
    "  \n",
    "> Moderate parallelization through grouping\n",
    "\n",
    "> Reduced API round-trips\n",
    "\n",
    "> Configurable group sizes for optimization\n",
    "- Expected Range\n",
    "> **8-15 seconds for 10 stories (groups of 3)*\n",
    "---\n",
    "- ðŸ”¤ Token Usage\n",
    "> **Efficiency:** â­â­â­â­ (Balanced)\n",
    "- Estimated Tokens\n",
    "> **~600-800 per story + analysis overhead**\n",
    "---\n",
    "- API Costs\n",
    "> **Cost Rating:** â­â­â­â­ (Moderate)\n",
    "\n",
    "- Cost Structure\n",
    "> **6 API calls ** (vs. 4 for batch-all)\n",
    "---\n",
    "- Dependency Detection Accuracy\n",
    "> **Accuracy:** â­â­â­â­ (High)\n",
    "- Advantages\n",
    "  \n",
    "> Good task breakdown within groups\n",
    "\n",
    "> Minimal context dilution\n",
    "\n",
    "> Balanced detail vs. efficiency\n",
    "- Trade-offs\n",
    "  \n",
    "> Slightly less granular than individual processing\n",
    "\n",
    "> Bad inter-group dependency mapping\n",
    "\n",
    "---\n",
    "- Error Recovery Capability\n",
    "> **Resilience:** â­â­â­ (Moderate)\n",
    "- Recovery Features\n",
    "  \n",
    "> **Group Isolation:** Failed group doesn't affect others\n",
    "\n",
    "> **Partial Processing:** Can continue with successful groups\n",
    "\n",
    "> **Moderate Debugging:** Can isolate issues to specific groups\n",
    "\n",
    ">**No retries:** Failed groups required full reprocessing.\n",
    "---\n",
    "- ðŸ“Š Summary\n",
    "- Best Performance Aspects\n",
    "  \n",
    "> **âœ… Balanced efficiency vs. reliability**\n",
    "\n",
    "> **âœ… Scalable to large story sets**\n",
    "\n",
    "> **âœ… Error resilience per group**\n",
    "\n",
    "> **âœ… Configurable group sizes**\n",
    "- Trade-offs\n",
    "  \n",
    "> **âš ï¸ More complex than batch-all**\n",
    "\n",
    "> **âš ï¸ Missed cross-group deps**\n",
    "\n",
    "> **âš ï¸ Risk of rate limits persists** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246adda9-d01d-463b-9089-f904cc1896cd",
   "metadata": {},
   "source": [
    "### 2.2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a5c28-3c78-479c-ba90-8f4ae32d8281",
   "metadata": {},
   "source": [
    "#### Processing Methods Performance Comparison :\n",
    "\n",
    "\n",
    "| **Criteria** | **Individual Processing** | **Grouped Processing** | **Batch All Processing** |\n",
    "|--------------|---------------------------|------------------------|--------------------------|\n",
    "| **Processing Time** | â­â­ (Slowest)<br>53-106 seconds for 10 stories<br>Sequential | â­â­â­ (Balanced)<br>8-15 seconds for 10 stories<br>Moderate parallelization | â­â­â­â­â­ (Fastest)<br>5-10 seconds for 10 stories<br>Maximum parallelization |\n",
    "| **Output Token Usage** | â­â­ (Highest Usage)<br>Maximum token consumption<br>Redundant context transmission | â­â­â­â­ (Balanced)<br>~600-800 per story<br>| â­â­â­â­â­ (Most Efficient)<br>~623.6 per story<br>82% cheaper output tokens |\n",
    "| **Input Token Usage** | â­â­â­â­ (Low Usage)<br>Single story per API call<br>Minimal context per request<br>Small input payload each time | â­â­â­ (Moderate)<br>Multiple stories per group<br>Moderate context size | â­ (Highest Usage)<br>ALL stories in single context<br>Maximum input token load<br>Risk of hitting token limits |\n",
    "| **API Costs** | â­ (Most Expensive)<br>~4.2 API calls per story<br> | â­â­â­â­ (Moderate)<br>4 API calls total per group<br>| â­â­â­â­â­ (Cheapest)<br>4 API calls total<br>|\n",
    "| **Dependency Detection** | â­â­â­â­â­ (Highest)<br>Individual attention per story<br>No context dilution<br>Poor inter-user story dependencies detection | â­â­â­â­ (High)<br>Good task breakdown<br>Minimal context dilution<br>Poor inter-group mapping | â­â­â­ (Variable)<br>Context dilution risk<br>Works best with <15 stories<br>May miss subtle dependencies |\n",
    "| **Error Recovery** | â­â­â­â­ (Good)<br>Error isolation per story<br>Individual story debugging<br>Partial processing possible | â­â­â­ (Moderate)<br>Group isolation<br>Partial processing<br>No retry mechanism | â­â­ (Risky)<br>Single point of failure<br>All-or-nothing processing<br>Difficult debugging |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751fbaa-c298-4c34-803b-291002c2c4b4",
   "metadata": {},
   "source": [
    "## 3. Data structure\n",
    "The processing system works with a hierarchical JSON structure that organizes user stories by project and breaks them down into actionable tasks with dependencies and effort estimation.\n",
    "### 3.1 Root Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98dbd148-30e0-401d-b9d1-5d07e8697b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'project': 'project 1', 'user_stories': [Ellipsis]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\n",
    "    \"project\": \"project 1\",\n",
    "    \"user_stories\": [...]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d04bb-2fa9-449f-b337-be15b835181a",
   "metadata": {},
   "source": [
    "### 3.2 User Story Structure\n",
    "Each user story contains an input (original story) and output (processed breakdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "399bd653-4a69-4065-a235-94ce7cdbd889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'As a team member, I want to see the first iteration of beta up on cloud.gov',\n",
       " 'output': {'story_points': 13, 'tasks': [Ellipsis]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"input\": \"As a team member, I want to see the first iteration of beta up on cloud.gov\",\n",
    "  \"output\": {\n",
    "    \"story_points\": 13,\n",
    "    \"tasks\": [...]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef714b-8a3c-4fb1-9683-cfb4f15a78ce",
   "metadata": {},
   "source": [
    "### 3.3 Task Breakdown Structure\n",
    "Each task represents a specific, actionable work item with detailed metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2392753c-ae28-43af-a84c-57261bfc06bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Set up cloud.gov account and project space',\n",
       " 'id': 'CLD_001',\n",
       " 'story_points': 3,\n",
       " 'depends_on': [],\n",
       " 'required_skills': ['devops', 'cloud_platforms', 'account_management']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"description\": \"Set up cloud.gov account and project space\",\n",
    "  \"id\": \"CLD_001\",\n",
    "  \"story_points\": 3,\n",
    "  \"depends_on\": [],\n",
    "  \"required_skills\": [\"devops\", \"cloud_platforms\", \"account_management\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902eb34-046b-44e1-8bea-18c8f838d971",
   "metadata": {},
   "source": [
    "### 3.4 Dependency Structure\n",
    "Tasks can depend on other tasks with associated rework effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7822b27-c5f4-4697-9b4e-51e60c529c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Configure deployment pipeline',\n",
       " 'id': 'CLD_002',\n",
       " 'story_points': 4,\n",
       " 'depends_on': [{'task_id': 'CLD_001', 'reward_effort': 2}],\n",
       " 'required_skills': ['devops', 'ci_cd', 'pipeline_configuration']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"description\": \"Configure deployment pipeline\",\n",
    "  \"id\": \"CLD_002\",\n",
    "  \"story_points\": 4,\n",
    "  \"depends_on\": [\n",
    "    {\n",
    "      \"task_id\": \"CLD_001\",\n",
    "      \"rework_effort\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"required_skills\": [\"devops\", \"ci_cd\", \"pipeline_configuration\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0449b28-59c9-4bb3-9541-b856006d10f7",
   "metadata": {},
   "source": [
    "### 3.5 Field Definitions\n",
    "\n",
    "| **Field** | **Type** | **Description** | **Example** |\n",
    "|-----------|----------|-----------------|-------------|\n",
    "| `project` | String | Project identifier/name | `\"project 1\"` |\n",
    "| `input` | String | Original user story text | `\"As a user, I want...\"` |\n",
    "| `story_points` | Integer | Total effort estimation for the story | `13` |\n",
    "| `description` | String | Specific task description | `\"Set up cloud.gov account\"` |\n",
    "| `id` | String | Unique task identifier (prefix + number) | `\"CLD_001\"` |\n",
    "| `depends_on` | Array | List of task dependencies | `[{task_id: \"CLD_001\", rework_effort: 2}]` |\n",
    "| `required_skills` | Array | Skills needed to complete the task | `[\"devops\", \"cloud_platforms\"]` |\n",
    "| `rework_effort` | Integer | Effort needed if dependency changes | `2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2317ac0-ce9d-4cb3-a858-d78e3543f996",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering Architecture & Implementation\n",
    "\n",
    "### 4.1 Architecture Decision: Multi-Agent vs Single Agent\n",
    "\n",
    "#### 4.1.1 Single Agent Approach\n",
    "> **One Agent handles all steps: decomposition, dependency analysis, and skill mapping**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Simpler implementation\n",
    "  - No coordination overhead\n",
    "  - Single point of control\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Limited specialization\n",
    "  - Potential context overload\n",
    "  - Reduced modularity\n",
    "\n",
    "\n",
    "#### 4.1.2 Multi-Agent System \n",
    "> **A collaborative system of specialized LLM-based agents, each optimized for a distinct responsibility**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Modularity:** Each agent specializes in one task\n",
    "  - **Parallelism:** Agents can work simultaneously \n",
    "  - **Token Optimization:** Focused context per agent\n",
    "  - **Scalability:** Easy to add/modify individual agents\n",
    "    \n",
    "---\n",
    "#### 4.1.3 **Decision Rationale:**\n",
    "> **We chose the multi-agent system for modularity, parallelism and token optimization**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Multi-Agent Workflow Architecture\n",
    "\n",
    "### 4.2.1 Agent Composition\n",
    "- **Task Extractor Agent**\n",
    "  \n",
    "> **Input:** User stories\n",
    "  \n",
    "> **Output:** Raw tasks with descriptions and IDs\n",
    "  \n",
    "> **Responsibility:** Break down user stories into actionable tasks with unique identifiers\n",
    "\n",
    "- **Story Point Estimator Agent**\n",
    "  \n",
    "> **Input:** Individual task\n",
    "  \n",
    "> **Output:** Story point estimation\n",
    "  \n",
    "> **Responsibility:** Estimate effort required for each task (1-5 points typically)\n",
    "\n",
    "- **Required Skills Agent**\n",
    "  \n",
    "> **Input:** Individual task\n",
    "  \n",
    "> **Output:** Required skills array\n",
    "  \n",
    "> **Responsibility:** Identify technical skills needed for task completion\n",
    "\n",
    "- **Dependency Agent**\n",
    "  \n",
    "> **Input:** All refined tasks with story points and skills\n",
    "  \n",
    "> **Output:** Task dependencies with rework effort\n",
    "  \n",
    "> **Responsibility:** Determine task dependencies and calculate rework effort\n",
    "\n",
    "- **Format Validator Agent**\n",
    "  \n",
    "> **Input:** Complete task breakdown\n",
    "  \n",
    "> **Output:** Validated JSON structure\n",
    "  \n",
    "> **Responsibility:** Ensure output conforms to required data structure\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2.2 Workflow Process\n",
    "- **Data Flow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79d6f7-4816-4490-a6db-c0f01b82314b",
   "metadata": {},
   "source": [
    "![LLM Multi-Agents Pipeline](llm_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9042808-1a68-42cd-8484-0f1d91b6f86e",
   "metadata": {},
   "source": [
    "**Process Steps:**\n",
    "\n",
    "1. **Task Extraction:** User stories processed by Task Extractor Agent to generate raw tasks\n",
    "2. **Parallel Task Enhancement:** For each extracted task simultaneously:\n",
    "   - Story Point Estimator Agent calculates effort estimation\n",
    "   - Required Skills Agent identifies necessary skills\n",
    "3. **Dependency Analysis:** Dependency Agent processes all enriched tasks to identify relationships and rework effort\n",
    "4. **Format Validation:** Format Validator Agent ensures final output matches required JSON structure\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61963a-a99f-4bae-a7ba-02b7104a8ade",
   "metadata": {},
   "source": [
    "### 4.3 Model Selection & Deployment Strategy\n",
    "\n",
    "#### 4.3.1 Local Models\n",
    "> **Run on-premises but resource heavy**\n",
    "\n",
    "- **Advantages:**\n",
    "  - Full data control and privacy\n",
    "  - No API rate limits\n",
    "  - One-time setup cost\n",
    "  - Offline capability\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - High computational requirements\n",
    "  - Significant hardware investment\n",
    "  - Model maintenance overhead\n",
    "  - Limited to available local resources\n",
    "\n",
    "#### 4.3.2 API-Based Models  \n",
    "> **Served by third-party providers but the free plan is constrained**\n",
    "\n",
    "- **Advantages:**\n",
    "  - No infrastructure setup required\n",
    "  - Access to state-of-the-art models\n",
    "  - Automatic updates and maintenance\n",
    "  - Scalable on-demand\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - API rate limits on free tiers\n",
    "  - Data privacy considerations\n",
    "  - Ongoing API costs\n",
    "  - Internet dependency\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> **We chose Llama3-70B on Groq for both performance and prompt length advantages**\n",
    "\n",
    "> **Why Llama3-70B?**\n",
    "\n",
    "- **High Performance:** 70B parameters provide excellent reasoning\n",
    "- **Long Context Window:** Supports complex multi-story processing\n",
    "- **Open Source:** Flexible deployment options\n",
    "- **Proven Track Record:** Well-tested for text generation tasks\n",
    "\n",
    "> **Alternative options**\n",
    "- **Local deployment** for data-sensitive applications\n",
    "- **Other API providers** for comparison and redundancy\n",
    "- **Model fine-tuning** for domain-specific improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e4876-dcb3-4859-9b9a-64f8b0f42348",
   "metadata": {},
   "source": [
    "### configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b197091-5e45-4fc2-a28e-3d950deb27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117bf31-311a-4df9-8fa6-3e88faba105c",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "- **Model:** llama3-70b-8192\n",
    "- **Temperature:** 0.3 (balanced creativity/consistency)\n",
    "- **Max Tokens:** 4000 (supports large responses)\n",
    "- **Context Window:** 8192 tokens (sufficient for multi-story processing)\n",
    "- **Rate limit:** 30 reauest per minute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027dbfe-c396-4de4-a0a7-2b93860930b3",
   "metadata": {},
   "source": [
    "### 4.4 Prompt Engineering Strategies\n",
    "\n",
    "#### 4.4.1 Strategy 1: Zero-Shot Prompting\n",
    "\n",
    "- **Definition**\n",
    "> **Asking the model to perform a task without providing any examples or prior context, relying entirely on the model's pre-trained knowledge**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Direct task instruction** without examples\n",
    "\n",
    "> **Relies on model's inherent capabilities**\n",
    "\n",
    ">  **Minimal context and guidance**\n",
    "\n",
    "> **Straightforward task description**\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> Shortest possible prompts reduce API costs and token usage\n",
    "\n",
    "> Good for straightforward, well-defined tasks\n",
    "\n",
    "> Fast execution\n",
    "\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> Less control over output format and structure\n",
    "\n",
    "> May produce inconsistent results across runs\n",
    "\n",
    "> Output format may vary without examples\n",
    "\n",
    "> Complex tasks may be misunderstood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031b96e-a369-4394-8391-5f8854972351",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3d59b8d-69ad-4dcf-9bd9-ad62efff64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stories = [\n",
    "    \"As a user, I want to create an account so that I can access personalized features\",\n",
    "    \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38b9998f-f831-48cd-a54d-8793eacb4430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Processing 2 user stories...\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to create an account so that I can access ...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 125, Output: 84, Total: 209\n",
      "âœ“ Extracted 7 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 224, Output: 65, Total: 289\n",
      "âœ“ Estimated story points for 7 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 120, Output: 22, Total: 142\n",
      "[REQUIRED_SKILLS] Tokens - Input: 114, Output: 16, Total: 130\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 29, Total: 142\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 30, Total: 143\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 27, Total: 140\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 22, Total: 135\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 19, Total: 132\n",
      "âœ“ Identified skills for 7 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 254, Output: 121, Total: 375\n",
      "âœ“ Analyzed dependencies for 5 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to view analytics dashboard so that I ca...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 125, Output: 54, Total: 179\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 194, Output: 49, Total: 243\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 112, Output: 29, Total: 141\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 21, Total: 134\n",
      "[REQUIRED_SKILLS] Tokens - Input: 115, Output: 30, Total: 145\n",
      "[REQUIRED_SKILLS] Tokens - Input: 111, Output: 23, Total: 134\n",
      "[REQUIRED_SKILLS] Tokens - Input: 114, Output: 25, Total: 139\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 217, Output: 251, Total: 468\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "âœ… Completed processing 2 stories\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 3420\n",
      "ESTIMATED COST: $0.043370\n",
      "\n",
      "================================================================================\n",
      "PROCESSED USER STORIES\n",
      "================================================================================\n",
      "\n",
      "--- Story 1 ---\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 28,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design user registration form with required fields (username, email, password, etc.)\",\n",
      "        \"id\": \"UCA_001\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"html\",\n",
      "          \"css\",\n",
      "          \"ui_design\",\n",
      "          \"user_experience\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement email validation logic to ensure valid email addresses\",\n",
      "        \"id\": \"UCA_002\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"regex\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create password strength checker with minimum password requirements\",\n",
      "        \"id\": \"UCA_003\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Create password strength checker with minimum password requirements\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop backend API to handle user registration requests\",\n",
      "        \"id\": \"UCA_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"api_development\",\n",
      "          \"backend_development\",\n",
      "          \"authentication\",\n",
      "          \"database_integration\",\n",
      "          \"security_best_practices\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate API with frontend registration form submission\",\n",
      "        \"id\": \"UCA_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"api_development\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"react\",\n",
      "          \"http_protocol\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement user account creation logic in the database\",\n",
      "        \"id\": \"UCA_006\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_design\",\n",
      "          \"sql\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send confirmation email to users upon successful registration\",\n",
      "        \"id\": \"UCA_007\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_006\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"email_integration\",\n",
      "          \"api_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Story 2 ---\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 23,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout and user interface\",\n",
      "        \"id\": \"AVA_001\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_design\",\n",
      "          \"human_computer_interaction\",\n",
      "          \"data_visualization\",\n",
      "          \"frontend_development\",\n",
      "          \"html_css\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop data visualization components for key performance metrics\",\n",
      "        \"id\": \"AVA_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"d3js\",\n",
      "          \"react\",\n",
      "          \"data_analysis\",\n",
      "          \"frontend_development\",\n",
      "          \"charting_library_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate data retrieval API to fetch system performance data\",\n",
      "        \"id\": \"AVA_003\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"api_development\",\n",
      "          \"data_retrieval\",\n",
      "          \"api_integration\",\n",
      "          \"json\",\n",
      "          \"restful_api\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement dashboard filtering and sorting functionality\",\n",
      "        \"id\": \"AVA_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"react\",\n",
      "          \"frontend_development\",\n",
      "          \"ui_component_design\",\n",
      "          \"data_binding\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create data caching mechanism to improve dashboard loading speed\",\n",
      "        \"id\": \"AVA_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"caching\",\n",
      "          \"backend_development\",\n",
      "          \"data_structures\",\n",
      "          \"algorithm_design\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks from user story\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task extraction specialist. Break down this user story into specific, actionable tasks.\n",
    "\n",
    "Requirements:\n",
    "- Minimum 2 tasks, maximum 7 tasks\n",
    "- Each task should be concise (10-30 words)\n",
    "- Tasks must be clear and actionable\n",
    "- Each task focus on a single responsibility \n",
    "- Avoid unnecessary subdivision\n",
    "\n",
    "Return ONLY a clean numbered list of tasks, no headers or explanations.\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Example format:\n",
    "1. Design user registration form\n",
    "2. Implement email validation logic\n",
    "3. Create password strength checker\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "        \n",
    "        tasks = self._parse_tasks(output_text)\n",
    "        print(f\"âœ“ Extracted {len(tasks)} tasks\")\n",
    "        return tasks\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'user story:', 'tasks:', 'here are', 'the following', \n",
    "                'broken down', 'example format:', '**'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "            clean_task = clean_task.strip()\n",
    "            \n",
    "            if clean_task and len(clean_task) > 10:\n",
    "                tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2a: Estimate story points for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Consider:\n",
    "- Complexity of implementation\n",
    "- Time required\n",
    "- Risk and uncertainty\n",
    "- Dependencies on external systems\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "Task 3: Z points\n",
    "\n",
    "Where X, Y, Z are numbers from the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "        \n",
    "        points = self._parse_story_points(output_text, tasks)\n",
    "        print(f\"âœ“ Estimated story points for {len(points)} tasks\")\n",
    "        total_points = sum(points.values())\n",
    "        return {\n",
    "        'total_story_points': total_points,\n",
    "        'task_points': points,\n",
    "        'estimated_sum': total_points\n",
    "    }\n",
    "    \n",
    "    def _parse_story_points(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    # Extract task number and points\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    # Find closest valid point\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default points\n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3  # Default moderate complexity\n",
    "        \n",
    "        return points\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 2b: Identify required skills for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task\"\"\"\n",
    "        user_story = \"General task completion\"\n",
    "        tasks = [task]\n",
    "        tasks_str = \"1. \" + task\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify the specific skills required for each task.\n",
    "\n",
    "Consider:\n",
    "- Programming languages\n",
    "- Frameworks and tools\n",
    "- Domain expertise\n",
    "- Technical disciplines (frontend, backend, database, etc.)\n",
    "- Soft skills when relevant\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\n",
    "\n",
    "Use concise skill names like: javascript, react, database_design, api_development, user_research, etc.\n",
    "\"\"\"\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "    \n",
    "    # Parse for single task\n",
    "        skills_map = self._parse_skills(output_text, tasks)\n",
    "        return skills_map.get(task, [\"general_development\"])\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "    \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "    \n",
    "        print(f\"âœ“ Identified skills for {len(skills_map)} tasks\")\n",
    "        return skills_map\n",
    "\n",
    "    def _parse_skills(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Parse skills\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 3: Analyze dependencies between tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        tasks_with_points = []\n",
    "        for i, task in enumerate(tasks):\n",
    "            points = story_points.get(task, 3)\n",
    "            tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "        \n",
    "        tasks_str = \"\\n\".join(tasks_with_points)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "Consider:\n",
    "- Logical workflow order\n",
    "- Technical dependencies\n",
    "- Data flow requirements\n",
    "- Integration points\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "For each dependency, estimate rework_effort (1-3):\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed\n",
    "- 3: High rework effort required\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "Task A depends on Task B (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies. Don't create artificial ones.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "        \n",
    "        dependencies = self._parse_dependencies(output_text, tasks)\n",
    "        print(f\"âœ“ Analyzed dependencies for {len(dependencies)} tasks\")\n",
    "        return dependencies\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers\n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            # Validate rework_effort\n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2  # Default\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "\n",
    "class FormatValidatorAgent:\n",
    "    \"\"\"Step 4: Validate and format final output\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def validate_and_format(self, user_story: str, tasks: List[str], \n",
    "                                 story_points: Dict[str, int], skills: Dict[str, List[str]], \n",
    "                                 dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, any]:\n",
    "        \n",
    "        # Generate task IDs\n",
    "        task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "        \n",
    "        # Build final structure\n",
    "        formatted_tasks = []\n",
    "        total_story_points = 0\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = task_ids[i]\n",
    "            task_points = story_points.get(task, 3)\n",
    "            task_skills = skills.get(task, [\"general_development\"])\n",
    "            task_dependencies = dependencies.get(task, [])\n",
    "            \n",
    "            # Convert dependencies to use task IDs\n",
    "            formatted_dependencies = []\n",
    "            for dep in task_dependencies:\n",
    "                dep_task = dep[\"task_id\"]\n",
    "                if dep_task in tasks:\n",
    "                    dep_index = tasks.index(dep_task)\n",
    "                    dep_task_id = task_ids[dep_index]\n",
    "                    formatted_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            formatted_tasks.append({\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": task_points,\n",
    "                \"depends_on\": formatted_dependencies,\n",
    "                \"required_skills\": task_skills\n",
    "            })\n",
    "            \n",
    "            total_story_points += task_points\n",
    "        \n",
    "        result = {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"story_points\": total_story_points,\n",
    "                \"tasks\": formatted_tasks\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            json.dumps(result)\n",
    "            print(\"âœ“ Format validation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Format validation failed: {e}\")\n",
    "            # Apply fixes if needed\n",
    "            result = self._fix_json_issues(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        # Extract key words from user story to create meaningful prefix\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        # Take first few significant words and create acronym\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        # Ensure prefix is exactly 3 characters\n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        \n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _fix_json_issues(self, result: Dict[str, any]) -> Dict[str, any]:\n",
    "        # Implement basic JSON fixes\n",
    "        try:\n",
    "            # Convert any non-serializable items\n",
    "            json_str = json.dumps(result, default=str)\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return result\n",
    "\n",
    "class UserStoryPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_extractor = TaskExtractorAgent()\n",
    "        self.story_point_estimator = StoryPointEstimatorAgent()\n",
    "        self.skills_agent = RequiredSkillsAgent()\n",
    "        self.dependency_agent = DependencyAgent()\n",
    "        self.format_validator = FormatValidatorAgent()\n",
    "    \n",
    "    async def process_story(self, user_story: str) -> Dict[str, any]:\n",
    "        try:\n",
    "            print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "            \n",
    "            # Step 1: Extract tasks\n",
    "            print(\"  Step 1: Extracting tasks...\")\n",
    "            tasks = await self.task_extractor.decompose(user_story)\n",
    "            \n",
    "            if not tasks:\n",
    "                raise ValueError(\"No tasks extracted from user story\")\n",
    "            \n",
    "            # Step 2: Parallel processing of story points and skills\n",
    "            print(\"  Step 2: Estimating story points and identifying skills...\")\n",
    "            story_points_results, skills = await asyncio.gather(\n",
    "                self.story_point_estimator.estimate_story_points(user_story, tasks),\n",
    "                self.skills_agent.identify_skills(user_story, tasks)\n",
    "            )\n",
    "            \n",
    "            story_points= story_points_results['task_points']\n",
    "            # Step 3: Analyze dependencies\n",
    "            print(\"  Step 3: Analyzing dependencies...\")\n",
    "            dependencies = await self.dependency_agent.analyze_dependencies(\n",
    "                user_story, tasks, story_points\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format and validate\n",
    "            print(\"  Step 4: Formatting and validating...\")\n",
    "            result = await self.format_validator.validate_and_format(\n",
    "                user_story, tasks, story_points, skills, dependencies\n",
    "            )\n",
    "            \n",
    "            print(\"  âœ… Story processing complete!\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing story: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"error\": str(e),\n",
    "                \"output\": None\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_stories(self, user_stories: List[str]) -> List[Dict[str, any]]:\n",
    "        print(f\"\\nðŸš€ Processing {len(user_stories)} user stories...\")\n",
    "        \n",
    "        # Reset token tracker\n",
    "        global token_tracker\n",
    "        token_tracker = TokenTracker()\n",
    "        \n",
    "        # Process all stories\n",
    "        results = []\n",
    "        for story in user_stories:\n",
    "            result = await self.process_story(story)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Completed processing {len(results)} stories\")\n",
    "        return results\n",
    "\n",
    "def format_output(results: List[Dict[str, any]]) -> str:\n",
    "    \"\"\"Format results for display\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Token usage summary\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"TOKEN USAGE SUMMARY\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    if summary:\n",
    "        breakdown = summary.get(\"breakdown\", {})\n",
    "        output.append(f\"TOTAL TOKENS CONSUMED: {breakdown.get('total_consumed', 0)}\")\n",
    "        \n",
    "        cost_data = summary.get(\"cost_estimate\", {})\n",
    "        if cost_data:\n",
    "            output.append(f\"ESTIMATED COST: ${cost_data['total_cost']:.6f}\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Results\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"PROCESSED USER STORIES\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- Story {i} ---\")\n",
    "        if \"error\" in result:\n",
    "            output.append(f\"âŒ Error: {result['error']}\")\n",
    "        else:\n",
    "            # Pretty print JSON\n",
    "            formatted_json = json.dumps(result, indent=2)\n",
    "            output.append(formatted_json)\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    pipeline = UserStoryPipeline()\n",
    "    results = await pipeline.process_multiple_stories(stories_list)\n",
    "    print(format_output(results))\n",
    "    return results\n",
    "\n",
    "results = await run_pipeline(user_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75808c4a-2dbb-41cc-8130-97d6b256c298",
   "metadata": {},
   "source": [
    "- **Dependencies:**\n",
    "  \n",
    "> **Groq API:** Large Language Model provider using llama3-70b-8192 model\n",
    "\n",
    "> **tiktoken:** Token estimation using cl100k_base encoding\n",
    "\n",
    "> **python-dotenv:** Environment variable management\n",
    "\n",
    "> **asyncio:** Asynchronous programming support\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TokenTracker:** Monitors API usage across all pipeline stages, tracks input/output tokens per operation, estimates costs, and provides efficiency metrics using tiktoken's cl100k_base encoding.\n",
    "\n",
    "> **TaskExtractorAgent:** Decomposes user stories into 2-7 actionable tasks (10-30 words each) following single responsibility principle. Key method: decompose(user_story) -> List[str]\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Estimates task complexity using Fibonacci sequence [1,2,3,5,8,13]. Key method: estimate_story_points(user_story, tasks) -> Dict returns total points and per-task breakdown.\n",
    "\n",
    "> **RequiredSkillsAgent:** Maps technical skills to tasks including programming languages, frameworks, and domain expertise. Key method: identify_skills(user_story, tasks) -> Dict[str, List[str]]\n",
    "\n",
    "> **DependencyAgent:** Identifies inter-task dependencies with rework effort scoring (1-3 scale). Key method: analyze_dependencies(user_story, tasks, story_points) -> Dict returns dependency mappings.\n",
    "\n",
    "> **FormatValidatorAgent:** Generates meaningful task IDs from user story keywords and validates JSON output. Key method: validate_and_format(...) -> Dict produces final structured result.\n",
    "\n",
    "> **UserStoryPipeline:** Main orchestrator executing 4-step workflow: (1) Task extraction, (2) Parallel story points + skills analysis, (3) Dependency analysis, (4) Format validation. Key method: process_story(user_story) -> Dict\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **process_story(user_story: str):** Main pipeline method executing sequential workflow with parallel processing in step 2 for story points and skills identification using asyncio.gather().\n",
    "\n",
    "> **decompose(user_story: str):** Extracts 2-7 actionable tasks from user story using structured LLM prompts with parsing resilience for malformed responses.\n",
    "\n",
    "> **estimate_story_points(user_story, tasks):** Returns {'total_story_points': int, 'task_points': Dict, 'estimated_sum': int} using Fibonacci sequence with automatic correction for invalid values.\n",
    "\n",
    "> **identify_skills(user_story, tasks):** Maps technical skills per task returning {'task': ['skill1', 'skill2']} covering programming languages, frameworks, and domain expertise.\n",
    "\n",
    "> **analyze_dependencies(user_story, tasks, story_points):** Identifies task dependencies returning {'dependent_task': [{'task_id': 'prereq', 'rework_effort': 1-3}]} based on logical workflow and technical requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18352967-abee-4464-a992-2cc051de649b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Processing 2 user stories...\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to create an account so that I can access ...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 125, Output: 74, Total: 199\n",
      "âœ“ Extracted 6 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 214, Output: 57, Total: 271\n",
      "âœ“ Estimated story points for 6 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 120, Output: 23, Total: 143\n",
      "[REQUIRED_SKILLS] Tokens - Input: 114, Output: 21, Total: 135\n",
      "[REQUIRED_SKILLS] Tokens - Input: 114, Output: 16, Total: 130\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 31, Total: 144\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 18, Total: 131\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 27, Total: 140\n",
      "âœ“ Identified skills for 6 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 241, Output: 149, Total: 390\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to view analytics dashboard so that I ca...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 125, Output: 62, Total: 187\n",
      "âœ“ Extracted 6 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 201, Output: 57, Total: 258\n",
      "âœ“ Estimated story points for 6 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 112, Output: 30, Total: 142\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 24, Total: 137\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 19, Total: 132\n",
      "[REQUIRED_SKILLS] Tokens - Input: 111, Output: 23, Total: 134\n",
      "[REQUIRED_SKILLS] Tokens - Input: 113, Output: 26, Total: 139\n",
      "[REQUIRED_SKILLS] Tokens - Input: 112, Output: 32, Total: 144\n",
      "âœ“ Identified skills for 6 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 233, Output: 245, Total: 478\n",
      "âœ“ Analyzed dependencies for 5 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "âœ… Completed processing 2 stories\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 3434\n",
      "ESTIMATED COST: $0.043680\n",
      "\n",
      "================================================================================\n",
      "PROCESSED USER STORIES\n",
      "================================================================================\n",
      "\n",
      "--- Story 1 ---\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 26,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design user registration form with required fields (username, email, password, etc.)\",\n",
      "        \"id\": \"UCA_001\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"html\",\n",
      "          \"css\",\n",
      "          \"ui_design\",\n",
      "          \"user_experience\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement email validation logic to ensure valid email format\",\n",
      "        \"id\": \"UCA_002\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"regex\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create password strength checker to enforce strong password policy\",\n",
      "        \"id\": \"UCA_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"regex\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop backend API to handle user registration requests\",\n",
      "        \"id\": \"UCA_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"api_development\",\n",
      "          \"backend_development\",\n",
      "          \"authentication\",\n",
      "          \"database_design\",\n",
      "          \"python\",\n",
      "          \"nodejs\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate API with frontend registration form submission\",\n",
      "        \"id\": \"UCA_005\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"api_development\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement user account creation logic in backend API\",\n",
      "        \"id\": \"UCA_006\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_development\",\n",
      "          \"authentication\",\n",
      "          \"database_design\",\n",
      "          \"sql\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Story 2 ---\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 36,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout and user interface.\",\n",
      "        \"id\": \"AVA_001\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_design\",\n",
      "          \"human_computer_interaction\",\n",
      "          \"data_visualization\",\n",
      "          \"front_end_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop data visualization components for key performance metrics.\",\n",
      "        \"id\": \"AVA_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"d3js\",\n",
      "          \"react\",\n",
      "          \"data_analysis\",\n",
      "          \"frontend_development\",\n",
      "          \"visualization_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate data analytics API with the dashboard.\",\n",
      "        \"id\": \"AVA_003\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"api_development\",\n",
      "          \"dashboard_development\",\n",
      "          \"data_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data filtering and sorting functionality.\",\n",
      "        \"id\": \"AVA_004\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"frontend_development\",\n",
      "          \"data_structures\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create dashboard widgets for real-time system monitoring.\",\n",
      "        \"id\": \"AVA_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"react\",\n",
      "          \"dashboard_design\",\n",
      "          \"data_visualization\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop alert system for performance threshold breaches.\",\n",
      "        \"id\": \"AVA_006\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_development\",\n",
      "          \"data_analytics\",\n",
      "          \"threshold_algorithm_design\",\n",
      "          \"notification_system_integration\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks from user story\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task extraction specialist. Break down this user story into specific, actionable tasks.\n",
    "\n",
    "Requirements:\n",
    "- Minimum 2 tasks, maximum 7 tasks\n",
    "- Each task should be concise (10-30 words)\n",
    "- Tasks must be clear and actionable\n",
    "- Each task focus on a single responsibility \n",
    "- Avoid unnecessary subdivision\n",
    "\n",
    "Return ONLY a clean numbered list of tasks, no headers or explanations.\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Example format:\n",
    "1. Design user registration form\n",
    "2. Implement email validation logic\n",
    "3. Create password strength checker\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "        \n",
    "        tasks = self._parse_tasks(output_text)\n",
    "        print(f\"âœ“ Extracted {len(tasks)} tasks\")\n",
    "        return tasks\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'user story:', 'tasks:', 'here are', 'the following', \n",
    "                'broken down', 'example format:', '**'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "            clean_task = clean_task.strip()\n",
    "            \n",
    "            if clean_task and len(clean_task) > 10:\n",
    "                tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2a: Estimate story points for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Consider:\n",
    "- Complexity of implementation\n",
    "- Time required\n",
    "- Risk and uncertainty\n",
    "- Dependencies on external systems\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "Task 3: Z points\n",
    "\n",
    "Where X, Y, Z are numbers from the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "        \n",
    "        points = self._parse_story_points(output_text, tasks)\n",
    "        print(f\"âœ“ Estimated story points for {len(points)} tasks\")\n",
    "        total_points = sum(points.values())\n",
    "        return {\n",
    "        'total_story_points': total_points,\n",
    "        'task_points': points,\n",
    "        'estimated_sum': total_points\n",
    "    }\n",
    "    \n",
    "    def _parse_story_points(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    # Extract task number and points\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    # Find closest valid point\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default points\n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3  # Default moderate complexity\n",
    "        \n",
    "        return points\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 2b: Identify required skills for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task\"\"\"\n",
    "        user_story = \"General task completion\"\n",
    "        tasks = [task]\n",
    "        tasks_str = \"1. \" + task\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify the specific skills required for each task.\n",
    "\n",
    "Consider:\n",
    "- Programming languages\n",
    "- Frameworks and tools\n",
    "- Domain expertise\n",
    "- Technical disciplines (frontend, backend, database, etc.)\n",
    "- Soft skills when relevant\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\n",
    "\n",
    "Use concise skill names like: javascript, react, database_design, api_development, user_research, etc.\n",
    "\"\"\"\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "    \n",
    "    # Parse for single task\n",
    "        skills_map = self._parse_skills(output_text, tasks)\n",
    "        return skills_map.get(task, [\"general_development\"])\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "    \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "    \n",
    "        print(f\"âœ“ Identified skills for {len(skills_map)} tasks\")\n",
    "        return skills_map\n",
    "\n",
    "    def _parse_skills(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Parse skills\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 3: Analyze dependencies between tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        tasks_with_points = []\n",
    "        for i, task in enumerate(tasks):\n",
    "            points = story_points.get(task, 3)\n",
    "            tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "        \n",
    "        tasks_str = \"\\n\".join(tasks_with_points)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "Consider:\n",
    "- Logical workflow order\n",
    "- Technical dependencies\n",
    "- Data flow requirements\n",
    "- Integration points\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "For each dependency, estimate rework_effort (1-3):\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed\n",
    "- 3: High rework effort required\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "Task A depends on Task B (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies. Don't create artificial ones.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "        \n",
    "        dependencies = self._parse_dependencies(output_text, tasks)\n",
    "        print(f\"âœ“ Analyzed dependencies for {len(dependencies)} tasks\")\n",
    "        return dependencies\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers\n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            # Validate rework_effort\n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2  # Default\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "\n",
    "class FormatValidatorAgent:\n",
    "    \"\"\"Step 4: Validate and format final output\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def validate_and_format(self, user_story: str, tasks: List[str], \n",
    "                                 story_points: Dict[str, int], skills: Dict[str, List[str]], \n",
    "                                 dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, any]:\n",
    "        \n",
    "        # Generate task IDs\n",
    "        task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "        \n",
    "        # Build final structure\n",
    "        formatted_tasks = []\n",
    "        total_story_points = 0\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = task_ids[i]\n",
    "            task_points = story_points.get(task, 3)\n",
    "            task_skills = skills.get(task, [\"general_development\"])\n",
    "            task_dependencies = dependencies.get(task, [])\n",
    "            \n",
    "            # Convert dependencies to use task IDs\n",
    "            formatted_dependencies = []\n",
    "            for dep in task_dependencies:\n",
    "                dep_task = dep[\"task_id\"]\n",
    "                if dep_task in tasks:\n",
    "                    dep_index = tasks.index(dep_task)\n",
    "                    dep_task_id = task_ids[dep_index]\n",
    "                    formatted_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            formatted_tasks.append({\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": task_points,\n",
    "                \"depends_on\": formatted_dependencies,\n",
    "                \"required_skills\": task_skills\n",
    "            })\n",
    "            \n",
    "            total_story_points += task_points\n",
    "        \n",
    "        result = {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"story_points\": total_story_points,\n",
    "                \"tasks\": formatted_tasks\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            json.dumps(result)\n",
    "            print(\"âœ“ Format validation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Format validation failed: {e}\")\n",
    "            # Apply fixes if needed\n",
    "            result = self._fix_json_issues(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        # Extract key words from user story to create meaningful prefix\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        # Take first few significant words and create acronym\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        # Ensure prefix is exactly 3 characters\n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        \n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _fix_json_issues(self, result: Dict[str, any]) -> Dict[str, any]:\n",
    "        # Implement basic JSON fixes\n",
    "        try:\n",
    "            # Convert any non-serializable items\n",
    "            json_str = json.dumps(result, default=str)\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return result\n",
    "\n",
    "class UserStoryPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_extractor = TaskExtractorAgent()\n",
    "        self.story_point_estimator = StoryPointEstimatorAgent()\n",
    "        self.skills_agent = RequiredSkillsAgent()\n",
    "        self.dependency_agent = DependencyAgent()\n",
    "        self.format_validator = FormatValidatorAgent()\n",
    "    \n",
    "    async def process_story(self, user_story: str) -> Dict[str, any]:\n",
    "        try:\n",
    "            print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "            \n",
    "            # Step 1: Extract tasks\n",
    "            print(\"  Step 1: Extracting tasks...\")\n",
    "            tasks = await self.task_extractor.decompose(user_story)\n",
    "            \n",
    "            if not tasks:\n",
    "                raise ValueError(\"No tasks extracted from user story\")\n",
    "            \n",
    "            # Step 2: Parallel processing of story points and skills\n",
    "            print(\"  Step 2: Estimating story points and identifying skills...\")\n",
    "            story_points_results, skills = await asyncio.gather(\n",
    "                self.story_point_estimator.estimate_story_points(user_story, tasks),\n",
    "                self.skills_agent.identify_skills(user_story, tasks)\n",
    "            )\n",
    "            \n",
    "            story_points= story_points_results['task_points']\n",
    "            # Step 3: Analyze dependencies\n",
    "            print(\"  Step 3: Analyzing dependencies...\")\n",
    "            dependencies = await self.dependency_agent.analyze_dependencies(\n",
    "                user_story, tasks, story_points\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format and validate\n",
    "            print(\"  Step 4: Formatting and validating...\")\n",
    "            result = await self.format_validator.validate_and_format(\n",
    "                user_story, tasks, story_points, skills, dependencies\n",
    "            )\n",
    "            \n",
    "            print(\"  âœ… Story processing complete!\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing story: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"error\": str(e),\n",
    "                \"output\": None\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_stories(self, user_stories: List[str]) -> List[Dict[str, any]]:\n",
    "        print(f\"\\nðŸš€ Processing {len(user_stories)} user stories...\")\n",
    "        \n",
    "        # Reset token tracker\n",
    "        global token_tracker\n",
    "        token_tracker = TokenTracker()\n",
    "        \n",
    "        # Process all stories\n",
    "        results = []\n",
    "        for story in user_stories:\n",
    "            result = await self.process_story(story)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Completed processing {len(results)} stories\")\n",
    "        return results\n",
    "\n",
    "def format_output(results: List[Dict[str, any]]) -> str:\n",
    "    \"\"\"Format results for display\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Token usage summary\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"TOKEN USAGE SUMMARY\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    if summary:\n",
    "        breakdown = summary.get(\"breakdown\", {})\n",
    "        output.append(f\"TOTAL TOKENS CONSUMED: {breakdown.get('total_consumed', 0)}\")\n",
    "        \n",
    "        cost_data = summary.get(\"cost_estimate\", {})\n",
    "        if cost_data:\n",
    "            output.append(f\"ESTIMATED COST: ${cost_data['total_cost']:.6f}\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Results\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"PROCESSED USER STORIES\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- Story {i} ---\")\n",
    "        if \"error\" in result:\n",
    "            output.append(f\"âŒ Error: {result['error']}\")\n",
    "        else:\n",
    "            # Pretty print JSON\n",
    "            formatted_json = json.dumps(result, indent=2)\n",
    "            output.append(formatted_json)\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    pipeline = UserStoryPipeline()\n",
    "    results = await pipeline.process_multiple_stories(stories_list)\n",
    "    print(format_output(results))\n",
    "    return results\n",
    "\n",
    "results = await run_pipeline(user_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4746b-d2b8-4973-87fb-9aa51ebf95d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4.2 Strategy 2: Few-Shot Prompting\n",
    "\n",
    "- **Definition**\n",
    "> **Providing 2-5 examples of desired input-output pairs before asking the AI to perform the task on new data**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Demonstration-based learning** through examples\n",
    "\n",
    "> **Pattern recognition** from provided samples\n",
    "\n",
    "> **Structured guidance** for consistent output\n",
    "\n",
    "> **Balance between efficiency and control**\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Examples demonstrate exact output structure and style**\n",
    "\n",
    "> **Moderate token usage with significant performance **\n",
    "\n",
    "> **Examples guide the model toward predictable outputs**\n",
    "\n",
    "> **Demonstrations clarify complex or ambiguous requirements**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Can be biased toward the specific examples provided**\n",
    "\n",
    "> **May not generalize well to very different user story types**\n",
    "\n",
    "> **More tokens required than zero-shot approach**\n",
    "\n",
    "> **Quality depends on careful example curation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e6ba3-6fa5-4bb6-bfed-99242b91f614",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f8810b2-50fd-4f34-9349-d1803f9097b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Processing 2 user stories...\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to create an account so that I can access ...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 341, Output: 54, Total: 395\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 379, Output: 49, Total: 428\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 398, Output: 23, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 23, Total: 422\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 19, Total: 418\n",
      "[REQUIRED_SKILLS] Tokens - Input: 398, Output: 21, Total: 419\n",
      "[REQUIRED_SKILLS] Tokens - Input: 397, Output: 22, Total: 419\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 440, Output: 90, Total: 530\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to view analytics dashboard so that I ca...\n",
      "  Step 1: Extracting tasks...\n",
      "[TASK_EXTRACTION] Tokens - Input: 341, Output: 60, Total: 401\n",
      "âœ“ Extracted 5 tasks\n",
      "  Step 2: Estimating story points and identifying skills...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 385, Output: 49, Total: 434\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 22, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 23, Total: 422\n",
      "[REQUIRED_SKILLS] Tokens - Input: 399, Output: 28, Total: 427\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 21, Total: 421\n",
      "[REQUIRED_SKILLS] Tokens - Input: 400, Output: 21, Total: 421\n",
      "âœ“ Identified skills for 5 tasks\n",
      "  Step 3: Analyzing dependencies...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 446, Output: 73, Total: 519\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "  Step 4: Formatting and validating...\n",
      "âœ“ Format validation successful\n",
      "  âœ… Story processing complete!\n",
      "\n",
      "âœ… Completed processing 2 stories\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 6918\n",
      "ESTIMATED COST: $0.075160\n",
      "\n",
      "================================================================================\n",
      "PROCESSED USER STORIES\n",
      "================================================================================\n",
      "\n",
      "--- Story 1 ---\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 19,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design user registration form interface\",\n",
      "        \"id\": \"UCA_001\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_design\",\n",
      "          \"form_design\",\n",
      "          \"frontend\",\n",
      "          \"usability_testing\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement email validation and verification system\",\n",
      "        \"id\": \"UCA_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend\",\n",
      "          \"email_systems\",\n",
      "          \"validation\",\n",
      "          \"security_patterns\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create password strength requirements and validation\",\n",
      "        \"id\": \"UCA_003\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"security\",\n",
      "          \"password_management\",\n",
      "          \"validation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Build user profile creation workflow\",\n",
      "        \"id\": \"UCA_004\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_002\",\n",
      "            \"reward_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"UCA_003\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"workflow_design\",\n",
      "          \"user_management\",\n",
      "          \"backend\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add account activation process\",\n",
      "        \"id\": \"UCA_005\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"UCA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend\",\n",
      "          \"email_systems\",\n",
      "          \"validation\",\n",
      "          \"security\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Story 2 ---\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 29,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout and components\",\n",
      "        \"id\": \"AVA_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_design\",\n",
      "          \"dashboard_design\",\n",
      "          \"data_visualization\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data collection and aggregation system\",\n",
      "        \"id\": \"AVA_002\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"data_engineering\",\n",
      "          \"data_aggregation\",\n",
      "          \"data_processing\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create real-time performance metrics display\",\n",
      "        \"id\": \"AVA_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend\",\n",
      "          \"real_time_systems\",\n",
      "          \"data_visualization\",\n",
      "          \"charting_libraries\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add filtering and date range selection features\",\n",
      "        \"id\": \"AVA_004\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend\",\n",
      "          \"filtering_systems\",\n",
      "          \"date_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate dashboard with existing data sources\",\n",
      "        \"id\": \"AVA_005\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"data_integration\",\n",
      "          \"api_design\",\n",
      "          \"data_mapping\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks from user story\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task extraction specialist. Break down user stories into 2-7 specific, actionable tasks.\n",
    "\n",
    "Requirements:\n",
    "- Minimum 2 tasks, maximum 7 tasks\n",
    "- Each task should be concise (10-30 words)\n",
    "- Tasks must be clear and actionable\n",
    "- Focus on essential steps only\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks:\n",
    "1. Design user registration form interface\n",
    "2. Implement email validation and verification system\n",
    "3. Create password strength requirements and validation\n",
    "4. Build user profile creation workflow\n",
    "5. Add account activation process\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks:\n",
    "1. Design analytics dashboard layout and components\n",
    "2. Implement data collection and aggregation system\n",
    "3. Create real-time performance metrics display\n",
    "4. Add filtering and date range selection features\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Tasks:\n",
    "1. Design search interface with filters\n",
    "2. Implement search algorithm and indexing\n",
    "3. Create search results display with pagination\n",
    "4. Add search history and suggestions feature\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Tasks:\n",
    "1. Configure automated build and testing pipeline\n",
    "2. Set up deployment staging and production environments\n",
    "3. Implement code quality checks and security scanning\n",
    "\n",
    "Now break down this user story:\n",
    "User Story: {user_story}\n",
    "\n",
    "Return ONLY a numbered list of tasks:\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "        \n",
    "        tasks = self._parse_tasks(output_text)\n",
    "        print(f\"âœ“ Extracted {len(tasks)} tasks\")\n",
    "        return tasks\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'user story:', 'tasks:', 'here are', 'the following', \n",
    "                'broken down', 'example format:', '**'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "            clean_task = clean_task.strip()\n",
    "            \n",
    "            if clean_task and len(clean_task) > 10:\n",
    "                tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "\n",
    "    \"\"\" Step 2: Estimate story points for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Consider complexity, time, risk, and uncertainty.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks and Estimates:\n",
    "1. Design user registration form interface (3 points)\n",
    "2. Implement email validation and verification system (5 points)\n",
    "3. Create password strength requirements and validation (3 points)\n",
    "4. Build user profile creation workflow (5 points)\n",
    "5. Add account activation process (3 points)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks and Estimates:\n",
    "1. Design analytics dashboard layout and components (5 points)\n",
    "2. Implement data collection and aggregation system (8 points)\n",
    "3. Create real-time performance metrics display (5 points)\n",
    "4. Add filtering and date range selection features (3 points)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Tasks and Estimates:\n",
    "1. Design search interface with filters (3 points)\n",
    "2. Implement search algorithm and indexing (8 points)\n",
    "3. Create search results display with pagination (3 points)\n",
    "4. Add search history and suggestions feature (5 points)\n",
    "\n",
    "Now estimate points for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "Task 3: Z points\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "        \n",
    "        points = self._parse_story_points(output_text, tasks)\n",
    "        print(f\"âœ“ Estimated story points for {len(points)} tasks\")\n",
    "        total_points = sum(points.values())\n",
    "        return {\n",
    "        'total_story_points': total_points,\n",
    "        'task_points': points,\n",
    "        'estimated_sum': total_points\n",
    "    }\n",
    "    \n",
    "    def _parse_story_points(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    # Extract task number and points\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    # Find closest valid point\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default points\n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3  # Default moderate complexity\n",
    "        \n",
    "        return points\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 2b: Identify required skills for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task using few-shot examples\"\"\"\n",
    "        # Create a mini user story context and single task for the existing prompt\n",
    "        user_story = \"General task completion\"\n",
    "        tasks = [task]\n",
    "        tasks_str = \"1. \" + task\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify specific skills required for each task.\n",
    "\n",
    "Consider programming languages, frameworks, domains, and specializations.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, form_design, frontend\n",
    "Task 2: backend, email_systems, validation, security\n",
    "Task 3: frontend, validation, security_patterns\n",
    "Task 4: backend, workflow_design, user_management\n",
    "Task 5: backend, email_systems, activation_flows\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, dashboard_design, data_visualization\n",
    "Task 2: backend, database_design, data_processing, analytics\n",
    "Task 3: frontend, real_time_systems, charting_libraries\n",
    "Task 4: frontend, filtering_systems, date_handling\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Task Skills:\n",
    "Task 1: ui_design, search_interface, filtering_systems\n",
    "Task 2: backend, search_algorithms, database_optimization, indexing\n",
    "Task 3: frontend, pagination, results_display\n",
    "Task 4: backend, data_storage, recommendation_systems\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Task Skills:\n",
    "Task 1: devops, ci_cd, automated_testing, build_systems\n",
    "Task 2: devops, infrastructure, deployment_automation\n",
    "Task 3: devops, security_scanning, code_quality, static_analysis\n",
    "\n",
    "Now identify skills for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\"\"\"\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "    \n",
    "        # Parse for single task\n",
    "        skills_map = self._parse_skills(output_text, tasks)\n",
    "        return skills_map.get(task, [\"general_development\"])\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "    \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "    \n",
    "        print(f\"âœ“ Identified skills for {len(skills_map)} tasks\")\n",
    "        return skills_map\n",
    "        \n",
    "    \n",
    "    def _parse_skills(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Parse skills\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 3: Analyze dependencies between tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        tasks_with_points = []\n",
    "        for i, task in enumerate(tasks):\n",
    "            points = story_points.get(task, 3)\n",
    "            tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "        \n",
    "        tasks_str = \"\\n\".join(tasks_with_points)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "Consider logical workflow order and technical dependencies.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Dependencies:\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "Task 4 depends on Task 2 (rework_effort: 3)\n",
    "Task 5 depends on Task 2 (rework_effort: 2)\n",
    "Task 5 depends on Task 4 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 2 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Dependencies:\n",
    "Task 2 depends on Task 1 (rework_effort: 2)\n",
    "Task 3 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "rework_effort scale:\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed  \n",
    "- 3: High rework effort required\n",
    "\n",
    "Now analyze dependencies for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies. Don't create artificial ones.\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "        \n",
    "        dependencies = self._parse_dependencies(output_text, tasks)\n",
    "        print(f\"âœ“ Analyzed dependencies for {len(dependencies)} tasks\")\n",
    "        return dependencies\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers\n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            # Validate rework_effort\n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2  # Default\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "\n",
    "class FormatValidatorAgent:\n",
    "    \"\"\"Step 4: Validate and format final output\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def validate_and_format(self, user_story: str, tasks: List[str], \n",
    "                                 story_points: Dict[str, int], skills: Dict[str, List[str]], \n",
    "                                 dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, any]:\n",
    "        \n",
    "        # Generate task IDs\n",
    "        task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "        \n",
    "        # Build final structure\n",
    "        formatted_tasks = []\n",
    "        total_story_points = 0\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = task_ids[i]\n",
    "            task_points = story_points.get(task, 3)\n",
    "            task_skills = skills.get(task, [\"general_development\"])\n",
    "            task_dependencies = dependencies.get(task, [])\n",
    "            \n",
    "            # Convert dependencies to use task IDs\n",
    "            formatted_dependencies = []\n",
    "            for dep in task_dependencies:\n",
    "                dep_task = dep[\"task_id\"]\n",
    "                if dep_task in tasks:\n",
    "                    dep_index = tasks.index(dep_task)\n",
    "                    dep_task_id = task_ids[dep_index]\n",
    "                    formatted_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            formatted_tasks.append({\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": task_points,\n",
    "                \"depends_on\": formatted_dependencies,\n",
    "                \"required_skills\": task_skills\n",
    "            })\n",
    "            \n",
    "            total_story_points += task_points\n",
    "        \n",
    "        result = {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"story_points\": total_story_points,\n",
    "                \"tasks\": formatted_tasks\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            json.dumps(result)\n",
    "            print(\"âœ“ Format validation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Format validation failed: {e}\")\n",
    "            # Apply fixes if needed\n",
    "            result = self._fix_json_issues(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        # Extract key words from user story to create meaningful prefix\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        # Take first few significant words and create acronym\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        # Ensure prefix is exactly 3 characters\n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        \n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _fix_json_issues(self, result: Dict[str, any]) -> Dict[str, any]:\n",
    "        # Implement basic JSON fixes\n",
    "        try:\n",
    "            # Convert any non-serializable items\n",
    "            json_str = json.dumps(result, default=str)\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return result\n",
    "\n",
    "class UserStoryPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_extractor = TaskExtractorAgent()\n",
    "        self.story_point_estimator = StoryPointEstimatorAgent()\n",
    "        self.skills_agent = RequiredSkillsAgent()\n",
    "        self.dependency_agent = DependencyAgent()\n",
    "        self.format_validator = FormatValidatorAgent()\n",
    "    \n",
    "    async def process_story(self, user_story: str) -> Dict[str, any]:\n",
    "        try:\n",
    "            print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "            \n",
    "            # Step 1: Extract tasks\n",
    "            print(\"  Step 1: Extracting tasks...\")\n",
    "            tasks = await self.task_extractor.decompose(user_story)\n",
    "            \n",
    "            if not tasks:\n",
    "                raise ValueError(\"No tasks extracted from user story\")\n",
    "            \n",
    "            # Step 2: Parallel processing of story points and skills\n",
    "            print(\"  Step 2: Estimating story points and identifying skills...\")\n",
    "            story_points_results, skills = await asyncio.gather(\n",
    "                self.story_point_estimator.estimate_story_points(user_story, tasks),\n",
    "                self.skills_agent.identify_skills(user_story, tasks)\n",
    "            )\n",
    "            story_points = story_points_results['task_points']\n",
    "            # Step 3: Analyze dependencies\n",
    "            print(\"  Step 3: Analyzing dependencies...\")\n",
    "            dependencies = await self.dependency_agent.analyze_dependencies(\n",
    "                user_story, tasks, story_points\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format and validate\n",
    "            print(\"  Step 4: Formatting and validating...\")\n",
    "            result = await self.format_validator.validate_and_format(\n",
    "                user_story, tasks, story_points, skills, dependencies\n",
    "            )\n",
    "            \n",
    "            print(\"  âœ… Story processing complete!\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing story: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"error\": str(e),\n",
    "                \"output\": None\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_stories(self, user_stories: List[str]) -> List[Dict[str, any]]:\n",
    "        print(f\"\\nðŸš€ Processing {len(user_stories)} user stories...\")\n",
    "        \n",
    "        # Reset token tracker\n",
    "        global token_tracker\n",
    "        token_tracker = TokenTracker()\n",
    "        \n",
    "        # Process all stories\n",
    "        results = []\n",
    "        for story in user_stories:\n",
    "            result = await self.process_story(story)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Completed processing {len(results)} stories\")\n",
    "        return results\n",
    "\n",
    "def format_output(results: List[Dict[str, any]]) -> str:\n",
    "    \"\"\"Format results for display\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Token usage summary\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"TOKEN USAGE SUMMARY\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    if summary:\n",
    "        breakdown = summary.get(\"breakdown\", {})\n",
    "        output.append(f\"TOTAL TOKENS CONSUMED: {breakdown.get('total_consumed', 0)}\")\n",
    "        \n",
    "        cost_data = summary.get(\"cost_estimate\", {})\n",
    "        if cost_data:\n",
    "            output.append(f\"ESTIMATED COST: ${cost_data['total_cost']:.6f}\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Results\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"PROCESSED USER STORIES\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- Story {i} ---\")\n",
    "        if \"error\" in result:\n",
    "            output.append(f\"âŒ Error: {result['error']}\")\n",
    "        else:\n",
    "            # Pretty print JSON\n",
    "            formatted_json = json.dumps(result, indent=2)\n",
    "            output.append(formatted_json)\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    \"\"\"Notebook-friendly version to run the pipeline\"\"\"\n",
    "    pipeline = UserStoryPipeline()\n",
    "    results = await pipeline.process_multiple_stories(stories_list)\n",
    "    print(format_output(results))\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_pipeline(user_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b437de-e72c-4606-9014-48eccfe1255a",
   "metadata": {},
   "source": [
    "- **Dependencies:**\n",
    "  \n",
    "> **Groq API:** Large Language Model provider using llama3-70b-8192 model with few-shot prompting\n",
    "\n",
    "> **tiktoken:** Token estimation using cl100k_base encoding for cost tracking\n",
    "\n",
    "> **python-dotenv:** Environment variable management for API keys\n",
    "\n",
    "> **asyncio:** Asynchronous programming support for parallel processing\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TokenTracker:** Enhanced monitoring of API usage across all pipeline stages, tracks input/output tokens per operation, estimates costs with configurable rates, and provides detailed efficiency metrics using tiktoken's cl100k_base encoding with fallback counting.\n",
    "\n",
    "> **TaskExtractorAgent:** Decomposes user stories into 2-7 actionable tasks (10-30 words each) using few-shot examples and structured prompts. Key method: decompose(user_story) -> List[str] with improved parsing resilience.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Estimates task complexity using Fibonacci sequence [1,2,3,5,8,13] with few-shot examples for consistent estimation. Key method: estimate_story_points(user_story, tasks) -> Dict returns total points, task breakdown, and estimated sum.\n",
    "\n",
    "> **RequiredSkillsAgent:** Maps technical skills to individual tasks using few-shot prompting, processes each task separately for better accuracy. Key methods: identify_skills(user_story, tasks) -> Dict[str, List[str]] and map_skills(task) -> List[str] for granular skill mapping.\n",
    "\n",
    "> **DependencyAgent:** Identifies inter-task dependencies with rework effort scoring (1-3 scale) using examples-based prompting. Key method: analyze_dependencies(user_story, tasks, story_points) -> Dict returns dependency mappings with rework_effort calculations.\n",
    "\n",
    "> **FormatValidatorAgent:** Generates meaningful task IDs from user story keywords, validates JSON structure, and applies automatic fixes. Key method: validate_and_format(...) -> Dict produces final structured result with task ID generation using acronyms.\n",
    "\n",
    "> **UserStoryPipeline:** Enhanced orchestrator executing 4-step workflow with improved error handling: (1) Task extraction, (2) Parallel story points + skills analysis, (3) Dependency analysis, (4) Format validation. Key methods: process_story(user_story) -> Dict and process_multiple_stories(user_stories) -> List[Dict]\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **process_story(user_story: str):** Main pipeline method with enhanced error handling, executing sequential workflow with parallel processing in step 2 using asyncio.gather() for story points and skills identification.\n",
    "\n",
    "> **decompose(user_story: str):** Extracts 2-7 actionable tasks using few-shot examples in prompts, improved parsing with better filtering of explanatory text and headers.\n",
    "\n",
    "> **estimate_story_points(user_story, tasks):** Returns {'total_story_points': int, 'task_points': Dict, 'estimated_sum': int} using Fibonacci sequence with few-shot examples and automatic correction for invalid values.\n",
    "\n",
    "> **map_skills(task: str):** Individual task skill mapping using few-shot examples, processes single tasks for better accuracy than batch processing. Returns List[str] of technical skills.\n",
    "\n",
    "> **identify_skills(user_story, tasks):** Orchestrates skill mapping for all tasks by calling map_skills for each task individually, ensuring comprehensive skill coverage.\n",
    "\n",
    "> **analyze_dependencies(user_story, tasks, story_points):** Enhanced dependency analysis using few-shot examples, identifies logical and technical dependencies returning {'dependent_task': [{'task_id': 'prereq', 'rework_effort': 1-3}]}.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae462c3c-59c6-4d31-b569-5a362e8ddacb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4.3 Strategy 3: Chain of Thought\n",
    "\n",
    "- **Definition**\n",
    "> **Providing the model with a reasoning process step-by-step to breaking down complex problems into intermediate steps.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Step-by-step reasoning** process\n",
    "\n",
    "> **Problem decomposition** into manageable parts\n",
    "\n",
    "> **Explicit thinking steps** before final answer\n",
    "\n",
    "> **Systematic approach** to complex tasks\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Good for complex user stories with intricate dependencies**\n",
    "\n",
    "> **Reduces errors through systematic approach**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Longer response times and token usage**\n",
    "\n",
    "> **Can sometimes get stuck in analysis paralysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07870b77-75dd-405c-a786-5b4a47b59b8e",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fc31961-9e01-4131-a6bb-7746b714c2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TASK_EXTRACTION] Tokens - Input: 133, Output: 504, Total: 637\n",
      "[TASK_EXTRACTION] Tokens - Input: 133, Output: 586, Total: 719\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 165, Output: 212, Total: 377\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 161, Output: 1, Total: 162\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 173, Output: 170, Total: 343\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 163, Output: 243, Total: 406\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 161, Output: 198, Total: 359\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 164, Output: 166, Total: 330\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 204, Output: 159, Total: 363\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 151, Total: 331\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 165, Total: 345\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 186, Output: 1, Total: 187\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 176, Output: 158, Total: 334\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 174, Output: 210, Total: 384\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 167, Total: 347\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 179, Output: 1, Total: 180\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 178, Output: 171, Total: 349\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 178, Output: 143, Total: 321\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 176, Output: 238, Total: 414\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 61, Total: 241\n",
      "[REQUIRED_SKILLS] Tokens - Input: 235, Output: 259, Total: 494\n",
      "[REQUIRED_SKILLS] Tokens - Input: 231, Output: 129, Total: 360\n",
      "[REQUIRED_SKILLS] Tokens - Input: 243, Output: 243, Total: 486\n",
      "[REQUIRED_SKILLS] Tokens - Input: 233, Output: 202, Total: 435\n",
      "[REQUIRED_SKILLS] Tokens - Input: 231, Output: 279, Total: 510\n",
      "[REQUIRED_SKILLS] Tokens - Input: 234, Output: 230, Total: 464\n",
      "[REQUIRED_SKILLS] Tokens - Input: 274, Output: 197, Total: 471\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 234, Total: 484\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 209, Total: 459\n",
      "[REQUIRED_SKILLS] Tokens - Input: 256, Output: 167, Total: 423\n",
      "[REQUIRED_SKILLS] Tokens - Input: 246, Output: 120, Total: 366\n",
      "[REQUIRED_SKILLS] Tokens - Input: 244, Output: 197, Total: 441\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 220, Total: 470\n",
      "[REQUIRED_SKILLS] Tokens - Input: 249, Output: 255, Total: 504\n",
      "[REQUIRED_SKILLS] Tokens - Input: 248, Output: 190, Total: 438\n",
      "[REQUIRED_SKILLS] Tokens - Input: 248, Output: 242, Total: 490\n",
      "[REQUIRED_SKILLS] Tokens - Input: 246, Output: 234, Total: 480\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 206, Total: 456\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 193, Output: 152, Total: 345\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 172, Output: 158, Total: 330\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 179, Output: 180, Total: 359\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 177, Output: 271, Total: 448\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 173, Output: 191, Total: 364\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 157, Output: 1, Total: 158\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 165, Output: 158, Total: 323\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 167, Output: 160, Total: 327\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 164, Output: 208, Total: 372\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 164, Output: 158, Total: 322\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 164, Output: 158, Total: 322\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 186, Total: 366\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 175, Output: 195, Total: 370\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 183, Output: 156, Total: 339\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 183, Output: 177, Total: 360\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 184, Output: 183, Total: 367\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 178, Output: 164, Total: 342\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 175, Output: 221, Total: 396\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 180, Output: 201, Total: 381\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 174, Output: 186, Total: 360\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 182, Output: 155, Total: 337\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 176, Output: 117, Total: 293\n",
      "[REQUIRED_SKILLS] Tokens - Input: 263, Output: 261, Total: 524\n",
      "[REQUIRED_SKILLS] Tokens - Input: 242, Output: 226, Total: 468\n",
      "[REQUIRED_SKILLS] Tokens - Input: 249, Output: 279, Total: 528\n",
      "[REQUIRED_SKILLS] Tokens - Input: 247, Output: 228, Total: 475\n",
      "[REQUIRED_SKILLS] Tokens - Input: 243, Output: 296, Total: 539\n",
      "[REQUIRED_SKILLS] Tokens - Input: 227, Output: 76, Total: 303\n",
      "[REQUIRED_SKILLS] Tokens - Input: 235, Output: 201, Total: 436\n",
      "[REQUIRED_SKILLS] Tokens - Input: 237, Output: 264, Total: 501\n",
      "[REQUIRED_SKILLS] Tokens - Input: 234, Output: 259, Total: 493\n",
      "[REQUIRED_SKILLS] Tokens - Input: 234, Output: 248, Total: 482\n",
      "[REQUIRED_SKILLS] Tokens - Input: 234, Output: 176, Total: 410\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 167, Total: 417\n",
      "[REQUIRED_SKILLS] Tokens - Input: 245, Output: 254, Total: 499\n",
      "[REQUIRED_SKILLS] Tokens - Input: 253, Output: 246, Total: 499\n",
      "[REQUIRED_SKILLS] Tokens - Input: 253, Output: 227, Total: 480\n",
      "[REQUIRED_SKILLS] Tokens - Input: 254, Output: 232, Total: 486\n",
      "[REQUIRED_SKILLS] Tokens - Input: 248, Output: 216, Total: 464\n",
      "[REQUIRED_SKILLS] Tokens - Input: 245, Output: 253, Total: 498\n",
      "[REQUIRED_SKILLS] Tokens - Input: 250, Output: 274, Total: 524\n",
      "[REQUIRED_SKILLS] Tokens - Input: 244, Output: 259, Total: 503\n",
      "[REQUIRED_SKILLS] Tokens - Input: 252, Output: 251, Total: 503\n",
      "[REQUIRED_SKILLS] Tokens - Input: 246, Output: 165, Total: 411\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 618, Output: 596, Total: 1214\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 699, Output: 679, Total: 1378\n",
      "\n",
      "RESULTS:\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 130,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"The user wants to create an account to access personalized features.\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"To accomplish this, we'll need:\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A user interface (UI) to collect user information (e.g., username, password, email)\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A backend API to handle account creation and storage\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A database to store user account information\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Authentication and authorization mechanisms to secure access to personalized features\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The UI depends on the backend API to send user information for account creation. The backend API depends on the database to store user account information. The authentication and authorization mechanisms depend on the backend API and database to verify user credentials and grant access to personalized features.\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design and implement the account creation UI form**: Create a user-friendly interface to collect user information (username, password, email, etc.).\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a backend API endpoint for account creation**: Develop a RESTful API endpoint to receive user information from the UI and handle account creation.\",\n",
      "        \"id\": \"T_009\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement user account data model in the database**: Define and create a database schema to store user account information (e.g., username, password hash, email, etc.).\",\n",
      "        \"id\": \"T_010\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Write a database query to store user account information**: Create a database query to insert user account information into the database.\",\n",
      "        \"id\": \"T_011\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement password hashing and salting**: Develop a password hashing and salting mechanism to securely store user passwords.\",\n",
      "        \"id\": \"T_012\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"backend_development\",\n",
      "          \"cybersecurity\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"data_science\",\n",
      "          \"ui_ux_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate the backend API with the database**: Connect the backend API to the database and use the database query to store user account information.\",\n",
      "        \"id\": \"T_013\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication and authorization mechanisms**: Develop a system to verify user credentials and grant access to personalized features based on user roles and permissions.\",\n",
      "        \"id\": \"T_014\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"frontend_development\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"none_directly_required_but_some_understanding_of\",\n",
      "          \"ui_ux_design\",\n",
      "          \"project_management_for_managing_the_implementation_process\",\n",
      "          \"communication\",\n",
      "          \"cybersecurity\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate the UI with the backend API**: Connect the UI to the backend API endpoint to send user information for account creation.\",\n",
      "        \"id\": \"T_015\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Test account creation functionality**: Verify that the entire account creation process works as expected, from UI input to database storage and authentication.\",\n",
      "        \"id\": \"T_016\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"testing_qa\",\n",
      "          \"automation\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"communication\",\n",
      "          \"stakeholder_management_to_manage_expectations_and_provide_updates\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement error handling and validation**: Add error handling and validation for the account creation process to ensure robustness and security.\",\n",
      "        \"id\": \"T_017\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"backend_development\",\n",
      "          \"frontend_development\",\n",
      "          \"database_management\",\n",
      "          \"javascript\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"communication\",\n",
      "          \"stakeholder_management_for_managing_expectations_and_ensuring_the_implementation_meets_the_requirements\",\n",
      "          \"testing_qa\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"These tasks are simple, focused, and each has a single responsibility, making it easier to develop, test, and maintain the system.\",\n",
      "        \"id\": \"T_018\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 151,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"The admin wants to view an analytics dashboard to monitor system performance. This means they need to access a visual representation of key performance indicators (KPIs) that provide insights into the system's behavior.\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A data storage system to collect and store performance metrics (e.g., database or data warehouse)\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A data processing system to transform and aggregate the metrics into meaningful KPIs (e.g., data pipeline or ETL process)\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A visualization component to render the KPIs in a user-friendly dashboard (e.g., charts, graphs, tables)\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"frontend_development\",\n",
      "          \"database_management\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"ui_ux_design\",\n",
      "          \"data_analysis\",\n",
      "          \"testing_qa\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"A web application to serve the dashboard to the admin user (e.g., web framework, templates)\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The dependencies are:\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The data storage system provides data to the data processing system.\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The data processing system provides processed KPIs to the visualization component.\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The visualization component provides the dashboard to the web application.\",\n",
      "        \"id\": \"T_009\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The web application serves the dashboard to the admin user.\",\n",
      "        \"id\": \"T_010\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"How can I break this into simple, focused tasks?\",\n",
      "        \"id\": \"T_011\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design database schema for performance metrics**: Define the database structure to store system performance metrics, including table schema, data types, and relationships.\",\n",
      "        \"id\": \"T_012\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"nontechnical_skills\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data ingestion pipeline**: Develop a data pipeline to collect and store performance metrics from the system into the database.\",\n",
      "        \"id\": \"T_013\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"communication\",\n",
      "          \"devops\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop data processing script**: Create a script to transform and aggregate the performance metrics into meaningful KPIs (e.g., averages, sums, counts).\",\n",
      "        \"id\": \"T_014\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"data_science\",\n",
      "          \"data_analysis\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design visualization components**: Create a design concept for the analytics dashboard, including the types of charts, graphs, and tables to display the KPIs.\",\n",
      "        \"id\": \"T_015\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement visualization library**: Choose and integrate a visualization library (e.g., D3.js, Chart.js) to render the KPIs in the dashboard.\",\n",
      "        \"id\": \"T_016\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop web application framework**: Set up a web application framework (e.g., React, Angular) to serve the analytics dashboard.\",\n",
      "        \"id\": \"T_017\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement dashboard template**: Create a template for the analytics dashboard, including the layout, styling, and visualization components.\",\n",
      "        \"id\": \"T_018\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"frontend_development\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"ui_ux_design\",\n",
      "          \"communication\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate data processing script with visualization component**: Connect the data processing script to the visualization component to populate the dashboard with KPIs.\",\n",
      "        \"id\": \"T_019\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication and authorization**: Add authentication and authorization mechanisms to ensure only authorized admins can access the analytics dashboard.\",\n",
      "        \"id\": \"T_020\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Deploy and test the analytics dashboard**: Deploy the analytics dashboard to a production environment and test it with sample data to ensure it's functioning as expected.\",\n",
      "        \"id\": \"T_021\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"technical_skills\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"infrastructure_management\",\n",
      "          \"nontechnical_skills\",\n",
      "          \"testing_qa\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"These tasks are simple, focused, and independent, making it easier to assign them to team members and track progress.\",\n",
      "        \"id\": \"T_022\",\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 35,976\n",
      "ESTIMATED COST: $0.535630\n",
      "\n",
      "BREAKDOWN BY CATEGORY:\n",
      "--------------------------------------------------\n",
      "Task Extraction          :  1,356 tokens (  3.8%)\n",
      "  Input                  :    266 tokens\n",
      "  Output                 :  1,090 tokens\n",
      "\n",
      "Story Point Estimation   : 13,354 tokens ( 37.1%)\n",
      "  Input                  :  7,003 tokens\n",
      "  Output                 :  6,351 tokens\n",
      "\n",
      "Required Skills          : 18,674 tokens ( 51.9%)\n",
      "  Input                  :  9,803 tokens\n",
      "  Output                 :  8,871 tokens\n",
      "\n",
      "Dependency Analysis      :  2,592 tokens (  7.2%)\n",
      "  Input                  :  1,317 tokens\n",
      "  Output                 :  1,275 tokens\n",
      "\n",
      "INPUT COST:  $0.183890\n",
      "OUTPUT COST: $0.351740\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001  # per token\n",
    "        output_rate = 0.00002  # per token\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks using Chain of Thought reasoning\"\"\"\n",
    "    \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task decomposition expert. Break down the following user story into specific, actionable technical tasks.\n",
    "Each task should be simple and focused on a single responsibility.\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Think through this step by step:\n",
    "1. What is the user trying to accomplish?\n",
    "2. What technical components are needed?\n",
    "3. What are the dependencies between components?\n",
    "4. How can I break this into simple, focused tasks?\n",
    "\n",
    "After your reasoning, provide a numbered list of tasks:\n",
    "\n",
    "Tasks:\n",
    "1. [First task]\n",
    "2. [Second task]\n",
    "3. [Third task]\n",
    "...\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "            \n",
    "            tasks = self._parse_tasks(output_text)\n",
    "            return tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Task extraction failed: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract clean task list from LLM response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        in_tasks_section = False\n",
    "        reasoning_keywords = ['reasoning:', 'let me think', 'step by step', 'what is', 'what are']\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Skip obvious reasoning sections\n",
    "            if any(keyword in line.lower() for keyword in reasoning_keywords):\n",
    "                in_tasks_section = False\n",
    "                continue\n",
    "            \n",
    "            # Detect when we reach the tasks section\n",
    "            if any(task_indicator in line.lower() for task_indicator in ['tasks:', 'task list:', 'the tasks are:']):\n",
    "                in_tasks_section = True\n",
    "                continue\n",
    "            \n",
    "            # Look for numbered or bulleted items that could be tasks\n",
    "            task_pattern = re.match(r'^[\\d\\-\\*\\.\\)\\s]*(.+)', line)\n",
    "            if task_pattern:\n",
    "                potential_task = task_pattern.group(1).strip()\n",
    "                \n",
    "                # Skip headers, explanatory text, and formatting\n",
    "                skip_phrases = [\n",
    "                    'user story:', 'here are', 'the following', 'broken down', \n",
    "                    'specific', 'technical', 'note:', 'reasoning:', 'step by step',\n",
    "                    'what is', 'what are', 'components are', 'dependencies between'\n",
    "                ]\n",
    "                \n",
    "                if any(skip_phrase in potential_task.lower() for skip_phrase in skip_phrases):\n",
    "                    continue\n",
    "                \n",
    "                # Clean up markdown formatting\n",
    "                potential_task = re.sub(r'^\\*\\*|\\*\\*$', '', potential_task)\n",
    "                potential_task = potential_task.strip()\n",
    "                \n",
    "                # Only add substantial tasks (longer than 10 chars and not just numbers/symbols)\n",
    "                if (potential_task and \n",
    "                    len(potential_task) > 10 and \n",
    "                    not re.match(r'^[\\d\\.\\)\\-\\*\\s]+$', potential_task) and\n",
    "                    potential_task not in tasks):\n",
    "                    tasks.append(potential_task)\n",
    "        \n",
    "        # If no tasks found with section detection, try extracting any numbered/bulleted items\n",
    "        if not tasks:\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                # Look for numbered items (1., 2., etc.) or bullet points\n",
    "                if re.match(r'^[\\d]+\\.', line) or re.match(r'^[\\-\\*]', line):\n",
    "                    clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "                    clean_task = re.sub(r'^\\*\\*|\\*\\*$', '', clean_task).strip()\n",
    "                    \n",
    "                    # Skip reasoning or explanation lines\n",
    "                    skip_phrases = [\n",
    "                        'reasoning:', 'let me think', 'step by step', 'what is', 'what are',\n",
    "                        'user story:', 'here are', 'the following', 'broken down'\n",
    "                    ]\n",
    "                    \n",
    "                    if (clean_task and \n",
    "                        len(clean_task) > 10 and \n",
    "                        not any(skip in clean_task.lower() for skip in skip_phrases) and\n",
    "                        clean_task not in tasks):\n",
    "                        tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2: Estimate story points using Fibonacci scale\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "\n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        task_points = {}\n",
    "        for task in tasks:\n",
    "            points = await self._estimate_single_task(task)\n",
    "            task_points[task] = points\n",
    "        \n",
    "        total_points = sum(task_points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': task_points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "        \n",
    "    async def _estimate_single_task(self, task: str) -> int:\n",
    "        prompt = f\"\"\"\n",
    "Estimate story points for this task using the Fibonacci scale: {self.fibonacci_scale}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Use Chain of Thought reasoning to think through the complexity factors:\n",
    "\n",
    "Let me assess this task step by step:\n",
    "1. What is the technical complexity (simple/moderate/complex)?\n",
    "2. What is the uncertainty level (low/medium/high)?\n",
    "3. What integration requirements are needed?\n",
    "4. What are the risk factors?\n",
    "5. How much effort will this realistically take?\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "Story Points: [Select from Fibonacci scale: 1, 2, 3, 5, 8, 13, 21]\n",
    "\n",
    "Return ONLY the final number from the Fibonacci scale.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "            \n",
    "            points = self._parse_story_points(output_text)\n",
    "            return points\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Story point estimation failed: {str(e)}\")\n",
    "            return 3  # Default moderate estimate\n",
    "    \n",
    "    def _parse_story_points(self, content: str) -> int:\n",
    "        \"\"\"Extract story points from response\"\"\"\n",
    "        \n",
    "        match = re.search(r'story\\s+points?:\\s*(\\d+)', content.lower())\n",
    "        if match:\n",
    "            points = int(match.group(1))\n",
    "            return points if points in self.fibonacci_scale else min(self.fibonacci_scale, key=lambda x: abs(x - points))\n",
    "        \n",
    "        # Look for numbers in Fibonacci scale\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', content)\n",
    "        for num_str in reversed(numbers):\n",
    "            num = int(num_str)\n",
    "            if num in self.fibonacci_scale:\n",
    "                return num\n",
    "        \n",
    "        return 3  # Default\n",
    "\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 3: Map required skills using Chain of Thought\"\"\"\n",
    "    \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "Identify the specific skills required to complete this task. Consider both technical and non-technical skills.\n",
    "\n",
    "Available skill categories:\n",
    "TECHNICAL SKILLS:\n",
    "- frontend_development, backend_development, database_management, javascript\n",
    "- mobile_development, cloud_computing, devops, infrastructure_management\n",
    "- data_science, machine_learning, cybersecurity, api_development\n",
    "- testing_qa, automation, system_architecture\n",
    "\n",
    "NON-TECHNICAL SKILLS:\n",
    "- ui_ux_design, graphic_design, product_management, project_management\n",
    "- business_analysis, marketing, sales, customer_service\n",
    "- communication, stakeholder_management, team_leadership\n",
    "- content_creation, technical_writing, training, research\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Let me think about what skills are needed for this task:\n",
    "1. What type of work is this (technical, design, business, etc.)?\n",
    "2. What specific technologies or domains are involved?\n",
    "3. What level of expertise is required?\n",
    "4. Are there any secondary skills needed?\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "Required Skills:\n",
    "- [List the relevant skills from the categories above]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "            \n",
    "            skills = self._parse_skills(output_text)\n",
    "            return skills if skills else [\"general_development\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skill mapping failed: {str(e)}\")\n",
    "            return [\"general_development\"]\n",
    "    \n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "    def _parse_skills(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract clean skills list from LLM response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        skills = []\n",
    "        in_skills_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect when we reach the skills section\n",
    "            if line.lower().startswith('required skills:'):\n",
    "                in_skills_section = True\n",
    "                continue\n",
    "            \n",
    "            # Skip reasoning section\n",
    "            if not in_skills_section:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'task:', 'here are', 'the following', 'skills needed'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Clean skill from bullet points and special characters\n",
    "            clean_skill = re.sub(r'^[\\-\\*\\s\\u2022]+', '', line)  # Remove bullets including unicode bullet\n",
    "            clean_skill = re.sub(r'[\\u2022\\u2023\\u25E6\\u2043\\u2219]', '', clean_skill)  # Remove all bullet characters\n",
    "            clean_skill = clean_skill.strip()\n",
    "            \n",
    "            if clean_skill and len(clean_skill) > 2:\n",
    "                # Normalize the skill\n",
    "                normalized_skill = self._normalize_skill(clean_skill)\n",
    "                if normalized_skill and normalized_skill not in skills:\n",
    "                    skills.append(normalized_skill)\n",
    "        \n",
    "        return skills\n",
    "    \n",
    "    def _normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"Normalize skills to comprehensive categories including technical and non-technical\"\"\"\n",
    "        skill_lower = skill.lower().strip()\n",
    "        \n",
    "        # Remove any remaining special characters and clean up\n",
    "        skill_lower = re.sub(r'[^\\w\\s]', '', skill_lower)\n",
    "        skill_lower = re.sub(r'\\s+', ' ', skill_lower).strip()\n",
    "        \n",
    "        # Comprehensive skill mapping with technical and non-technical skills\n",
    "        skill_mappings = {\n",
    "            'frontend_development': [\n",
    "                'frontend', 'front-end', 'front end', 'ui development', 'client-side', \n",
    "                'client side', 'html', 'css', 'responsive design', 'web development', \n",
    "                'browser', 'dom manipulation', 'web ui'\n",
    "            ],\n",
    "            'backend_development': [\n",
    "                'backend', 'back-end', 'back end', 'server', 'server-side', \n",
    "                'server side', 'api', 'rest api', 'microservices', 'processing logic',\n",
    "                'business logic', 'server development', 'web services'\n",
    "            ],\n",
    "            'database_management': [\n",
    "                'database', 'db', 'sql', 'data storage', 'database skills', \n",
    "                'data', 'query', 'mongodb', 'postgresql', 'mysql', 'data model',\n",
    "                'data modeling', 'database design', 'nosql', 'data warehouse'\n",
    "            ],\n",
    "            'javascript': [\n",
    "                'javascript', 'js', 'scripting', 'client scripting', 'web scripting',\n",
    "                'node.js', 'typescript', 'react', 'angular', 'vue'\n",
    "            ],\n",
    "            'mobile_development': [\n",
    "                'mobile', 'mobile development', 'ios', 'android', 'react native',\n",
    "                'flutter', 'mobile app', 'smartphone', 'tablet'\n",
    "            ],\n",
    "            'cloud_computing': [\n",
    "                'cloud', 'aws', 'azure', 'gcp', 'cloud services', 'serverless',\n",
    "                'lambda', 'cloud infrastructure', 'cloud platform'\n",
    "            ],\n",
    "            'devops': [\n",
    "                'devops', 'dev ops', 'ci/cd', 'continuous integration', 'continuous deployment',\n",
    "                'pipeline', 'build automation', 'deployment automation', 'jenkins',\n",
    "                'gitlab ci', 'github actions'\n",
    "            ],\n",
    "            'infrastructure_management': [\n",
    "                'infrastructure', 'infrastructure management', 'server management',\n",
    "                'docker', 'kubernetes', 'containerization', 'orchestration',\n",
    "                'monitoring', 'deployment', 'system administration'\n",
    "            ],\n",
    "            'data_science': [\n",
    "                'data science', 'data scientist', 'machine learning', 'ml', 'ai',\n",
    "                'artificial intelligence', 'analytics', 'statistical analysis',\n",
    "                'data mining', 'predictive modeling'\n",
    "            ],\n",
    "            'cybersecurity': [\n",
    "                'security', 'cybersecurity', 'information security', 'authentication', \n",
    "                'authorization', 'encryption', 'penetration testing', 'vulnerability',\n",
    "                'access control', 'security audit'\n",
    "            ],\n",
    "            'api_development': [\n",
    "                'api development', 'api design', 'rest', 'graphql', 'soap',\n",
    "                'web api', 'microservices', 'service integration'\n",
    "            ],\n",
    "            'testing_qa': [\n",
    "                'testing', 'qa', 'quality assurance', 'test automation', 'unit testing',\n",
    "                'integration testing', 'usability testing', 'accessibility testing',\n",
    "                'performance testing', 'selenium'\n",
    "            ],\n",
    "            'automation': [\n",
    "                'automation', 'process automation', 'script automation', 'workflow automation',\n",
    "                'robotic process automation', 'rpa'\n",
    "            ],\n",
    "            'system_architecture': [\n",
    "                'system architecture', 'software architecture', 'solution architecture',\n",
    "                'design patterns', 'scalability', 'system design'\n",
    "            ],\n",
    "            \n",
    "            # DESIGN & UX SKILLS\n",
    "            'ui_ux_design': [\n",
    "                'ui ux design', 'ui/ux design', 'ux design', 'ui design', 'user experience',\n",
    "                'user interface', 'interaction design', 'visual design', 'design',\n",
    "                'interface design', 'layout design', 'typography', 'design styles',\n",
    "                'design guidelines', 'wireframing', 'prototyping'\n",
    "            ],\n",
    "            'graphic_design': [\n",
    "                'graphic design', 'visual design', 'brand design', 'logo design',\n",
    "                'illustration', 'photoshop', 'illustrator', 'creative design'\n",
    "            ],\n",
    "            \n",
    "            # PROJECT & PRODUCT MANAGEMENT\n",
    "            'product_management': [\n",
    "                'product management', 'product manager', 'product strategy',\n",
    "                'product development', 'product planning', 'roadmap', 'feature planning'\n",
    "            ],\n",
    "            'project_management': [\n",
    "                'project management', 'project manager', 'project coordination',\n",
    "                'agile', 'scrum', 'kanban', 'planning', 'scheduling', 'resource management',\n",
    "                'timeline management', 'milestone tracking'\n",
    "            ],\n",
    "            'stakeholder_management': [\n",
    "                'stakeholder management', 'stakeholder coordination', 'client management',\n",
    "                'vendor management', 'relationship management', 'negotiation'\n",
    "            ],\n",
    "            'team_leadership': [\n",
    "                'team leadership', 'leadership', 'team management', 'people management',\n",
    "                'mentoring', 'coaching', 'team building'\n",
    "            ],\n",
    "            \n",
    "            # BUSINESS & ANALYSIS\n",
    "            'business_analysis': [\n",
    "                'business analysis', 'business analyst', 'requirements analysis',\n",
    "                'process analysis', 'business requirements', 'functional analysis',\n",
    "                'business process', 'requirements gathering'\n",
    "            ],\n",
    "            'business_strategy': [\n",
    "                'business strategy', 'strategic planning', 'business development',\n",
    "                'market analysis', 'competitive analysis', 'business planning'\n",
    "            ],\n",
    "            'data_analysis': [\n",
    "                'data analysis', 'data analytics', 'analytics', 'reporting', \n",
    "                'data processing', 'data manipulation', 'business intelligence',\n",
    "                'dashboard', 'metrics', 'kpi'\n",
    "            ],\n",
    "            \n",
    "            # MARKETING & SALES\n",
    "            'marketing': [\n",
    "                'marketing', 'digital marketing', 'marketing strategy', 'campaign management',\n",
    "                'social media marketing', 'email marketing', 'seo', 'sem', 'advertising'\n",
    "            ],\n",
    "            'content_creation': [\n",
    "                'content creation', 'content marketing', 'copywriting', 'content strategy',\n",
    "                'blog writing', 'social media content', 'video content'\n",
    "            ],\n",
    "            'sales': [\n",
    "                'sales', 'sales development', 'lead generation', 'customer acquisition',\n",
    "                'sales strategy', 'account management'\n",
    "            ],\n",
    "            'customer_service': [\n",
    "                'customer service', 'customer support', 'customer success',\n",
    "                'help desk', 'customer experience', 'support'\n",
    "            ],\n",
    "            \n",
    "            # COMMUNICATION & DOCUMENTATION\n",
    "            'communication': [\n",
    "                'communication', 'verbal communication', 'written communication',\n",
    "                'presentation', 'public speaking', 'interpersonal skills'\n",
    "            ],\n",
    "            'technical_writing': [\n",
    "                'technical writing', 'documentation', 'technical documentation',\n",
    "                'user manuals', 'api documentation', 'knowledge base'\n",
    "            ],\n",
    "            'training': [\n",
    "                'training', 'training development', 'curriculum development',\n",
    "                'knowledge transfer', 'workshop facilitation', 'education'\n",
    "            ],\n",
    "            'research': [\n",
    "                'research', 'market research', 'user research', 'competitive research',\n",
    "                'analysis', 'investigation', 'data gathering'\n",
    "            ],\n",
    "            \n",
    "            # SPECIALIZED SKILLS\n",
    "            'error_handling': [\n",
    "                'error handling', 'debugging', 'exception handling', 'error management',\n",
    "                'troubleshooting', 'problem solving'\n",
    "            ],\n",
    "            'logging': [\n",
    "                'logging', 'auditing', 'tracking', 'monitoring', 'observability'\n",
    "            ],\n",
    "            'email_integration': [\n",
    "                'email integration', 'email', 'messaging', 'notification',\n",
    "                'email automation', 'smtp'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Check for exact matches first\n",
    "        for standard_skill, variations in skill_mappings.items():\n",
    "            if skill_lower in variations:\n",
    "                return standard_skill\n",
    "            # Check for partial matches\n",
    "            for variation in variations:\n",
    "                if variation in skill_lower or skill_lower in variation:\n",
    "                    return standard_skill\n",
    "        \n",
    "        # If no match found, but it's a valid skill, return it cleaned\n",
    "        if len(skill_lower) > 2 and not skill_lower in ['general', 'development', 'general development']:\n",
    "            return skill_lower.replace(' ', '_')\n",
    "        \n",
    "        # Only return None if we really can't identify the skill\n",
    "        return None\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 4: Analyze dependencies using Chain of Thought\"\"\"\n",
    "    \n",
    "    async def analyze_dependencies(self, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        prompt = f\"\"\"\n",
    "Analyze dependencies between these tasks. Identify which tasks must be completed before others can start.\n",
    "\n",
    "Use Chain of Thought reasoning to think through each task and its relationships:\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Let me analyze these tasks step by step:\n",
    "1. What does each task involve technically?\n",
    "2. Which tasks need foundational components from other tasks?\n",
    "3. What would happen if a prerequisite task fails or changes?\n",
    "4. How much rework would be needed in dependent tasks?\n",
    "\n",
    "For each dependency, estimate rework effort (1-8 story points) if prerequisite fails:\n",
    "- 1-2: minimal changes, mostly configuration\n",
    "- 3-5: moderate changes, some logic rework  \n",
    "- 8: major changes, architectural rework\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "Dependencies:\n",
    "[Only list actual dependencies using format: \"- Task X depends on Task Y (rework_effort: POINTS)\"]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=800\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "            \n",
    "            dependencies = self._parse_dependencies(output_text, tasks)\n",
    "            return dependencies\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Dependency analysis failed: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _parse_dependencies(self, text: str, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        dependencies = {}\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        in_dependencies_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            # Detect when we reach the dependencies section\n",
    "            if line.lower().startswith('dependencies:'):\n",
    "                in_dependencies_section = True\n",
    "                continue\n",
    "            \n",
    "            # Skip reasoning section\n",
    "            if not in_dependencies_section:\n",
    "                continue\n",
    "                \n",
    "            if \"depends on\" in line.lower():\n",
    "                try:\n",
    "                    # Parse \"Task X depends on Task Y (rework_effort: N)\"\n",
    "                    match = re.search(r'task\\s+(\\d+)\\s+depends\\s+on\\s+task\\s+(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_idx = int(match.group(1)) - 1\n",
    "                        prerequisite_idx = int(match.group(2)) - 1\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 0 <= dependent_idx < len(tasks) and 0 <= prerequisite_idx < len(tasks):\n",
    "                            dependent_task = tasks[dependent_idx]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                'task_id': f\"T_{prerequisite_idx + 1:03d}\",\n",
    "                                'rework_effort': min(8, max(1, rework_effort))\n",
    "                            })\n",
    "                            \n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        return dependencies\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Step 5: Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Validation error: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the Chain of Thought enhanced pipeline\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        # Step 2 & 3: Parallel processing of Story Points and Skills\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Process all tasks in parallel\n",
    "        story_points_tasks = [estimator._estimate_single_task(task) for task in tasks]\n",
    "        skills_tasks = [skills_agent.map_skills(task) for task in tasks]\n",
    "        \n",
    "        story_points_results, skills_results = await asyncio.gather(\n",
    "            asyncio.gather(*story_points_tasks),\n",
    "            asyncio.gather(*skills_tasks)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Dependency Analysis\n",
    "        dependency_agent = DependencyAgent()\n",
    "        dependencies = await dependency_agent.analyze_dependencies(tasks)\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points_results)\n",
    "        \n",
    "        for i, (task, story_points, skills) in enumerate(zip(tasks, story_points_results, skills_results)):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                task_dependencies = dependencies[task]\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": story_points,\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        # Final validation and formatting\n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the Chain of Thought enhanced pipeline\"\"\"\n",
    "    \n",
    "    # Process stories in parallel for efficiency\n",
    "    tasks = [process_user_story_pipeline(story) for story in user_stories]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Handle any exceptions\n",
    "    final_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            final_results.append({\n",
    "                \"input\": user_stories[i],\n",
    "                \"output\": {\n",
    "                    \"error\": str(result),\n",
    "                    \"story_points\": 0,\n",
    "                    \"tasks\": []\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "def print_token_usage():\n",
    "    \"\"\"Print comprehensive token usage statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOKEN USAGE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    breakdown = summary['breakdown']\n",
    "    cost_estimate = summary['cost_estimate']\n",
    "    efficiency = summary['efficiency_metrics']\n",
    "    \n",
    "    print(f\"TOTAL TOKENS CONSUMED: {breakdown['total_consumed']:,}\")\n",
    "    print(f\"ESTIMATED COST: ${cost_estimate['total_cost']:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"BREAKDOWN BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis']\n",
    "    \n",
    "    for category in categories:\n",
    "        if category in breakdown:\n",
    "            cat_data = breakdown[category]\n",
    "            percentage = efficiency['percentage_breakdown'].get(category, 0)\n",
    "            print(f\"{category.replace('_', ' ').title():<25}: {cat_data['total']:>6,} tokens ({percentage:>5.1f}%)\")\n",
    "            print(f\"  {'Input':<23}: {cat_data['input']:>6,} tokens\")\n",
    "            print(f\"  {'Output':<23}: {cat_data['output']:>6,} tokens\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"INPUT COST:  ${cost_estimate['input_cost']:.6f}\")\n",
    "    print(f\"OUTPUT COST: ${cost_estimate['output_cost']:.6f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    results = await process_multiple_user_stories_pipeline(stories_list)\n",
    "    print(\"\\nRESULTS:\")\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print()\n",
    "    print_token_usage()\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_pipeline(user_stories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af175b33-b6de-459b-bb53-bcb3b01d1ea9",
   "metadata": {},
   "source": [
    "- **Dependencies:**\n",
    "  \n",
    "> **Groq API:** Large Language Model provider using llama3-70b-8192 model with Chain of Thought reasoning\n",
    "\n",
    "> **tiktoken:** Token estimation using cl100k_base encoding for comprehensive cost tracking\n",
    "\n",
    "> **python-dotenv:** Environment variable management for secure API key handling\n",
    "\n",
    "> **asyncio:** Asynchronous programming support for parallel processing across multiple tasks\n",
    "\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TokenTracker:** Advanced monitoring system tracking API usage across all pipeline stages with detailed breakdown by category, cost estimation with configurable input/output rates, and comprehensive efficiency metrics using tiktoken's cl100k_base encoding with intelligent fallback counting.\n",
    "\n",
    "> **TaskExtractorAgent:** Chain of Thought task decomposition using step-by-step reasoning prompts. Extracts actionable tasks with improved parsing that filters reasoning sections and explanatory text. Key method: decompose(user_story) -> List[str] with enhanced section detection and task validation.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Individual task estimation using Chain of Thought reasoning with Fibonacci scale [1,2,3,5,8,13,21]. Processes each task separately with detailed complexity assessment. Key methods: estimate_story_points(user_story, tasks) -> Dict and _estimate_single_task(task) -> int with reasoning-based evaluation.\n",
    "\n",
    "> **RequiredSkillsAgent:** Comprehensive skill mapping using Chain of Thought with extensive skill categorization including technical and non-technical skills. Advanced normalization system with skill mappings for frontend, backend, design, management, and specialized domains. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Chain of Thought dependency analysis identifying logical and technical relationships between tasks with rework effort estimation (1-8 story points scale). Key method: analyze_dependencies(tasks) -> Dict with reasoning-based dependency detection and effort scoring.\n",
    "\n",
    "> **FormatValidator:** Output structure validation and formatting with comprehensive error handling and field validation. Key method: validate_and_format(user_story, tasks_data, total_story_points) -> Dict ensuring data integrity and structure compliance.\n",
    "\n",
    "- **Pipeline Functions:**\n",
    "\n",
    "> **process_user_story_pipeline(user_story):** Main single story processing function with Chain of Thought reasoning, parallel processing for story points and skills, comprehensive error handling, and structured output generation.\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories):** Batch processing function handling multiple stories in parallel with exception management and graceful error handling for individual story failures.\n",
    "\n",
    "> **print_token_usage():** Comprehensive token usage reporting with detailed breakdown by category, cost analysis, efficiency metrics, and percentage distribution across all pipeline stages.\n",
    "\n",
    "> **run_pipeline(stories_list):** Notebook-friendly async wrapper with complete pipeline execution, results formatting, token usage reporting, and return value for further analysis.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **decompose(user_story: str):** Chain of Thought task extraction with step-by-step reasoning prompts, enhanced parsing that separates reasoning from task lists, improved section detection, and robust task validation with length and content filtering.\n",
    "\n",
    "> **_estimate_single_task(task: str):** Individual task complexity assessment using Chain of Thought reasoning covering technical complexity, uncertainty level, integration requirements, and risk factors with Fibonacci scale selection.\n",
    "\n",
    "> **map_skills(task: str):** Comprehensive skill identification using Chain of Thought reasoning with extensive skill categories covering technical skills (frontend, backend, database, cloud, devops) and non-technical skills (design, management, communication, business analysis).\n",
    "\n",
    "> **_normalize_skill(skill: str):** Advanced skill normalization system with comprehensive mappings for technical and non-technical skills, partial matching capabilities, and intelligent skill categorization with fallback handling.\n",
    "\n",
    "> **analyze_dependencies(tasks: List[str]):** Chain of Thought dependency analysis with step-by-step reasoning for task relationships, rework effort estimation using story points scale (1-8), and logical dependency detection with effort scoring.\n",
    "\n",
    "> **validate_and_format(user_story, tasks_data, total_story_points):** Complete output validation ensuring required fields, structure compliance, error handling with graceful degradation, and comprehensive formatting with validation error reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00443464-9976-4589-8b53-f5953f130d5c",
   "metadata": {},
   "source": [
    "#### 4.4.4 Strategy 4: fewshots-cot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cee64-6236-4c46-a6b5-ebecd646d076",
   "metadata": {},
   "source": [
    "- **Definition**\n",
    "> **Combining few-shot examples with step-by-step reasoning, showing both the process and the final output.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Hybrid methodology** combining examples and reasoning\n",
    "\n",
    "> **Demonstrated thinking process** through examples\n",
    "\n",
    "> **Pattern learning** with explicit reasoning steps\n",
    "\n",
    "> **Comprehensive guidance** for complex tasks\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Combines benefits of both few-shot and chain-of-thought**\n",
    "\n",
    "> **Highest quality and consistency for complex breakdowns**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Highest token usage and response time may be overly rigid for simple stories**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7d80-7f46-4a84-81a6-09d14867c050",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90d29bef-ef62-42d1-b3b9-68072a2d1fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TASK_EXTRACTION] Tokens - Input: 530, Output: 198, Total: 728\n",
      "[TASK_EXTRACTION] Tokens - Input: 530, Output: 234, Total: 764\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 132, Output: 184, Total: 316\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 134, Output: 149, Total: 283\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 134, Output: 133, Total: 267\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 134, Output: 157, Total: 291\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 134, Output: 187, Total: 321\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 133, Output: 181, Total: 314\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 133, Output: 163, Total: 296\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 133, Output: 143, Total: 276\n",
      "[REQUIRED_SKILLS] Tokens - Input: 706, Output: 85, Total: 791\n",
      "[REQUIRED_SKILLS] Tokens - Input: 708, Output: 80, Total: 788\n",
      "[REQUIRED_SKILLS] Tokens - Input: 708, Output: 81, Total: 789\n",
      "[REQUIRED_SKILLS] Tokens - Input: 708, Output: 75, Total: 783\n",
      "[REQUIRED_SKILLS] Tokens - Input: 708, Output: 85, Total: 793\n",
      "[REQUIRED_SKILLS] Tokens - Input: 707, Output: 101, Total: 808\n",
      "[REQUIRED_SKILLS] Tokens - Input: 707, Output: 89, Total: 796\n",
      "[REQUIRED_SKILLS] Tokens - Input: 707, Output: 98, Total: 805\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 132, Output: 206, Total: 338\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 135, Output: 227, Total: 362\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 135, Output: 188, Total: 323\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 137, Output: 126, Total: 263\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 136, Output: 212, Total: 348\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 138, Output: 146, Total: 284\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 136, Output: 194, Total: 330\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 136, Output: 204, Total: 340\n",
      "[REQUIRED_SKILLS] Tokens - Input: 706, Output: 91, Total: 797\n",
      "[REQUIRED_SKILLS] Tokens - Input: 709, Output: 87, Total: 796\n",
      "[REQUIRED_SKILLS] Tokens - Input: 709, Output: 83, Total: 792\n",
      "[REQUIRED_SKILLS] Tokens - Input: 711, Output: 100, Total: 811\n",
      "[REQUIRED_SKILLS] Tokens - Input: 710, Output: 80, Total: 790\n",
      "[REQUIRED_SKILLS] Tokens - Input: 712, Output: 103, Total: 815\n",
      "[REQUIRED_SKILLS] Tokens - Input: 710, Output: 70, Total: 780\n",
      "[REQUIRED_SKILLS] Tokens - Input: 710, Output: 94, Total: 804\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 680, Output: 454, Total: 1134\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 698, Output: 424, Total: 1122\n",
      "\n",
      "RESULTS:\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 30,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design registration form layout\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_ux_design\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement input validation for registration form\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"cybersecurity\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Hash and store user password securely\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"cybersecurity\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create new user account in database\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Log user in after successful registration\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Redirect user to personalized dashboard\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"frontend_development\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send confirmation email to user\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"communication\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Handle registration errors and exceptions\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"testing_qa\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 65,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_ux_design\",\n",
      "          \"frontend_development\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create admin-only access control for dashboard\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"cybersecurity\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Determine key performance indicators for system monitoring\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"system_architecture\",\n",
      "          \"database_management\",\n",
      "          \"business_analysis\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data collection and storage for KPIs\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create chart and graph components for data visualization\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"ui_ux_design\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Set up real-time or near real-time data updates\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"system_architecture\",\n",
      "          \"project_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate authentication and authorization for admin access\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"cybersecurity\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design customizable dashboard components for future use cases\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"ui_ux_design\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 21,438\n",
      "ESTIMATED COST: $0.269500\n",
      "\n",
      "BREAKDOWN BY CATEGORY:\n",
      "--------------------------------------------------\n",
      "Task Extraction          :  1,492 tokens (  7.0%)\n",
      "  Input                  :  1,060 tokens\n",
      "  Output                 :    432 tokens\n",
      "\n",
      "Story Point Estimation   :  4,952 tokens ( 23.1%)\n",
      "  Input                  :  2,152 tokens\n",
      "  Output                 :  2,800 tokens\n",
      "\n",
      "Required Skills          : 12,738 tokens ( 59.4%)\n",
      "  Input                  : 11,336 tokens\n",
      "  Output                 :  1,402 tokens\n",
      "\n",
      "Dependency Analysis      :  2,256 tokens ( 10.5%)\n",
      "  Input                  :  1,378 tokens\n",
      "  Output                 :    878 tokens\n",
      "\n",
      "INPUT COST:  $0.159260\n",
      "OUTPUT COST: $0.110240\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001  # per token\n",
    "        output_rate = 0.00002  # per token\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Step 1: Extract tasks using Chain of Thought reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_cot_examples = \"\"\"\n",
    "User Story: As a user, I want to click on the address so that it takes me to a new tab with Google Maps.\n",
    "\n",
    "Reasoning: Let me break this down step by step:\n",
    "1. First, I need to understand what the user wants: clickable addresses that open Google Maps\n",
    "2. For this to work, I need to make the address text clickable (UI component)\n",
    "3. I need to handle the click event and format the address for Google Maps URL\n",
    "4. I need to ensure the Maps opens in a new tab/window for good UX\n",
    "5. I should handle URL encoding to ensure addresses with special characters work properly\n",
    "\n",
    "Tasks:\n",
    "1. Make address text clickable\n",
    "2. Implement click handler to format address for Google Maps URL\n",
    "3. Open Google Maps in new tab/window\n",
    "4. Add proper URL encoding for address parameters\n",
    "\n",
    "User Story: As a user, I want to be able to anonymously view public information so that I know about recycling centers near me before creating an account.\n",
    "\n",
    "Reasoning: Let me think through this step by step:\n",
    "1. The user wants to see recycling centers without creating an account first\n",
    "2. This means I need a public-facing page that doesn't require authentication\n",
    "3. I need to handle anonymous users differently from authenticated users\n",
    "4. I need to search for facilities without requiring login\n",
    "5. I need to display basic facility information publicly\n",
    "6. I need to get the user's location to show nearby centers\n",
    "7. I should create reusable components for displaying facilities\n",
    "8. I need to show facilities within a reasonable radius\n",
    "9. I should encourage sign-up for additional features\n",
    "\n",
    "Tasks:\n",
    "1. Design public landing page layout\n",
    "2. Create anonymous user session handling\n",
    "3. Implement facility search without authentication\n",
    "4. Display basic facility information publicly \n",
    "5. Design facility component\n",
    "6. Detect user's location via browser API or IP\n",
    "7. Show recycling centers within a radius of the user\n",
    "8. Design facility list display component\n",
    "9. Add \"Sign up for more features\" prompt\n",
    "\"\"\"\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "You are a task decomposition expert. Break down the following user story into specific, actionable technical tasks.\n",
    "Each task should be simple and focused on a single responsibility.\n",
    "\n",
    "Use the Chain of Thought approach: First reason through the problem step by step, then provide the tasks.\n",
    "\n",
    "IMPORTANT: Return your reasoning first, then ONLY the numbered list of tasks. Do NOT include explanatory text or headers after the tasks.\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_cot_examples}\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Reasoning: Let me think through this step by step:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "            \n",
    "            tasks = self._parse_tasks(output_text)\n",
    "            return tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Task extraction failed: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract clean task list from LLM response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        in_tasks_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect when we reach the tasks section\n",
    "            if line.lower().startswith('tasks:'):\n",
    "                in_tasks_section = True\n",
    "                continue\n",
    "            \n",
    "            # Skip reasoning section and headers\n",
    "            if not in_tasks_section:\n",
    "                continue\n",
    "                \n",
    "            # Skip headers, explanatory text, and formatting\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'user story:', 'here are', 'the following', \n",
    "                'broken down', 'specific', 'technical', '**', 'note:'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            clean_task = re.sub(r'^[\\d\\-\\*\\.\\)\\s]+', '', line)\n",
    "            clean_task = re.sub(r'^\\*\\*|\\*\\*$', '', clean_task)\n",
    "            clean_task = clean_task.strip()\n",
    "            \n",
    "            # Only add non-empty, substantial tasks\n",
    "            if clean_task and len(clean_task) > 10:\n",
    "                tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2: Estimate story points using Fibonacci scale\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "    \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        task_points = {}\n",
    "        for task in tasks:\n",
    "            points = await self._estimate_single_task(task)\n",
    "            task_points[task] = points\n",
    "        \n",
    "        total_points = sum(task_points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': task_points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "\n",
    "    async def _estimate_single_task(self, task: str) -> int:\n",
    "        prompt = f\"\"\"\n",
    "Estimate story points for this task using the Fibonacci scale: {self.fibonacci_scale}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Consider:\n",
    "- Technical complexity (simple/moderate/complex)\n",
    "- Uncertainty level (low/medium/high)\n",
    "- Integration requirements\n",
    "- Risk factors\n",
    "\n",
    "Use Chain of Thought reasoning: Think through the complexity factors, then provide the estimate.\n",
    "\n",
    "Reasoning: Let me assess the complexity of this task:\n",
    "[Think through the technical complexity, uncertainty, and effort required]\n",
    "\n",
    "Story Points: [Select from Fibonacci scale]\n",
    "\n",
    "Return ONLY the final number from the Fibonacci scale:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "            \n",
    "            points = self._parse_story_points(output_text)\n",
    "            return points\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Story point estimation failed: {str(e)}\")\n",
    "            return 3  # Default moderate estimate\n",
    "    \n",
    "    def _parse_story_points(self, content: str) -> int:\n",
    "        \"\"\"Extract story points from response\"\"\"\n",
    "        # Look for \"Story Points: X\" pattern\n",
    "        match = re.search(r'story\\s+points?:\\s*(\\d+)', content.lower())\n",
    "        if match:\n",
    "            points = int(match.group(1))\n",
    "            return points if points in self.fibonacci_scale else min(self.fibonacci_scale, key=lambda x: abs(x - points))\n",
    "        \n",
    "        # Look for numbers in Fibonacci scale\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', content)\n",
    "        for num_str in reversed(numbers):\n",
    "            num = int(num_str)\n",
    "            if num in self.fibonacci_scale:\n",
    "                return num\n",
    "        \n",
    "        return 3  # Default\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 3: Map required skills using Chain of Thought\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_cot_examples = \"\"\"\n",
    "Task: Make address text clickable\n",
    "\n",
    "Reasoning: Let me think about what skills are needed for this task:\n",
    "- This involves modifying the user interface to make text clickable\n",
    "- I need to understand how to create interactive elements in the frontend\n",
    "- This is primarily a frontend development task\n",
    "\n",
    "Required Skills:\n",
    "- Frontend development\n",
    "\n",
    "Task: Implement click handler for Google Maps URL\n",
    "\n",
    "Reasoning: Let me analyze what this task requires:\n",
    "- This involves handling user click events\n",
    "- I need to manipulate URLs and format them for Google Maps\n",
    "- I need to understand JavaScript event handling\n",
    "- This is frontend development with JavaScript specifically\n",
    "\n",
    "Required Skills:\n",
    "- Frontend development\n",
    "- JavaScript\n",
    "\n",
    "Task: Design public landing page layout\n",
    "\n",
    "Reasoning: Let me consider what skills this requires:\n",
    "- This involves creating the visual design and layout of a page\n",
    "- I need to understand user experience principles\n",
    "- I need to know how to implement the design in code\n",
    "- This requires both design and development skills\n",
    "\n",
    "Required Skills:\n",
    "- Frontend development\n",
    "- UI/UX design\n",
    "\n",
    "Task: Create anonymous user session handling\n",
    "\n",
    "Reasoning: Let me think through what this involves:\n",
    "- This involves managing user sessions on the server side\n",
    "- I need to handle authentication states and anonymous users\n",
    "- This is server-side logic, not frontend work\n",
    "- This requires backend development skills\n",
    "\n",
    "Required Skills:\n",
    "- Backend development\n",
    "\n",
    "Task: Set up CI/CD pipeline for automated deployments\n",
    "\n",
    "Reasoning: Let me analyze the requirements:\n",
    "- This involves setting up automated build and deployment processes\n",
    "- I need to configure servers and deployment environments\n",
    "- This requires knowledge of containerization and orchestration\n",
    "- This is infrastructure and deployment work\n",
    "\n",
    "Required Skills:\n",
    "- DevOps\n",
    "- Infrastructure management\n",
    "\n",
    "Task: Coordinate user testing sessions with stakeholders\n",
    "\n",
    "Reasoning: Let me think about what this involves:\n",
    "- This requires scheduling and organizing meetings with multiple parties\n",
    "- I need to communicate testing objectives and gather feedback\n",
    "- This involves managing relationships with different teams\n",
    "- This is primarily coordination and communication work\n",
    "\n",
    "Required Skills:\n",
    "- Project management\n",
    "- Communication\n",
    "- Stakeholder management\n",
    "\n",
    "Task: Create marketing campaign for new feature launch\n",
    "\n",
    "Reasoning: Let me consider what this requires:\n",
    "- This involves developing marketing strategy and messaging\n",
    "- I need to understand target audience and market positioning\n",
    "- This requires content creation and campaign execution\n",
    "- This is marketing and business development work\n",
    "\n",
    "Required Skills:\n",
    "- Marketing\n",
    "- Content creation\n",
    "- Business strategy\n",
    "\"\"\"\n",
    "        \n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "Identify the specific skills required to complete this task. Consider both technical and non-technical skills.\n",
    "\n",
    "Available skill categories:\n",
    "TECHNICAL SKILLS:\n",
    "- frontend_development, backend_development, database_management, javascript\n",
    "- mobile_development, cloud_computing, devops, infrastructure_management\n",
    "- data_science, machine_learning, cybersecurity, api_development\n",
    "- testing_qa, automation, system_architecture\n",
    "\n",
    "NON-TECHNICAL SKILLS:\n",
    "- ui_ux_design, graphic_design, product_management, project_management\n",
    "- business_analysis, marketing, sales, customer_service\n",
    "- communication, stakeholder_management, team_leadership\n",
    "- content_creation, technical_writing, training, research\n",
    "\n",
    "Use Chain of Thought reasoning: First think through what the task involves, then identify the skills needed.\n",
    "\n",
    "Return your reasoning first, then ONLY a bulleted list using the standard skill names above.\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_cot_examples}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Reasoning: Let me think about what skills are needed for this task:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "            \n",
    "            skills = self._parse_skills(output_text)\n",
    "            return skills if skills else [\"general_development\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skill mapping failed: {str(e)}\")\n",
    "            return [\"general_development\"]\n",
    "    \n",
    "    def _parse_skills(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract clean skills list from LLM response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        skills = []\n",
    "        in_skills_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect when we reach the skills section\n",
    "            if line.lower().startswith('required skills:'):\n",
    "                in_skills_section = True\n",
    "                continue\n",
    "            \n",
    "            # Skip reasoning section\n",
    "            if not in_skills_section:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers and explanatory text\n",
    "            if any(skip_phrase in line.lower() for skip_phrase in [\n",
    "                'task:', 'here are', 'the following', 'skills needed'\n",
    "            ]):\n",
    "                continue\n",
    "            \n",
    "            # Clean skill from bullet points and special characters\n",
    "            clean_skill = re.sub(r'^[\\-\\*\\s\\u2022]+', '', line)  # Remove bullets including unicode bullet\n",
    "            clean_skill = re.sub(r'[\\u2022\\u2023\\u25E6\\u2043\\u2219]', '', clean_skill)  # Remove all bullet characters\n",
    "            clean_skill = clean_skill.strip()\n",
    "            \n",
    "            if clean_skill and len(clean_skill) > 2:\n",
    "                # Normalize the skill\n",
    "                normalized_skill = self._normalize_skill(clean_skill)\n",
    "                if normalized_skill and normalized_skill not in skills:\n",
    "                    skills.append(normalized_skill)\n",
    "        \n",
    "        return skills\n",
    "    \n",
    "    def _normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"Normalize skills to comprehensive categories including technical and non-technical\"\"\"\n",
    "        skill_lower = skill.lower().strip()\n",
    "        \n",
    "        # Remove any remaining special characters and clean up\n",
    "        skill_lower = re.sub(r'[^\\w\\s]', '', skill_lower)\n",
    "        skill_lower = re.sub(r'\\s+', ' ', skill_lower).strip()\n",
    "        \n",
    "        # Comprehensive skill mapping with technical and non-technical skills\n",
    "        skill_mappings = {\n",
    "            # CORE TECHNICAL SKILLS\n",
    "            'frontend_development': [\n",
    "                'frontend', 'front-end', 'front end', 'ui development', 'client-side', \n",
    "                'client side', 'html', 'css', 'responsive design', 'web development', \n",
    "                'browser', 'dom manipulation', 'web ui'\n",
    "            ],\n",
    "            'backend_development': [\n",
    "                'backend', 'back-end', 'back end', 'server', 'server-side', \n",
    "                'server side', 'api', 'rest api', 'microservices', 'processing logic',\n",
    "                'business logic', 'server development', 'web services'\n",
    "            ],\n",
    "            'database_management': [\n",
    "                'database', 'db', 'sql', 'data storage', 'database skills', \n",
    "                'data', 'query', 'mongodb', 'postgresql', 'mysql', 'data model',\n",
    "                'data modeling', 'database design', 'nosql', 'data warehouse'\n",
    "            ],\n",
    "            'javascript': [\n",
    "                'javascript', 'js', 'scripting', 'client scripting', 'web scripting',\n",
    "                'node.js', 'typescript', 'react', 'angular', 'vue'\n",
    "            ],\n",
    "            'mobile_development': [\n",
    "                'mobile', 'mobile development', 'ios', 'android', 'react native',\n",
    "                'flutter', 'mobile app', 'smartphone', 'tablet'\n",
    "            ],\n",
    "            'cloud_computing': [\n",
    "                'cloud', 'aws', 'azure', 'gcp', 'cloud services', 'serverless',\n",
    "                'lambda', 'cloud infrastructure', 'cloud platform'\n",
    "            ],\n",
    "            'devops': [\n",
    "                'devops', 'dev ops', 'ci/cd', 'continuous integration', 'continuous deployment',\n",
    "                'pipeline', 'build automation', 'deployment automation', 'jenkins',\n",
    "                'gitlab ci', 'github actions'\n",
    "            ],\n",
    "            'infrastructure_management': [\n",
    "                'infrastructure', 'infrastructure management', 'server management',\n",
    "                'docker', 'kubernetes', 'containerization', 'orchestration',\n",
    "                'monitoring', 'deployment', 'system administration'\n",
    "            ],\n",
    "            'data_science': [\n",
    "                'data science', 'data scientist', 'machine learning', 'ml', 'ai',\n",
    "                'artificial intelligence', 'analytics', 'statistical analysis',\n",
    "                'data mining', 'predictive modeling'\n",
    "            ],\n",
    "            'cybersecurity': [\n",
    "                'security', 'cybersecurity', 'information security', 'authentication', \n",
    "                'authorization', 'encryption', 'penetration testing', 'vulnerability',\n",
    "                'access control', 'security audit'\n",
    "            ],\n",
    "            'api_development': [\n",
    "                'api development', 'api design', 'rest', 'graphql', 'soap',\n",
    "                'web api', 'microservices', 'service integration'\n",
    "            ],\n",
    "            'testing_qa': [\n",
    "                'testing', 'qa', 'quality assurance', 'test automation', 'unit testing',\n",
    "                'integration testing', 'usability testing', 'accessibility testing',\n",
    "                'performance testing', 'selenium'\n",
    "            ],\n",
    "            'automation': [\n",
    "                'automation', 'process automation', 'script automation', 'workflow automation',\n",
    "                'robotic process automation', 'rpa'\n",
    "            ],\n",
    "            'system_architecture': [\n",
    "                'system architecture', 'software architecture', 'solution architecture',\n",
    "                'design patterns', 'scalability', 'system design'\n",
    "            ],\n",
    "            \n",
    "            # DESIGN & UX SKILLS\n",
    "            'ui_ux_design': [\n",
    "                'ui ux design', 'ui/ux design', 'ux design', 'ui design', 'user experience',\n",
    "                'user interface', 'interaction design', 'visual design', 'design',\n",
    "                'interface design', 'layout design', 'typography', 'design styles',\n",
    "                'design guidelines', 'wireframing', 'prototyping'\n",
    "            ],\n",
    "            'graphic_design': [\n",
    "                'graphic design', 'visual design', 'brand design', 'logo design',\n",
    "                'illustration', 'photoshop', 'illustrator', 'creative design'\n",
    "            ],\n",
    "            \n",
    "            # PROJECT & PRODUCT MANAGEMENT\n",
    "            'product_management': [\n",
    "                'product management', 'product manager', 'product strategy',\n",
    "                'product development', 'product planning', 'roadmap', 'feature planning'\n",
    "            ],\n",
    "            'project_management': [\n",
    "                'project management', 'project manager', 'project coordination',\n",
    "                'agile', 'scrum', 'kanban', 'planning', 'scheduling', 'resource management',\n",
    "                'timeline management', 'milestone tracking'\n",
    "            ],\n",
    "            'stakeholder_management': [\n",
    "                'stakeholder management', 'stakeholder coordination', 'client management',\n",
    "                'vendor management', 'relationship management', 'negotiation'\n",
    "            ],\n",
    "            'team_leadership': [\n",
    "                'team leadership', 'leadership', 'team management', 'people management',\n",
    "                'mentoring', 'coaching', 'team building'\n",
    "            ],\n",
    "            \n",
    "            # BUSINESS & ANALYSIS\n",
    "            'business_analysis': [\n",
    "                'business analysis', 'business analyst', 'requirements analysis',\n",
    "                'process analysis', 'business requirements', 'functional analysis',\n",
    "                'business process', 'requirements gathering'\n",
    "            ],\n",
    "            'business_strategy': [\n",
    "                'business strategy', 'strategic planning', 'business development',\n",
    "                'market analysis', 'competitive analysis', 'business planning'\n",
    "            ],\n",
    "            'data_analysis': [\n",
    "                'data analysis', 'data analytics', 'analytics', 'reporting', \n",
    "                'data processing', 'data manipulation', 'business intelligence',\n",
    "                'dashboard', 'metrics', 'kpi'\n",
    "            ],\n",
    "            \n",
    "            # MARKETING & SALES\n",
    "            'marketing': [\n",
    "                'marketing', 'digital marketing', 'marketing strategy', 'campaign management',\n",
    "                'social media marketing', 'email marketing', 'seo', 'sem', 'advertising'\n",
    "            ],\n",
    "            'content_creation': [\n",
    "                'content creation', 'content marketing', 'copywriting', 'content strategy',\n",
    "                'blog writing', 'social media content', 'video content'\n",
    "            ],\n",
    "            'sales': [\n",
    "                'sales', 'sales development', 'lead generation', 'customer acquisition',\n",
    "                'sales strategy', 'account management'\n",
    "            ],\n",
    "            'customer_service': [\n",
    "                'customer service', 'customer support', 'customer success',\n",
    "                'help desk', 'customer experience', 'support'\n",
    "            ],\n",
    "            \n",
    "            # COMMUNICATION & DOCUMENTATION\n",
    "            'communication': [\n",
    "                'communication', 'verbal communication', 'written communication',\n",
    "                'presentation', 'public speaking', 'interpersonal skills'\n",
    "            ],\n",
    "            'technical_writing': [\n",
    "                'technical writing', 'documentation', 'technical documentation',\n",
    "                'user manuals', 'api documentation', 'knowledge base'\n",
    "            ],\n",
    "            'training': [\n",
    "                'training', 'training development', 'curriculum development',\n",
    "                'knowledge transfer', 'workshop facilitation', 'education'\n",
    "            ],\n",
    "            'research': [\n",
    "                'research', 'market research', 'user research', 'competitive research',\n",
    "                'analysis', 'investigation', 'data gathering'\n",
    "            ],\n",
    "            \n",
    "            # SPECIALIZED SKILLS\n",
    "            'error_handling': [\n",
    "                'error handling', 'debugging', 'exception handling', 'error management',\n",
    "                'troubleshooting', 'problem solving'\n",
    "            ],\n",
    "            'logging': [\n",
    "                'logging', 'auditing', 'tracking', 'monitoring', 'observability'\n",
    "            ],\n",
    "            'email_integration': [\n",
    "                'email integration', 'email', 'messaging', 'notification',\n",
    "                'email automation', 'smtp'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Check for exact matches first\n",
    "        for standard_skill, variations in skill_mappings.items():\n",
    "            if skill_lower in variations:\n",
    "                return standard_skill\n",
    "            # Check for partial matches\n",
    "            for variation in variations:\n",
    "                if variation in skill_lower or skill_lower in variation:\n",
    "                    return standard_skill\n",
    "        \n",
    "        # If no match found, but it's a valid skill, return it cleaned\n",
    "        if len(skill_lower) > 2 and not skill_lower in ['general', 'development', 'general development']:\n",
    "            return skill_lower.replace(' ', '_')\n",
    "        \n",
    "        # Only return None if we really can't identify the skill\n",
    "        return None\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 4: Analyze dependencies using Chain of Thought\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_cot_examples = \"\"\"\n",
    "Tasks:\n",
    "1. Make address text clickable\n",
    "2. Implement click handler for Google Maps URL\n",
    "3. Open Google Maps in new tab/window\n",
    "4. Add URL encoding for address parameters\n",
    "5. Design facility component\n",
    "6. Create anonymous user session handling\n",
    "7. Implement facility search without authentication\n",
    "\n",
    "Reasoning: Let me analyze these tasks step by step to identify dependencies and assess coupling:\n",
    "- Task 1 (Make address clickable): This needs the address to be displayed first, which would be part of the facility component. If the component design changes, the clickable implementation would need major rework.\n",
    "- Task 2 (Click handler): This needs the clickable element to exist first, so it depends on Task 1. If Task 1 fails, Task 2 would need complete reimplementation.\n",
    "- Task 3 (Open Maps): This needs the URL to be properly formatted, so it depends on Task 2. However, the opening mechanism is somewhat independent, so moderate coupling.\n",
    "- Task 4 (URL encoding): This is part of the URL formatting process, so it should be done alongside Task 2. Loose coupling since it's a utility function.\n",
    "- Task 5 (Design facility component): This is a foundational component that other tasks need\n",
    "- Task 6 (Anonymous sessions): This is independent and can be done in parallel\n",
    "- Task 7 (Facility search): This needs the session handling to be in place for anonymous users. High coupling because search logic is tightly integrated with session management.\n",
    "\n",
    "For coupling assessment:\n",
    "- Tight coupling (8-13 story points): Core architectural dependencies where failure requires major rework\n",
    "- Moderate coupling (3-5 story points): Functional dependencies with some rework needed\n",
    "- Loose coupling (1-2 story points): Utility dependencies with minimal rework\n",
    "\n",
    "Dependencies:\n",
    "- Task 1 depends on Task 5 (rework_effort: 3)\n",
    "- Task 2 depends on Task 1 (rework_effort: 5)\n",
    "- Task 3 depends on Task 2 (rework_effort: 2)\n",
    "- Task 4 depends on Task 2 (rework_effort: 1)\n",
    "- Task 7 depends on Task 6 (rework_effort: 8)\n",
    "\"\"\"\n",
    "        \n",
    "    async def analyze_dependencies(self, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        prompt = f\"\"\"\n",
    "Analyze dependencies between these tasks. Identify which tasks must be completed before others can start.\n",
    "\n",
    "Use Chain of Thought reasoning: First think through each task and its relationships, then assess rework effort.\n",
    "\n",
    "For each dependency, estimate rework effort (1-8 story points) if prerequisite fails:\n",
    "- 1-2: minimal changes, mostly configuration\n",
    "- 3-5: moderate changes, some logic rework\n",
    "- 8: major changes, architectural rework\n",
    "\n",
    "IMPORTANT: \n",
    "- Only return actual dependencies, not every possible combination\n",
    "- After reasoning, return ONLY the dependency list using format: \"- Task X depends on Task Y (rework_effort: POINTS)\"\n",
    "\n",
    "Example:\n",
    "{self.few_shot_cot_examples}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Reasoning: Let me analyze these tasks step by step to identify dependencies:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=800\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "            \n",
    "            dependencies = self._parse_dependencies(output_text, tasks)\n",
    "            return dependencies\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Dependency analysis failed: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _parse_dependencies(self, text: str, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        dependencies = {}\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        in_dependencies_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            # Detect when we reach the dependencies section\n",
    "            if line.lower().startswith('dependencies:'):\n",
    "                in_dependencies_section = True\n",
    "                continue\n",
    "            \n",
    "            # Skip reasoning section\n",
    "            if not in_dependencies_section:\n",
    "                continue\n",
    "                \n",
    "            if \"depends on\" in line.lower():\n",
    "                try:\n",
    "                    # Parse \"Task X depends on Task Y (rework_effort: N)\"\n",
    "                    match = re.search(r'task\\s+(\\d+)\\s+depends\\s+on\\s+task\\s+(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_idx = int(match.group(1)) - 1\n",
    "                        prerequisite_idx = int(match.group(2)) - 1\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 0 <= dependent_idx < len(tasks) and 0 <= prerequisite_idx < len(tasks):\n",
    "                            dependent_task = tasks[dependent_idx]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                'task_id': f\"T_{prerequisite_idx + 1:03d}\",\n",
    "                                'rework_effort': min(8, max(1, rework_effort))\n",
    "                            })\n",
    "                            \n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        return dependencies\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Step 5: Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Validation error: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the Chain of Thought enhanced pipeline\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        # Step 2 & 3: Parallel processing of Story Points and Skills\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Process all tasks in parallel\n",
    "        story_points_tasks = [estimator._estimate_single_task(task) for task in tasks]\n",
    "        skills_tasks = [skills_agent.map_skills(task) for task in tasks]\n",
    "        \n",
    "        story_points_results, skills_results = await asyncio.gather(\n",
    "            asyncio.gather(*story_points_tasks),\n",
    "            asyncio.gather(*skills_tasks)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Dependency Analysis\n",
    "        dependency_agent = DependencyAgent()\n",
    "        dependencies = await dependency_agent.analyze_dependencies(tasks)\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points_results)\n",
    "        \n",
    "        for i, (task, story_points, skills) in enumerate(zip(tasks, story_points_results, skills_results)):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                task_dependencies = dependencies[task]\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": story_points,\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        # Final validation and formatting\n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the Chain of Thought enhanced pipeline\"\"\"\n",
    "    \n",
    "    # Process stories in parallel for efficiency\n",
    "    tasks = [process_user_story_pipeline(story) for story in user_stories]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Handle any exceptions\n",
    "    final_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            final_results.append({\n",
    "                \"input\": user_stories[i],\n",
    "                \"output\": {\n",
    "                    \"error\": str(result),\n",
    "                    \"story_points\": 0,\n",
    "                    \"tasks\": []\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "def print_token_usage():\n",
    "    \"\"\"Print comprehensive token usage statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOKEN USAGE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    breakdown = summary['breakdown']\n",
    "    cost_estimate = summary['cost_estimate']\n",
    "    efficiency = summary['efficiency_metrics']\n",
    "    \n",
    "    print(f\"TOTAL TOKENS CONSUMED: {breakdown['total_consumed']:,}\")\n",
    "    print(f\"ESTIMATED COST: ${cost_estimate['total_cost']:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"BREAKDOWN BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis']\n",
    "    \n",
    "    for category in categories:\n",
    "        if category in breakdown:\n",
    "            cat_data = breakdown[category]\n",
    "            percentage = efficiency['percentage_breakdown'].get(category, 0)\n",
    "            print(f\"{category.replace('_', ' ').title():<25}: {cat_data['total']:>6,} tokens ({percentage:>5.1f}%)\")\n",
    "            print(f\"  {'Input':<23}: {cat_data['input']:>6,} tokens\")\n",
    "            print(f\"  {'Output':<23}: {cat_data['output']:>6,} tokens\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"INPUT COST:  ${cost_estimate['input_cost']:.6f}\")\n",
    "    print(f\"OUTPUT COST: ${cost_estimate['output_cost']:.6f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    results = await process_multiple_user_stories_pipeline(stories_list)\n",
    "    print(\"\\nRESULTS:\")\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print()\n",
    "    print_token_usage()\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_pipeline(user_stories))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949383a4-046f-49c9-b12c-2a6685a27749",
   "metadata": {},
   "source": [
    "- **Main Classes:**\n",
    "\n",
    "> **TokenTracker:** Advanced monitoring system with comprehensive API usage tracking across all pipeline stages, detailed breakdown by category with input/output separation, cost estimation with configurable rates, and efficiency metrics using tiktoken's cl100k_base encoding with intelligent fallback counting for robust token management.\n",
    "\n",
    "> **TaskExtractorAgent:** Few-shot Chain of Thought task decomposition using concrete examples and step-by-step reasoning prompts. Contains few_shot_cot_examples with real user stories and detailed reasoning process. Enhanced parsing separates reasoning from task lists with improved section detection. Key method: decompose(user_story) -> List[str] with robust task validation.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Individual task estimation using Chain of Thought reasoning with Fibonacci scale [1,2,3,5,8,13,21]. Processes each task separately with detailed complexity assessment covering technical complexity, uncertainty level, integration requirements, and risk factors. Key methods: estimate_story_points(user_story, tasks) -> Dict and _estimate_single_task(task) -> int with reasoning-based evaluation.\n",
    "\n",
    "> **RequiredSkillsAgent:** Few-shot Chain of Thought skill mapping with concrete examples showing reasoning process for different task types. Contains few_shot_cot_examples demonstrating skill identification for frontend, backend, design, and management tasks. Advanced normalization system with comprehensive skill mappings covering technical and non-technical domains. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Few-shot Chain of Thought dependency analysis with concrete examples showing coupling assessment and rework effort estimation. Contains few_shot_cot_examples demonstrating dependency reasoning and effort scoring (1-8 story points scale). Enhanced parsing identifies logical and technical relationships. Key method: analyze_dependencies(tasks) -> Dict with reasoning-based dependency detection.\n",
    "\n",
    "> **FormatValidator:** Output structure validation and formatting with comprehensive error handling, field validation, and graceful degradation. Ensures data integrity and structure compliance with detailed validation error reporting. Key method: validate_and_format(user_story, tasks_data, total_story_points) -> Dict.\n",
    "\n",
    "- **Pipeline Functions:**\n",
    "\n",
    "> **process_user_story_pipeline(user_story):** Main single story processing function with few-shot Chain of Thought reasoning across all stages, parallel processing for story points and skills with asyncio.gather(), comprehensive error handling with detailed exception management, and structured output generation with validation.\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories):** Batch processing function handling multiple stories in parallel with individual exception management, graceful error handling for story failures, and comprehensive results aggregation with error reporting.\n",
    "\n",
    "> **print_token_usage():** Enhanced token usage reporting with detailed breakdown by category, cost analysis with input/output separation, efficiency metrics with percentage distribution, and comprehensive statistics across all pipeline stages.\n",
    "\n",
    "> **run_pipeline(stories_list):** Notebook-friendly async wrapper with complete pipeline execution, formatted results output with JSON pretty printing, comprehensive token usage reporting, and structured return values for analysis.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **decompose(user_story: str):** Few-shot Chain of Thought task extraction with concrete examples embedded in few_shot_cot_examples, step-by-step reasoning prompts guiding LLM through structured thinking, enhanced parsing that separates reasoning sections from task lists, and robust task validation with content filtering.\n",
    "\n",
    "> **_estimate_single_task(task: str):** Individual task complexity assessment using Chain of Thought reasoning covering technical complexity analysis, uncertainty level evaluation, integration requirements assessment, and risk factor identification with Fibonacci scale selection and reasoning validation.\n",
    "\n",
    "> **map_skills(task: str):** Few-shot Chain of Thought skill identification with concrete examples in few_shot_cot_examples showing reasoning for different task types (frontend, backend, design, management), comprehensive skill categories covering technical and non-technical domains, and advanced normalization with intelligent matching.\n",
    "\n",
    "> **_normalize_skill(skill: str):** Advanced skill normalization system with comprehensive mappings for technical skills (frontend_development, backend_development, database_management, cloud_computing, devops, cybersecurity) and non-technical skills (ui_ux_design, project_management, business_analysis, communication, marketing), partial matching capabilities, and intelligent categorization.\n",
    "\n",
    "> **analyze_dependencies(tasks: List[str]):** Few-shot Chain of Thought dependency analysis with concrete examples in few_shot_cot_examples demonstrating coupling assessment and rework effort reasoning, step-by-step dependency identification, and effort scoring using story points scale (1-8) with detailed coupling analysis.\n",
    "\n",
    "> **validate_and_format(user_story, tasks_data, total_story_points):** Complete output validation ensuring required fields presence, structure compliance verification, comprehensive error handling with graceful degradation, and detailed validation error reporting with field-level diagnostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990d793-fd95-49d1-bb57-1e4ec115f0d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4.5 Strategy 5: Meta Prompting\n",
    "- **Definition**\n",
    "> **Focuses on the structural and syntactical aspects of tasks rather than specific content details. It creates abstract, reusable frameworks that emphasize the form and pattern of information processing.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Template-driven methodology** for consistency\n",
    "\n",
    "> **Abstract pattern creation** for reusability\n",
    "\n",
    "> **Structural emphasis** over content specifics\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Creates consistent output structure and quality token efficiency**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Requires significant upfront investment in framework design**\n",
    "\n",
    ">**May be overly rigid for simple stories**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b6bef-1efd-4acf-8f2c-0073186ca45f",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2156e63-5c56-4d0d-b44c-47380e7c2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TASK_EXTRACTION] Tokens - Input: 101, Output: 116, Total: 217\n",
      "[TASK_EXTRACTION] Tokens - Input: 101, Output: 137, Total: 238\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 97, Output: 1, Total: 98\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 99, Output: 1, Total: 100\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 92, Output: 1, Total: 93\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 95, Output: 1, Total: 96\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 96, Output: 1, Total: 97\n",
      "[REQUIRED_SKILLS] Tokens - Input: 106, Output: 13, Total: 119\n",
      "[REQUIRED_SKILLS] Tokens - Input: 108, Output: 14, Total: 122\n",
      "[REQUIRED_SKILLS] Tokens - Input: 101, Output: 13, Total: 114\n",
      "[REQUIRED_SKILLS] Tokens - Input: 104, Output: 14, Total: 118\n",
      "[REQUIRED_SKILLS] Tokens - Input: 105, Output: 13, Total: 118\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 94, Output: 1, Total: 95\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 100, Output: 1, Total: 101\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 110, Output: 1, Total: 111\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 97, Output: 1, Total: 98\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 97, Output: 1, Total: 98\n",
      "[REQUIRED_SKILLS] Tokens - Input: 103, Output: 8, Total: 111\n",
      "[REQUIRED_SKILLS] Tokens - Input: 109, Output: 15, Total: 124\n",
      "[REQUIRED_SKILLS] Tokens - Input: 119, Output: 10, Total: 129\n",
      "[REQUIRED_SKILLS] Tokens - Input: 106, Output: 13, Total: 119\n",
      "[REQUIRED_SKILLS] Tokens - Input: 106, Output: 14, Total: 120\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 191, Output: 79, Total: 270\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 212, Output: 93, Total: 305\n",
      "\n",
      "RESULTS:\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 42,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design a database schema to store user account information, including username, password, and email address.\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"security_implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement a registration form on the website that collects user input for account creation, including validation for required fields.\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 4\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"testing_qa\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a password hashing algorithm to securely store user passwords in the database.\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"security_implementation\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Build an API endpoint to handle account creation requests, including input validation and error handling.\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"testing_qa\",\n",
      "          \"security_implementation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication logic to verify user credentials on subsequent logins and grant access to personalized features.\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"security_implementation\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 57,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design a data model to store system performance metrics, including data types and schema.\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data ingestion pipeline to collect system performance data from various sources (e.g. logs, APIs, etc.).\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 4\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"integration_development\",\n",
      "          \"backend_development\",\n",
      "          \"devops_deployment\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a dashboard layout with necessary UI components (charts, graphs, tables, etc.) using a front-end framework (e.g. React, Angular, etc.).\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Build a backend API to retrieve and process system performance data, and expose it to the dashboard.\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"integration_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate the dashboard with the backend API, and implement data visualization and rendering on the dashboard.\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 13,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 1\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"integration_development\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 3,211\n",
      "ESTIMATED COST: $0.037730\n",
      "\n",
      "BREAKDOWN BY CATEGORY:\n",
      "--------------------------------------------------\n",
      "Task Extraction          :    455 tokens ( 14.2%)\n",
      "  Input                  :    202 tokens\n",
      "  Output                 :    253 tokens\n",
      "\n",
      "Story Point Estimation   :    987 tokens ( 30.7%)\n",
      "  Input                  :    977 tokens\n",
      "  Output                 :     10 tokens\n",
      "\n",
      "Required Skills          :  1,194 tokens ( 37.2%)\n",
      "  Input                  :  1,067 tokens\n",
      "  Output                 :    127 tokens\n",
      "\n",
      "Dependency Analysis      :    575 tokens ( 17.9%)\n",
      "  Input                  :    403 tokens\n",
      "  Output                 :    172 tokens\n",
      "\n",
      "INPUT COST:  $0.026490\n",
      "OUTPUT COST: $0.011240\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        # Groq pricing (example rates)\n",
    "        input_rate = 0.00001  # per token\n",
    "        output_rate = 0.00002  # per token\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Task extraction using meta-prompting with structured templates\"\"\"\n",
    "    \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "Break down this user story into 4-6 specific, actionable technical tasks.\n",
    "\n",
    "USER STORY: {user_story}\n",
    "\n",
    "Requirements:\n",
    "- Each task should be implementable by a developer\n",
    "- Focus on concrete technical work\n",
    "- Use action verbs (create, implement, design, build, etc.)\n",
    "- Keep tasks specific and focused\n",
    "\n",
    "Return ONLY a numbered list of tasks, nothing else:\n",
    "\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3,\n",
    "                max_tokens=600\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('task_extraction', prompt, output_text)\n",
    "            \n",
    "            tasks = self._parse_tasks(output_text)\n",
    "            return tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            return []\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract clean task list from LLM response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Extract task from numbered list\n",
    "            if re.match(r'^\\d+\\.', line):\n",
    "                clean_task = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "                if clean_task and len(clean_task) > 10:\n",
    "                    tasks.append(clean_task)\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Story point estimation using Fibonacci scale\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "\n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        task_points = {}\n",
    "        for task in tasks:\n",
    "            points = await self._estimate_single_task(task)\n",
    "            task_points[task] = points\n",
    "        \n",
    "        total_points = sum(task_points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': task_points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "        \n",
    "    async def _estimate_single_task(self, task: str) -> int:\n",
    "        prompt = f\"\"\"\n",
    "Estimate story points for this task using the Fibonacci scale: {self.fibonacci_scale}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Consider:\n",
    "- Technical complexity (simple/moderate/complex)\n",
    "- Uncertainty level (low/medium/high)\n",
    "- Integration requirements\n",
    "- Risk factors\n",
    "\n",
    "Return ONLY the number from the Fibonacci scale, nothing else:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "            \n",
    "            points = self._parse_story_points(output_text)\n",
    "            return points\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 3  # Default moderate estimate\n",
    "    \n",
    "    def _parse_story_points(self, content: str) -> int:\n",
    "        \"\"\"Extract story points from response\"\"\"\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', content)\n",
    "        for num_str in numbers:\n",
    "            num = int(num_str)\n",
    "            if num in self.fibonacci_scale:\n",
    "                return num\n",
    "        return 3\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Skills mapping with clean output\"\"\"\n",
    "    \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "Identify 2-4 specific technical skills needed for this task:\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Choose from these skill categories:\n",
    "- frontend_development\n",
    "- backend_development\n",
    "- database_management\n",
    "- ui_ux_design\n",
    "- integration_development\n",
    "- testing_qa\n",
    "- devops_deployment\n",
    "- security_implementation\n",
    "\n",
    "Return ONLY a simple list with dashes, nothing else:\n",
    "\n",
    "- skill1\n",
    "- skill2\n",
    "- skill3\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.3,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "            \n",
    "            skills = self._parse_skills(output_text)\n",
    "            return skills if skills else [\"general_development\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return [\"general_development\"]\n",
    "    \n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "    def _parse_skills(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract skills from response\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        skills = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('-'):\n",
    "                skill = line.lstrip('- ').strip()\n",
    "                if skill and len(skill) > 2:\n",
    "                    normalized_skill = self._normalize_skill(skill)\n",
    "                    if normalized_skill and normalized_skill not in skills:\n",
    "                        skills.append(normalized_skill)\n",
    "        \n",
    "        return skills\n",
    "    \n",
    "    def _normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"Normalize skills to standard categories\"\"\"\n",
    "        skill_lower = skill.lower()\n",
    "        \n",
    "        skill_mapping = {\n",
    "            'frontend_development': ['frontend', 'front-end', 'ui', 'client-side', 'react', 'angular', 'vue', 'javascript', 'html', 'css'],\n",
    "            'backend_development': ['backend', 'back-end', 'server', 'api', 'logic', 'node.js', 'python', 'java'],\n",
    "            'database_management': ['database', 'db', 'sql', 'data', 'storage', 'mongodb', 'postgresql'],\n",
    "            'ui_ux_design': ['design', 'ux', 'ui design', 'user experience', 'visual'],\n",
    "            'integration_development': ['integration', 'api integration', 'external', 'service', 'microservices'],\n",
    "            'testing_qa': ['testing', 'qa', 'quality assurance', 'unit test'],\n",
    "            'devops_deployment': ['devops', 'deployment', 'ci/cd', 'docker', 'kubernetes'],\n",
    "            'security_implementation': ['security', 'authentication', 'authorization', 'encryption']\n",
    "        }\n",
    "        \n",
    "        for standard_skill, keywords in skill_mapping.items():\n",
    "            if any(keyword in skill_lower for keyword in keywords):\n",
    "                return standard_skill\n",
    "                \n",
    "        return skill_lower.replace(' ', '_') if len(skill) > 2 else None\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Dependency analysis with clean output\"\"\"\n",
    "        \n",
    "    async def analyze_dependencies(self, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        prompt = f\"\"\"\n",
    "Analyze dependencies between these tasks. Identify which tasks must be completed before others can start.\n",
    "\n",
    "TASKS:\n",
    "{tasks_str}\n",
    "\n",
    "For each dependency, estimate rework effort (1-8 story points) if the prerequisite fails.\n",
    "\n",
    "Return ONLY dependencies in this exact format, nothing else:\n",
    "\n",
    "Task X depends on Task Y (rework_effort: N)\n",
    "Task A depends on Task B (rework_effort: M)\n",
    "\n",
    "If no dependencies exist, return: No dependencies found\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-70b-8192\",\n",
    "                temperature=0.2,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content.strip()\n",
    "            token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "            \n",
    "            dependencies = self._parse_dependencies(output_text, tasks)\n",
    "            return dependencies\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {}\n",
    "    \n",
    "    def _parse_dependencies(self, text: str, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        dependencies = {}\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"depends on\" in line.lower():\n",
    "                try:\n",
    "                    # Parse \"Task X depends on Task Y (rework_effort: N)\"\n",
    "                    match = re.search(r'task\\s+(\\d+)\\s+depends\\s+on\\s+task\\s+(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_idx = int(match.group(1)) - 1\n",
    "                        prerequisite_idx = int(match.group(2)) - 1\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 0 <= dependent_idx < len(tasks) and 0 <= prerequisite_idx < len(tasks):\n",
    "                            dependent_task = tasks[dependent_idx]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                'task_id': f\"T_{prerequisite_idx + 1:03d}\",\n",
    "                                'rework_effort': min(8, max(1, rework_effort))\n",
    "                            })\n",
    "                            \n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        return dependencies\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the pipeline\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        # Step 2 & 3: Parallel processing of Story Points and Skills\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Process all tasks in parallel\n",
    "        story_points_tasks = [estimator._estimate_single_task(task) for task in tasks]\n",
    "        skills_tasks = [skills_agent.map_skills(task) for task in tasks]\n",
    "        \n",
    "        story_points_results, skills_results = await asyncio.gather(\n",
    "            asyncio.gather(*story_points_tasks),\n",
    "            asyncio.gather(*skills_tasks)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Dependency Analysis\n",
    "        dependency_agent = DependencyAgent()\n",
    "        dependencies = await dependency_agent.analyze_dependencies(tasks)\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points_results)\n",
    "        \n",
    "        for i, (task, story_points, skills) in enumerate(zip(tasks, story_points_results, skills_results)):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                task_dependencies = dependencies[task]\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": story_points,\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        # Final validation and formatting\n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the pipeline\"\"\"\n",
    "    \n",
    "    # Process stories in parallel for efficiency\n",
    "    tasks = [process_user_story_pipeline(story) for story in user_stories]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Handle any exceptions\n",
    "    final_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            final_results.append({\n",
    "                \"input\": user_stories[i],\n",
    "                \"output\": {\n",
    "                    \"error\": str(result),\n",
    "                    \"story_points\": 0,\n",
    "                    \"tasks\": []\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "def print_token_usage():\n",
    "    \"\"\"Print comprehensive token usage statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOKEN USAGE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    breakdown = summary['breakdown']\n",
    "    cost_estimate = summary['cost_estimate']\n",
    "    efficiency = summary['efficiency_metrics']\n",
    "    \n",
    "    print(f\"TOTAL TOKENS CONSUMED: {breakdown['total_consumed']:,}\")\n",
    "    print(f\"ESTIMATED COST: ${cost_estimate['total_cost']:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"BREAKDOWN BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis']\n",
    "    \n",
    "    for category in categories:\n",
    "        if category in breakdown:\n",
    "            cat_data = breakdown[category]\n",
    "            percentage = efficiency['percentage_breakdown'].get(category, 0)\n",
    "            print(f\"{category.replace('_', ' ').title():<25}: {cat_data['total']:>6,} tokens ({percentage:>5.1f}%)\")\n",
    "            print(f\"  {'Input':<23}: {cat_data['input']:>6,} tokens\")\n",
    "            print(f\"  {'Output':<23}: {cat_data['output']:>6,} tokens\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"INPUT COST:  ${cost_estimate['input_cost']:.6f}\")\n",
    "    print(f\"OUTPUT COST: ${cost_estimate['output_cost']:.6f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    results = await process_multiple_user_stories_pipeline(stories_list)\n",
    "    print(\"\\nRESULTS:\")\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print()\n",
    "    print_token_usage()\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_pipeline(user_stories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554b22f-74e4-4bc7-bfec-3a2554646df8",
   "metadata": {},
   "source": [
    "- **Main Classes:**\n",
    "\n",
    "> **TokenTracker:** Comprehensive monitoring system with detailed API usage tracking across all pipeline stages, category-specific breakdown with input/output separation, cost estimation with configurable Groq pricing rates, and efficiency metrics using tiktoken's cl100k_base encoding with intelligent fallback counting.\n",
    "\n",
    "> **TaskExtractorAgent:** Meta-prompting task decomposition using structured templates with clear requirements and formatting constraints. Generates 4-6 specific, actionable technical tasks with action verbs and concrete implementation focus. Key method: decompose(user_story) -> List[str] with clean numbered list parsing.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Individual task estimation using Fibonacci scale [1,2,3,5,8,13,21] with structured prompts focusing on complexity factors. Processes each task separately with consideration of technical complexity, uncertainty, integration requirements, and risk factors. Key methods: estimate_story_points(user_story, tasks) -> Dict and _estimate_single_task(task) -> int.\n",
    "\n",
    "> **RequiredSkillsAgent:** Skills mapping with clean output using predefined skill categories and structured templates. Maps 2-4 specific technical skills per task from standardized categories including frontend_development, backend_development, database_management, ui_ux_design, integration_development, testing_qa, devops_deployment, security_implementation. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Dependency analysis with clean output formatting using structured templates. Identifies logical and technical dependencies with rework effort estimation (1-8 story points scale). Uses precise parsing format for consistent dependency identification. Key method: analyze_dependencies(tasks) -> Dict with clean dependency parsing.\n",
    "\n",
    "> **FormatValidator:** Output structure validation and formatting with comprehensive error handling and field validation. Ensures data integrity and structure compliance with detailed validation error reporting. Key method: validate_and_format(user_story, tasks_data, total_story_points) -> Dict.\n",
    "\n",
    "- **Pipeline Functions:**\n",
    "\n",
    "> **process_user_story_pipeline(user_story):** Main single story processing function with meta-prompting across all stages, parallel processing for story points and skills using asyncio.gather(), comprehensive error handling with detailed exception management, and structured output generation with validation.\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories):** Batch processing function handling multiple stories in parallel with individual exception management, graceful error handling for story failures, and comprehensive results aggregation with error reporting.\n",
    "\n",
    "> **print_token_usage():** Detailed token usage reporting with category-specific breakdown, cost analysis with input/output separation, efficiency metrics with percentage distribution, and comprehensive statistics across all pipeline stages.\n",
    "\n",
    "> **run_pipeline(stories_list):** Notebook-friendly async wrapper with complete pipeline execution, formatted results output with JSON pretty printing, comprehensive token usage reporting, and structured return values for analysis.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **decompose(user_story: str):** Meta-prompting task extraction with structured template requiring 4-6 specific, actionable technical tasks using action verbs. Clean numbered list parsing with regex extraction ensuring implementable developer tasks.\n",
    "\n",
    "> **_estimate_single_task(task: str):** Individual task complexity assessment with structured prompts covering technical complexity, uncertainty level, integration requirements, and risk factors. Returns only Fibonacci scale numbers with clean numeric parsing.\n",
    "\n",
    "> **map_skills(task: str):** Structured skill identification using predefined categories with clean dash-list output format. Maps 2-4 specific technical skills from standardized categories with normalized skill names and consistent formatting.\n",
    "\n",
    "> **_normalize_skill(skill: str):** Skill normalization system with predefined mappings for technical categories including frontend_development, backend_development, database_management, ui_ux_design, integration_development, testing_qa, devops_deployment, security_implementation.\n",
    "\n",
    "> **analyze_dependencies(tasks: List[str]):** Clean dependency analysis with structured output format requiring exact parsing template. Identifies logical and technical dependencies with rework effort scoring (1-8 story points) and precise dependency format parsing.\n",
    "\n",
    "> **validate_and_format(user_story, tasks_data, total_story_points):** Complete output validation ensuring required fields presence, structure compliance verification, comprehensive error handling with graceful degradation, and detailed validation error reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d81d8-36e0-4b9b-a64e-e2161e126d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4.6 Strategy 6: tree-of-thought Prompting\n",
    "- **Definition**\n",
    "> **Enables the LM to self-evaluate the progress through intermediate thoughts made towards solving a problem through a deliberate reasoning process.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Multi-path exploration** of reasoning branches\n",
    "\n",
    "> **Self-evaluation mechanism** at each thinking step\n",
    "\n",
    "> **Deliberate reasoning process** with progress assessment\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Discovers multiple valid approaches**\n",
    "\n",
    ">**Excellent for complex, ambiguous user stories**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Very high complexity and token usage**\n",
    "\n",
    ">**May be overly rigid for simple stories**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1925a37-9431-4c3c-a4f8-9ee7eab8087e",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c8c565c-60fd-4e1b-814f-fea5aea8954e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ³ Tree of Thoughts Enhanced Multi-Agent User Story Pipeline\n",
      "======================================================================\n",
      "ðŸ”® Features:\n",
      "  â€¢ Tree of Thoughts exploration for each agent\n",
      "  â€¢ Few-shot learning with comprehensive examples\n",
      "  â€¢ Parallel processing for optimal performance\n",
      "  â€¢ Context preservation throughout pipeline\n",
      "  â€¢ Timeout controls and fallback mechanisms\n",
      "  â€¢ Token usage tracking and cost estimation\n",
      "======================================================================\n",
      "\n",
      "ðŸš€ Starting Tree of Thoughts Multi-Agent Pipeline\n",
      "ðŸ“Š Processing 2 user stories...\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Story 1/2\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to create an account so that I can access ...\n",
      "================================================================================\n",
      "ðŸ“ Step 1: Extracting tasks with Tree of Thoughts...\n",
      "ðŸ” Extracting tasks from: As a user, I want to create an account so that I can access ...\n",
      "  ðŸŒ³ Starting task_extraction tree exploration (timeout: 25s)...\n",
      "[TASK_EXTRACTION] Tokens - Input: 203, Output: 69, Total: 272\n",
      "[TASK_EXTRACTION] Tokens - Input: 203, Output: 68, Total: 271\n",
      "[TASK_EXTRACTION] Tokens - Input: 255, Output: 69, Total: 324\n",
      "[TASK_EXTRACTION] Tokens - Input: 255, Output: 71, Total: 326\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "[TASK_EXTRACTION] Tokens - Input: 254, Output: 88, Total: 342\n",
      "[TASK_EXTRACTION] Tokens - Input: 254, Output: 68, Total: 322\n",
      "  âœ… Completed task_extraction exploration: 2 solutions found\n",
      "  âœ… Generated 7 unique tasks\n",
      "âš¡ Step 2: Parallel estimation (Story Points & Skills) with Tree of Thoughts...\n",
      "ðŸ“Š Estimating story points for 7 tasks...\n",
      "  ðŸŒ³ Starting story_point_estimation tree exploration (timeout: 20s)...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 276, Output: 60, Total: 336\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 276, Output: 60, Total: 336\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed story_point_estimation exploration: 2 solutions found\n",
      "ðŸ› ï¸ Identifying skills for 7 tasks...\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 127, Output: 39, Total: 166\n",
      "[REQUIRED_SKILLS] Tokens - Input: 127, Output: 47, Total: 174\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 39, Total: 167\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 39, Total: 167\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 130, Output: 32, Total: 162\n",
      "[REQUIRED_SKILLS] Tokens - Input: 130, Output: 32, Total: 162\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 133, Output: 35, Total: 168\n",
      "[REQUIRED_SKILLS] Tokens - Input: 133, Output: 35, Total: 168\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 132, Output: 33, Total: 165\n",
      "[REQUIRED_SKILLS] Tokens - Input: 132, Output: 25, Total: 157\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 42, Total: 170\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 31, Total: 159\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 31, Total: 159\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 32, Total: 160\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  âœ… Identified skills for 7 tasks\n",
      "ðŸ”— Step 3: Analyzing dependencies with Tree of Thoughts...\n",
      "ðŸ”— Analyzing dependencies for 7 tasks...\n",
      "  ðŸŒ³ Starting dependency_analysis tree exploration (timeout: 20s)...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 104, Total: 364\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 118, Total: 378\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 86, Total: 346\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 135, Total: 395\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 85, Total: 345\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 260, Output: 102, Total: 362\n",
      "  âœ… Completed dependency_analysis exploration: 2 solutions found\n",
      "  âœ… Found 4 dependency relationships\n",
      "ðŸ“‹ Step 4: Formatting and validating output...\n",
      "âœ… Validating and formatting final output...\n",
      "  âœ… Format validation successful\n",
      "ðŸŽ‰ Story processing complete!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Story 2/2\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to view analytics dashboard so that I ca...\n",
      "================================================================================\n",
      "ðŸ“ Step 1: Extracting tasks with Tree of Thoughts...\n",
      "ðŸ” Extracting tasks from: As an admin, I want to view analytics dashboard so that I ca...\n",
      "  ðŸŒ³ Starting task_extraction tree exploration (timeout: 25s)...\n",
      "[TASK_EXTRACTION] Tokens - Input: 203, Output: 82, Total: 285\n",
      "[TASK_EXTRACTION] Tokens - Input: 203, Output: 91, Total: 294\n",
      "[TASK_EXTRACTION] Tokens - Input: 268, Output: 72, Total: 340\n",
      "[TASK_EXTRACTION] Tokens - Input: 268, Output: 70, Total: 338\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "[TASK_EXTRACTION] Tokens - Input: 277, Output: 89, Total: 366\n",
      "[TASK_EXTRACTION] Tokens - Input: 277, Output: 91, Total: 368\n",
      "  âœ… Completed task_extraction exploration: 2 solutions found\n",
      "  âœ… Generated 7 unique tasks\n",
      "âš¡ Step 2: Parallel estimation (Story Points & Skills) with Tree of Thoughts...\n",
      "ðŸ“Š Estimating story points for 7 tasks...\n",
      "  ðŸŒ³ Starting story_point_estimation tree exploration (timeout: 20s)...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 292, Output: 60, Total: 352\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 292, Output: 60, Total: 352\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed story_point_estimation exploration: 2 solutions found\n",
      "ðŸ› ï¸ Identifying skills for 7 tasks...\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 29, Total: 157\n",
      "[REQUIRED_SKILLS] Tokens - Input: 128, Output: 29, Total: 157\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 136, Output: 38, Total: 174\n",
      "[REQUIRED_SKILLS] Tokens - Input: 136, Output: 38, Total: 174\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 131, Output: 31, Total: 162\n",
      "[REQUIRED_SKILLS] Tokens - Input: 131, Output: 32, Total: 163\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 131, Output: 34, Total: 165\n",
      "[REQUIRED_SKILLS] Tokens - Input: 131, Output: 30, Total: 161\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 134, Output: 21, Total: 155\n",
      "[REQUIRED_SKILLS] Tokens - Input: 134, Output: 22, Total: 156\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 133, Output: 39, Total: 172\n",
      "[REQUIRED_SKILLS] Tokens - Input: 133, Output: 42, Total: 175\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  ðŸŒ³ Starting required_skills tree exploration (timeout: 15s)...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 129, Output: 30, Total: 159\n",
      "[REQUIRED_SKILLS] Tokens - Input: 129, Output: 31, Total: 160\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "  âœ… Completed required_skills exploration: 2 solutions found\n",
      "  âœ… Identified skills for 7 tasks\n",
      "ðŸ”— Step 3: Analyzing dependencies with Tree of Thoughts...\n",
      "ðŸ”— Analyzing dependencies for 7 tasks...\n",
      "  ðŸŒ³ Starting dependency_analysis tree exploration (timeout: 20s)...\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 101, Total: 377\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 133, Total: 409\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 101, Total: 377\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 101, Total: 377\n",
      "    ðŸ“Š Iteration 3, exploring depth 1\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 101, Total: 377\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 276, Output: 97, Total: 373\n",
      "  âœ… Completed dependency_analysis exploration: 2 solutions found\n",
      "  âœ… Found 5 dependency relationships\n",
      "ðŸ“‹ Step 4: Formatting and validating output...\n",
      "âœ… Validating and formatting final output...\n",
      "  âœ… Format validation successful\n",
      "ðŸŽ‰ Story processing complete!\n",
      "================================================================================\n",
      "\n",
      "âœ… Pipeline completed! Processed 2 stories\n",
      "\n",
      "â±ï¸ Total processing time: 95.76 seconds\n",
      "==========================================================================================\n",
      "ðŸŒ³ TREE OF THOUGHTS MULTI-AGENT PIPELINE RESULTS\n",
      "==========================================================================================\n",
      "ðŸ“Š TOTAL TOKENS CONSUMED: 14298\n",
      "ðŸ’° ESTIMATED COST: $0.176680\n",
      "\n",
      "ðŸ” TOKEN BREAKDOWN BY AGENT:\n",
      "  Task Extraction: 3848 tokens\n",
      "  Story Point Estimation: 1376 tokens\n",
      "  Required Skills: 4594 tokens\n",
      "  Dependency Analysis: 4480 tokens\n",
      "\n",
      "==========================================================================================\n",
      "ðŸ“‹ PROCESSED USER STORIES\n",
      "==========================================================================================\n",
      "\n",
      "--- ðŸ“– Story 1 ---\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 28,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design user registration form interface\",\n",
      "        \"id\": \"CAA_001\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"ui_design\",\n",
      "          \"form_design\",\n",
      "          \"frontend\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design user profile creation workflow UI\",\n",
      "        \"id\": \"CAA_002\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"user_profile_creation_workflow_ui\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create visual design concept for account activation process\",\n",
      "        \"id\": \"CAA_003\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"create_visual_design_concept_for_account_activation_process\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop responsive design for registration form on mobile and tablet devices\",\n",
      "        \"id\": \"CAA_004\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"CAA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"develop_responsive_design_for_registration_form_on_mobile_and_tablet_devices\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design error messaging and validation feedback for registration form fields\",\n",
      "        \"id\": \"CAA_005\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"CAA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"CAA_003\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_messaging_and_validation_feedback_for_registration_form_fields\",\n",
      "          \"ui_design\",\n",
      "          \"form_design\",\n",
      "          \"frontend\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement user registration form backend API\",\n",
      "        \"id\": \"CAA_006\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"CAA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"CAA_005\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"implement_user_registration_form_backend_api\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop password hashing and storage mechanism\",\n",
      "        \"id\": \"CAA_007\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"CAA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"develop_password_hashing_and_storage_mechanism\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "--- ðŸ“– Story 2 ---\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 28,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout and components\",\n",
      "        \"id\": \"AVA_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 1\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"analytics_dashboard_layout_and_components\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create wireframes for key performance indicators (KPIs) and metrics\",\n",
      "        \"id\": \"AVA_002\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"create_wireframes_for_key_performance_indicators_kpis_and_metrics\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop a color scheme and typography for the dashboard\",\n",
      "        \"id\": \"AVA_003\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"develop_a_color_scheme_and_typography_for_the_dashboard\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design interactive elements for filtering and date range selection\",\n",
      "        \"id\": \"AVA_004\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"interactive_elements_for_filtering_and_date_range_selection\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a responsive design for the dashboard to accommodate different screen sizes\",\n",
      "        \"id\": \"AVA_005\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_004\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"responsive_design\",\n",
      "          \"ui_design\",\n",
      "          \"frontend\",\n",
      "          \"css_media_queries\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data aggregation and processing for KPIs and metrics\",\n",
      "        \"id\": \"AVA_006\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_002\",\n",
      "            \"reward_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"AVA_003\",\n",
      "            \"reward_effort\": 1\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"AVA_001\",\n",
      "            \"reward_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"implement_data_aggregation_and_processing_for_kpis_and_metrics\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop API endpoint for retrieving dashboard data\",\n",
      "        \"id\": \"AVA_007\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"AVA_006\",\n",
      "            \"reward_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"develop_api_endpoint_for_retrieving_dashboard_data\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "==========================================================================================\n",
      "ðŸ“ˆ PIPELINE STATISTICS\n",
      "==========================================================================================\n",
      "âœ… Successfully processed: 2/2 stories\n",
      "ðŸ“ Total tasks generated: 14\n",
      "ðŸ“Š Total story points: 56\n",
      "ðŸ”— Tasks with dependencies: 9\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import deque\n",
    "from dataclasses import dataclass, field\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "@dataclass\n",
    "class Thought:\n",
    "    \"\"\"Represents a thought node in the Tree of Thoughts\"\"\"\n",
    "    content: str\n",
    "    depth: int\n",
    "    parent: Optional['Thought']\n",
    "    children: List['Thought'] = field(default_factory=list)\n",
    "    evaluation_score: float = 0.0\n",
    "    is_final: bool = False\n",
    "    thought_type: str = \"exploration\"\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "class OptimizedTreeOfThoughts:\n",
    "    \"\"\"Optimized Tree of Thoughts framework for pipeline agents\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth: int = 2, branching_factor: int = 2, timeout_seconds: int = 20):\n",
    "        self.max_depth = max_depth\n",
    "        self.branching_factor = branching_factor\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "        self.start_time = None\n",
    "    \n",
    "    async def explore_tree(self, initial_problem: str, agent_type: str, context: Dict[str, Any] = None) -> List[str]:\n",
    "        \"\"\"Explore the tree of thoughts with context preservation\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        print(f\"  ðŸŒ³ Starting {agent_type} tree exploration (timeout: {self.timeout_seconds}s)...\")\n",
    "        \n",
    "        # Initialize root thought\n",
    "        root_thought = Thought(\n",
    "            content=initial_problem,\n",
    "            depth=0,\n",
    "            parent=None\n",
    "        )\n",
    "        \n",
    "        # Use breadth-first search with timeout\n",
    "        queue = deque([root_thought])\n",
    "        final_solutions = []\n",
    "        iterations = 0\n",
    "        \n",
    "        while queue and len(final_solutions) < self.branching_factor and not self._is_timeout():\n",
    "            current_thought = queue.popleft()\n",
    "            iterations += 1\n",
    "            \n",
    "            if iterations % 3 == 0:\n",
    "                print(f\"    ðŸ“Š Iteration {iterations}, exploring depth {current_thought.depth}\")\n",
    "            \n",
    "            if current_thought.depth >= self.max_depth:\n",
    "                current_thought.is_final = True\n",
    "                current_thought.thought_type = \"solution\"\n",
    "                if current_thought.evaluation_score > 0.4:\n",
    "                    final_solutions.append(current_thought.content)\n",
    "                continue\n",
    "            \n",
    "            # Generate child thoughts with context\n",
    "            if self._is_timeout():\n",
    "                print(f\"    â° Timeout reached, finalizing exploration\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                child_thoughts = await self._generate_thoughts(current_thought, agent_type, context)\n",
    "                \n",
    "                # Evaluate and add promising thoughts\n",
    "                for child_thought in child_thoughts:\n",
    "                    if self._is_timeout():\n",
    "                        break\n",
    "                        \n",
    "                    evaluation_score = await self._evaluate_thought(child_thought, agent_type)\n",
    "                    child_thought.evaluation_score = evaluation_score\n",
    "                    child_thought.parent = current_thought\n",
    "                    current_thought.children.append(child_thought)\n",
    "                    \n",
    "                    if evaluation_score > 0.3:\n",
    "                        queue.append(child_thought)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ Error in thought generation: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  âœ… Completed {agent_type} exploration: {len(final_solutions)} solutions found\")\n",
    "        \n",
    "        # If no solutions found, use the best intermediate thoughts\n",
    "        if not final_solutions:\n",
    "            all_thoughts = self._collect_all_thoughts(root_thought)\n",
    "            best_thoughts = sorted(all_thoughts, key=lambda t: t.evaluation_score, reverse=True)[:2]\n",
    "            final_solutions = [t.content for t in best_thoughts if t.evaluation_score > 0.2]\n",
    "        \n",
    "        return self._select_best_solutions(final_solutions)\n",
    "    \n",
    "    def _is_timeout(self) -> bool:\n",
    "        return time.time() - self.start_time > self.timeout_seconds\n",
    "    \n",
    "    def _collect_all_thoughts(self, root: Thought) -> List[Thought]:\n",
    "        thoughts = []\n",
    "        queue = deque([root])\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            thoughts.append(current)\n",
    "            queue.extend(current.children)\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    async def _generate_thoughts(self, parent_thought: Thought, agent_type: str, context: Dict[str, Any] = None) -> List[Thought]:\n",
    "        \"\"\"Generate child thoughts using few-shot prompts\"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        for i in range(self.branching_factor):\n",
    "            if self._is_timeout():\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                temperature = 0.4 + (i * 0.2)\n",
    "                prompt = self._create_few_shot_prompt(parent_thought, agent_type, i, context)\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=min(temperature, 0.8),\n",
    "                    max_tokens=800\n",
    "                )\n",
    "                \n",
    "                thought_content = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Track token usage\n",
    "                token_tracker.track_api_call(agent_type, prompt, thought_content)\n",
    "                \n",
    "                thought = Thought(\n",
    "                    content=thought_content,\n",
    "                    depth=parent_thought.depth + 1,\n",
    "                    parent=parent_thought,\n",
    "                    thought_type=\"refinement\" if parent_thought.depth > 0 else \"exploration\"\n",
    "                )\n",
    "                thoughts.append(thought)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ Error generating thought {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def _create_few_shot_prompt(self, parent_thought: Thought, agent_type: str, branch_index: int, context: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Create few-shot prompts for each agent type\"\"\"\n",
    "        \n",
    "        if agent_type == \"task_extraction\":\n",
    "            return f\"\"\"\n",
    "You are a task extraction specialist. Break down user stories into 2-7 specific, actionable tasks.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks:\n",
    "1. Design user registration form interface\n",
    "2. Implement email validation and verification system\n",
    "3. Create password strength requirements and validation\n",
    "4. Build user profile creation workflow\n",
    "5. Add account activation process\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks:\n",
    "1. Design analytics dashboard layout and components\n",
    "2. Implement data collection and aggregation system\n",
    "3. Create real-time performance metrics display\n",
    "4. Add filtering and date range selection features\n",
    "\n",
    "Strategy for this iteration: {\"Focus on UI/UX tasks\" if branch_index == 0 else \"Focus on backend/logic tasks\"}\n",
    "\n",
    "Now break down this user story:\n",
    "User Story: {parent_thought.content}\n",
    "\n",
    "Return ONLY a numbered list of 2-7 tasks:\n",
    "\"\"\"\n",
    "\n",
    "        elif agent_type == \"story_point_estimation\":\n",
    "            tasks_context = context.get('tasks', []) if context else []\n",
    "            tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks_context)])\n",
    "            \n",
    "            return f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points using Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account\"\n",
    "Tasks and Estimates:\n",
    "1. Design user registration form interface (3 points)\n",
    "2. Implement email validation and verification system (5 points)\n",
    "3. Create password strength requirements and validation (3 points)\n",
    "4. Build user profile creation workflow (5 points)\n",
    "5. Add account activation process (3 points)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard\"\n",
    "Tasks and Estimates:\n",
    "1. Design analytics dashboard layout and components (5 points)\n",
    "2. Implement data collection and aggregation system (8 points)\n",
    "3. Create real-time performance metrics display (5 points)\n",
    "4. Add filtering and date range selection features (3 points)\n",
    "\n",
    "Current tasks to estimate:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "\"\"\"\n",
    "\n",
    "        elif agent_type == \"required_skills\":\n",
    "            tasks_context = context.get('tasks', []) if context else []\n",
    "            tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks_context)])\n",
    "            \n",
    "            return f\"\"\"\n",
    "You are a technical skills analyst. Identify specific skills required for each task.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Tasks and Skills:\n",
    "Task 1: Design user registration form interface\n",
    "Skills: ui_design, form_design, frontend\n",
    "\n",
    "Task 2: Implement email validation and verification system  \n",
    "Skills: backend, email_systems, validation, security\n",
    "\n",
    "Task 3: Create password strength requirements and validation\n",
    "Skills: frontend, validation, security_patterns\n",
    "\n",
    "Current tasks to analyze:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\n",
    "Task 2: skill1, skill2\n",
    "\"\"\"\n",
    "\n",
    "        elif agent_type == \"dependency_analysis\":\n",
    "            tasks_context = context.get('tasks', []) if context else []\n",
    "            story_points = context.get('story_points', {}) if context else {}\n",
    "            \n",
    "            tasks_with_points = []\n",
    "            for i, task in enumerate(tasks_context):\n",
    "                points = story_points.get(task, 3)\n",
    "                tasks_with_points.append(f\"{i+1}. {task} ({points} points)\")\n",
    "            \n",
    "            tasks_str = \"\\n\".join(tasks_with_points)\n",
    "            \n",
    "            return f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Tasks:\n",
    "1. Design user registration form interface (3 points)\n",
    "2. Implement email validation system (5 points)  \n",
    "3. Create password validation (3 points)\n",
    "4. Build user profile workflow (5 points)\n",
    "5. Add account activation (3 points)\n",
    "\n",
    "Dependencies:\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "Task 4 depends on Task 2 (rework_effort: 3)\n",
    "Task 5 depends on Task 2 (rework_effort: 2)\n",
    "Task 5 depends on Task 4 (rework_effort: 2)\n",
    "\n",
    "Current tasks to analyze:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies.\n",
    "\"\"\"\n",
    "        \n",
    "        return parent_thought.content\n",
    "    \n",
    "    async def _evaluate_thought(self, thought: Thought, agent_type: str) -> float:\n",
    "        \"\"\"Evaluate thought quality based on content structure and relevance\"\"\"\n",
    "        try:\n",
    "            content = thought.content.lower()\n",
    "            score = 0.0\n",
    "            \n",
    "            # Base score for having substantial content\n",
    "            if len(content) > 30:\n",
    "                score += 0.3\n",
    "            \n",
    "            # Agent-specific evaluation criteria\n",
    "            if agent_type == \"task_extraction\":\n",
    "                # Look for numbered lists and action words\n",
    "                if re.search(r'\\d+\\.', content):\n",
    "                    score += 0.3\n",
    "                action_words = ['design', 'implement', 'create', 'build', 'add', 'develop']\n",
    "                if any(word in content for word in action_words):\n",
    "                    score += 0.2\n",
    "                # Count tasks (should be 2-7)\n",
    "                task_count = len(re.findall(r'\\d+\\.', content))\n",
    "                if 2 <= task_count <= 7:\n",
    "                    score += 0.2\n",
    "                    \n",
    "            elif agent_type == \"story_point_estimation\":\n",
    "                # Look for point assignments\n",
    "                if re.search(r'\\d+\\s+points?', content):\n",
    "                    score += 0.4\n",
    "                fibonacci_numbers = ['1', '2', '3', '5', '8', '13']\n",
    "                if any(f\"{num} point\" in content for num in fibonacci_numbers):\n",
    "                    score += 0.2\n",
    "                    \n",
    "            elif agent_type == \"required_skills\":\n",
    "                # Look for skill lists\n",
    "                skills_keywords = ['frontend', 'backend', 'ui', 'database', 'api', 'security']\n",
    "                skill_count = sum(1 for skill in skills_keywords if skill in content)\n",
    "                score += min(0.3, skill_count * 0.1)\n",
    "                \n",
    "            elif agent_type == \"dependency_analysis\":\n",
    "                # Look for dependency statements\n",
    "                if 'depends on' in content:\n",
    "                    score += 0.4\n",
    "                if 'rework_effort' in content:\n",
    "                    score += 0.2\n",
    "            \n",
    "            # Length bonus (not too short, not too long)\n",
    "            if 50 <= len(content) <= 1000:\n",
    "                score += 0.1\n",
    "            \n",
    "            return min(1.0, score)\n",
    "            \n",
    "        except:\n",
    "            return 0.3\n",
    "    \n",
    "    def _select_best_solutions(self, solutions: List[str]) -> List[str]:\n",
    "        \"\"\"Select best solutions with content filtering\"\"\"\n",
    "        if not solutions:\n",
    "            return []\n",
    "        \n",
    "        filtered_solutions = []\n",
    "        for solution in solutions:\n",
    "            if len(solution) > 50 and solution not in filtered_solutions:\n",
    "                filtered_solutions.append(solution)\n",
    "        \n",
    "        return filtered_solutions[:2]\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Enhanced Task Extractor using Tree of Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tot_framework = OptimizedTreeOfThoughts(max_depth=2, branching_factor=2, timeout_seconds=25)\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        print(f\"ðŸ” Extracting tasks from: {user_story[:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Explore the tree of thoughts\n",
    "            thought_solutions = await self.tot_framework.explore_tree(user_story, \"task_extraction\")\n",
    "            \n",
    "            # Extract and consolidate tasks from all solutions\n",
    "            all_tasks = []\n",
    "            for solution in thought_solutions:\n",
    "                tasks = self._extract_tasks_from_solution(solution)\n",
    "                all_tasks.extend(tasks)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            unique_tasks = []\n",
    "            seen = set()\n",
    "            for task in all_tasks:\n",
    "                normalized = task.lower().strip()\n",
    "                if normalized not in seen and len(task.strip()) > 10:\n",
    "                    unique_tasks.append(task.strip())\n",
    "                    seen.add(normalized)\n",
    "            \n",
    "            # Limit to 2-7 tasks as specified\n",
    "            if len(unique_tasks) > 7:\n",
    "                unique_tasks = unique_tasks[:7]\n",
    "            elif len(unique_tasks) < 2:\n",
    "                # Fallback: generate basic tasks\n",
    "                unique_tasks = self._generate_fallback_tasks(user_story)\n",
    "            \n",
    "            print(f\"  âœ… Generated {len(unique_tasks)} unique tasks\")\n",
    "            return unique_tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error in task extraction: {str(e)}\")\n",
    "            return self._generate_fallback_tasks(user_story)\n",
    "    \n",
    "    def _extract_tasks_from_solution(self, solution: str) -> List[str]:\n",
    "        \"\"\"Extract tasks from Tree of Thoughts solution\"\"\"\n",
    "        tasks = []\n",
    "        lines = solution.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Extract numbered tasks\n",
    "            if re.match(r'^\\d+\\.', line):\n",
    "                task = re.sub(r'^\\d+\\.\\s*', '', line)\n",
    "                if len(task) > 10:\n",
    "                    tasks.append(task)\n",
    "            elif line.startswith('-') or line.startswith('*'):\n",
    "                task = re.sub(r'^[\\-\\*]\\s*', '', line)\n",
    "                if len(task) > 10:\n",
    "                    tasks.append(task)\n",
    "        \n",
    "        return tasks\n",
    "    \n",
    "    def _generate_fallback_tasks(self, user_story: str) -> List[str]:\n",
    "        \"\"\"Generate basic fallback tasks if Tree of Thoughts fails\"\"\"\n",
    "        return [\n",
    "            \"Design user interface components\",\n",
    "            \"Implement core functionality\",\n",
    "            \"Add data validation and processing\",\n",
    "            \"Test and validate implementation\"\n",
    "        ]\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Enhanced Story Point Estimator using Tree of Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tot_framework = OptimizedTreeOfThoughts(max_depth=1, branching_factor=2, timeout_seconds=20)\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        print(f\"ðŸ“Š Estimating story points for {len(tasks)} tasks...\")\n",
    "        \n",
    "        try:\n",
    "            context = {'tasks': tasks}\n",
    "            thought_solutions = await self.tot_framework.explore_tree(\n",
    "                f\"Estimate story points for tasks related to: {user_story}\", \n",
    "                \"story_point_estimation\", \n",
    "                context\n",
    "            )\n",
    "            \n",
    "            # Extract points from all solutions\n",
    "            all_points = {}\n",
    "            for solution in thought_solutions:\n",
    "                points = self._extract_points_from_solution(solution, tasks)\n",
    "                all_points.update(points)\n",
    "            \n",
    "            # Fill in missing tasks with default points\n",
    "            for task in tasks:\n",
    "                if task not in all_points:\n",
    "                    all_points[task] = 3  # Default moderate complexity\n",
    "            \n",
    "            total_points = sum(all_points.values())\n",
    "            return {\n",
    "               'total_story_points': total_points,\n",
    "               'task_points': all_points,\n",
    "               'estimated_sum': total_points\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error in story point estimation: {str(e)}\")\n",
    "            return {task: 3 for task in tasks}  # Default fallback\n",
    "    \n",
    "    def _extract_points_from_solution(self, solution: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Extract story points from Tree of Thoughts solution\"\"\"\n",
    "        points = {}\n",
    "        lines = solution.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line and 'point' in line.lower():\n",
    "                try:\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        return points\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Enhanced Required Skills Agent using Tree of Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tot_framework = OptimizedTreeOfThoughts(max_depth=1, branching_factor=2, timeout_seconds=15)\n",
    "        \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task using Tree of Thoughts\"\"\"\n",
    "        try:\n",
    "            context = {'tasks': [task]}\n",
    "            thought_solutions = await self.tot_framework.explore_tree(\n",
    "                f\"Identify skills for task: {task}\",\n",
    "                \"required_skills\",\n",
    "                context\n",
    "            )\n",
    "        \n",
    "            # Extract skills from all solutions\n",
    "            all_skills = []\n",
    "            for solution in thought_solutions:\n",
    "                skills = self._extract_skills_from_solution(solution, [task])\n",
    "                if task in skills:\n",
    "                    all_skills.extend(skills[task])\n",
    "        \n",
    "        # Deduplicate and normalize skills\n",
    "            if all_skills:\n",
    "                unique_skills = list(dict.fromkeys(all_skills))  # Remove duplicates\n",
    "                return [self._normalize_skill(skill) for skill in unique_skills[:4]]\n",
    "            else:\n",
    "                return [\"general_development\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error in skill mapping: {str(e)}\")\n",
    "            return [\"general_development\"]\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        print(f\"ðŸ› ï¸ Identifying skills for {len(tasks)} tasks...\")\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "    \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "    \n",
    "        print(f\"  âœ… Identified skills for {len(skills_map)} tasks\")\n",
    "        return skills_map\n",
    "    def _extract_skills_from_solution(self, solution: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract skills from Tree of Thoughts solution\"\"\"\n",
    "        skills_map = {}\n",
    "        lines = solution.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Parse skills\n",
    "                            skills = [skill.strip() for skill in skills_part.split(',')]\n",
    "                            skills = [skill for skill in skills if skill and len(skill) > 1]\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        return skills_map\n",
    "    \n",
    "    def _normalize_skill(self, skill: str) -> str:\n",
    "        \"\"\"Normalize skill names to standard format\"\"\"\n",
    "        normalized = skill.lower().strip()\n",
    "        \n",
    "        # Remove common prefixes/suffixes\n",
    "        normalized = re.sub(r'^(skill|development|programming|design)[\\s:]*', '', normalized)\n",
    "        normalized = re.sub(r'[\\s:]*(skill|development|programming)$', '', normalized)\n",
    "        \n",
    "        # Convert to snake_case\n",
    "        normalized = re.sub(r'[^a-z0-9]+', '_', normalized)\n",
    "        normalized = normalized.strip('_')\n",
    "        \n",
    "        return normalized if len(normalized) > 1 else \"general_development\"\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Enhanced Dependency Agent using Tree of Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tot_framework = OptimizedTreeOfThoughts(max_depth=2, branching_factor=2, timeout_seconds=20)\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "            \n",
    "        print(f\"ðŸ”— Analyzing dependencies for {len(tasks)} tasks...\")\n",
    "        \n",
    "        try:\n",
    "            context = {'tasks': tasks, 'story_points': story_points}\n",
    "            thought_solutions = await self.tot_framework.explore_tree(\n",
    "                f\"Analyze dependencies for tasks in: {user_story}\",\n",
    "                \"dependency_analysis\",\n",
    "                context\n",
    "            )\n",
    "            \n",
    "            # Extract dependencies from all solutions\n",
    "            all_dependencies = {}\n",
    "            for solution in thought_solutions:\n",
    "                deps = self._extract_dependencies_from_solution(solution, tasks)\n",
    "                for dependent_task, deps_list in deps.items():\n",
    "                    if dependent_task not in all_dependencies:\n",
    "                        all_dependencies[dependent_task] = []\n",
    "                    all_dependencies[dependent_task].extend(deps_list)\n",
    "            \n",
    "            # Deduplicate dependencies\n",
    "            deduplicated = self._deduplicate_dependencies(all_dependencies)\n",
    "            \n",
    "            print(f\"  âœ… Found {len(deduplicated)} dependency relationships\")\n",
    "            return deduplicated\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error in dependency analysis: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _extract_dependencies_from_solution(self, solution: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Extract dependencies from Tree of Thoughts solution\"\"\"\n",
    "        dependencies = {}\n",
    "        lines = solution.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if \"depends on\" in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers and rework_effort\n",
    "                        if (1 <= dependent_num <= len(tasks) and \n",
    "                            1 <= prerequisite_num <= len(tasks) and\n",
    "                            1 <= rework_effort <= 3):\n",
    "                            \n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "    \n",
    "    def _deduplicate_dependencies(self, dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Remove duplicate dependencies\"\"\"\n",
    "        deduplicated = {}\n",
    "        \n",
    "        for dependent_task, deps_list in dependencies.items():\n",
    "            seen_deps = set()\n",
    "            unique_deps = []\n",
    "            \n",
    "            for dep in deps_list:\n",
    "                dep_key = dep['task_id']\n",
    "                if dep_key not in seen_deps:\n",
    "                    unique_deps.append(dep)\n",
    "                    seen_deps.add(dep_key)\n",
    "            \n",
    "            if unique_deps:\n",
    "                deduplicated[dependent_task] = unique_deps\n",
    "        \n",
    "        return deduplicated\n",
    "\n",
    "class FormatValidatorAgent:\n",
    "    \"\"\"Format Validator Agent for final output structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def validate_and_format(self, user_story: str, tasks: List[str], \n",
    "                                 story_points: Dict[str, int], skills: Dict[str, List[str]], \n",
    "                                 dependencies: Dict[str, List[Dict[str, any]]]) -> Dict[str, any]:\n",
    "        \n",
    "        print(f\"âœ… Validating and formatting final output...\")\n",
    "        \n",
    "        # Generate task IDs\n",
    "        task_ids = self._generate_task_ids(user_story, len(tasks))\n",
    "        \n",
    "        # Build final structure\n",
    "        formatted_tasks = []\n",
    "        total_story_points = 0\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = task_ids[i]\n",
    "            task_points = story_points.get(task, 3)\n",
    "            task_skills = skills.get(task, [\"general_development\"])\n",
    "            task_dependencies = dependencies.get(task, [])\n",
    "            \n",
    "            # Convert dependencies to use task IDs\n",
    "            formatted_dependencies = []\n",
    "            for dep in task_dependencies:\n",
    "                dep_task = dep[\"task_id\"]\n",
    "                if dep_task in tasks:\n",
    "                    dep_index = tasks.index(dep_task)\n",
    "                    dep_task_id = task_ids[dep_index]\n",
    "                    formatted_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            formatted_tasks.append({\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": task_points,\n",
    "                \"depends_on\": formatted_dependencies,\n",
    "                \"required_skills\": task_skills\n",
    "            })\n",
    "            \n",
    "            total_story_points += task_points\n",
    "        \n",
    "        result = {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"story_points\": total_story_points,\n",
    "                \"tasks\": formatted_tasks\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate JSON structure\n",
    "        try:\n",
    "            json.dumps(result)\n",
    "            print(\"  âœ… Format validation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Format validation warning: {e}\")\n",
    "            result = self._fix_json_issues(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_task_ids(self, user_story: str, num_tasks: int) -> List[str]:\n",
    "        \"\"\"Generate meaningful task IDs from user story content\"\"\"\n",
    "        # Extract key words from user story\n",
    "        words = re.findall(r'\\b[A-Z][A-Z]+\\b|\\b[a-z]+\\b', user_story)\n",
    "        significant_words = [w for w in words if len(w) > 2 and w.lower() not in \n",
    "                           ['the', 'and', 'that', 'want', 'can', 'will', 'have', 'this', 'with', 'user']]\n",
    "        \n",
    "        if len(significant_words) >= 2:\n",
    "            prefix = ''.join([w[0].upper() for w in significant_words[:3]])\n",
    "        elif len(significant_words) == 1:\n",
    "            prefix = significant_words[0][:3].upper()\n",
    "        else:\n",
    "            prefix = \"TSK\"\n",
    "        \n",
    "        # Ensure prefix is exactly 3 characters\n",
    "        prefix = (prefix + \"XXX\")[:3]\n",
    "        \n",
    "        return [f\"{prefix}_{i+1:03d}\" for i in range(num_tasks)]\n",
    "    \n",
    "    def _fix_json_issues(self, result: Dict[str, any]) -> Dict[str, any]:\n",
    "        \"\"\"Fix JSON serialization issues\"\"\"\n",
    "        try:\n",
    "            json_str = json.dumps(result, default=str)\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return result\n",
    "\n",
    "class TreeOfThoughtsUserStoryPipeline:\n",
    "    \"\"\"Enhanced Multi-Agent Pipeline with Tree of Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_extractor = TaskExtractorAgent()\n",
    "        self.story_point_estimator = StoryPointEstimatorAgent()\n",
    "        self.skills_agent = RequiredSkillsAgent()\n",
    "        self.dependency_agent = DependencyAgent()\n",
    "        self.format_validator = FormatValidatorAgent()\n",
    "    \n",
    "    async def process_story(self, user_story: str) -> Dict[str, any]:\n",
    "        \"\"\"Process single user story through the enhanced pipeline\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Step 1: Extract tasks using Tree of Thoughts\n",
    "            print(\"ðŸ“ Step 1: Extracting tasks with Tree of Thoughts...\")\n",
    "            tasks = await self.task_extractor.decompose(user_story)\n",
    "            \n",
    "            if not tasks:\n",
    "                raise ValueError(\"No tasks extracted from user story\")\n",
    "            \n",
    "            # Step 2: Parallel processing of story points and skills with Tree of Thoughts\n",
    "            print(\"âš¡ Step 2: Parallel estimation (Story Points & Skills) with Tree of Thoughts...\")\n",
    "            story_points_results, skills = await asyncio.gather(\n",
    "                self.story_point_estimator.estimate_story_points(user_story, tasks),\n",
    "                self.skills_agent.identify_skills(user_story, tasks)\n",
    "            )\n",
    "            story_points= story_points_results['task_points']\n",
    "            \n",
    "            # Step 3: Analyze dependencies with Tree of Thoughts\n",
    "            print(\"ðŸ”— Step 3: Analyzing dependencies with Tree of Thoughts...\")\n",
    "            dependencies = await self.dependency_agent.analyze_dependencies(\n",
    "                user_story, tasks, story_points\n",
    "            )\n",
    "            \n",
    "            # Step 4: Format and validate\n",
    "            print(\"ðŸ“‹ Step 4: Formatting and validating output...\")\n",
    "            result = await self.format_validator.validate_and_format(\n",
    "                user_story, tasks, story_points, skills, dependencies\n",
    "            )\n",
    "            \n",
    "            print(\"ðŸŽ‰ Story processing complete!\")\n",
    "            print(\"=\"*80)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing story: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"error\": str(e),\n",
    "                \"output\": None\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_stories(self, user_stories: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"Process multiple user stories through the enhanced pipeline\"\"\"\n",
    "        print(f\"\\nðŸš€ Starting Tree of Thoughts Multi-Agent Pipeline\")\n",
    "        print(f\"ðŸ“Š Processing {len(user_stories)} user stories...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Reset token tracker\n",
    "        global token_tracker\n",
    "        token_tracker = TokenTracker()\n",
    "        \n",
    "        # Process all stories\n",
    "        results = []\n",
    "        for i, story in enumerate(user_stories, 1):\n",
    "            print(f\"\\nðŸ“– Story {i}/{len(user_stories)}\")\n",
    "            result = await self.process_story(story)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Pipeline completed! Processed {len(results)} stories\")\n",
    "        return results\n",
    "\n",
    "def format_output(results: List[Dict[str, any]]) -> str:\n",
    "    \"\"\"Format results for display with Tree of Thoughts information\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    # Header\n",
    "    output.append(\"=\" * 90)\n",
    "    output.append(\"ðŸŒ³ TREE OF THOUGHTS MULTI-AGENT PIPELINE RESULTS\")\n",
    "    output.append(\"=\" * 90)\n",
    "    \n",
    "    # Token usage summary\n",
    "    summary = token_tracker.get_summary()\n",
    "    if summary:\n",
    "        breakdown = summary.get(\"breakdown\", {})\n",
    "        output.append(f\"ðŸ“Š TOTAL TOKENS CONSUMED: {breakdown.get('total_consumed', 0)}\")\n",
    "        \n",
    "        cost_data = summary.get(\"cost_estimate\", {})\n",
    "        if cost_data:\n",
    "            output.append(f\"ðŸ’° ESTIMATED COST: ${cost_data['total_cost']:.6f}\")\n",
    "        \n",
    "        # Token breakdown by agent\n",
    "        output.append(\"\\nðŸ” TOKEN BREAKDOWN BY AGENT:\")\n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis']\n",
    "        for cat in categories:\n",
    "            if cat in breakdown:\n",
    "                data = breakdown[cat]\n",
    "                cat_name = cat.replace('_', ' ').title()\n",
    "                output.append(f\"  {cat_name}: {data['total']} tokens\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Results\n",
    "    output.append(\"=\" * 90)\n",
    "    output.append(\"ðŸ“‹ PROCESSED USER STORIES\")\n",
    "    output.append(\"=\" * 90)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- ðŸ“– Story {i} ---\")\n",
    "        if \"error\" in result:\n",
    "            output.append(f\"âŒ Error: {result['error']}\")\n",
    "        else:\n",
    "            # Pretty print JSON\n",
    "            formatted_json = json.dumps(result, indent=2)\n",
    "            output.append(formatted_json)\n",
    "        output.append(\"\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    successful_results = [r for r in results if \"error\" not in r]\n",
    "    if successful_results:\n",
    "        output.append(\"=\" * 90)\n",
    "        output.append(\"ðŸ“ˆ PIPELINE STATISTICS\")\n",
    "        output.append(\"=\" * 90)\n",
    "        \n",
    "        total_tasks = sum(len(r[\"output\"][\"tasks\"]) for r in successful_results)\n",
    "        total_story_points = sum(r[\"output\"][\"story_points\"] for r in successful_results)\n",
    "        total_dependencies = sum(len([task for task in r[\"output\"][\"tasks\"] if task[\"depends_on\"]]) for r in successful_results)\n",
    "        \n",
    "        output.append(f\"âœ… Successfully processed: {len(successful_results)}/{len(results)} stories\")\n",
    "        output.append(f\"ðŸ“ Total tasks generated: {total_tasks}\")\n",
    "        output.append(f\"ðŸ“Š Total story points: {total_story_points}\")\n",
    "        output.append(f\"ðŸ”— Tasks with dependencies: {total_dependencies}\")\n",
    "        \n",
    "        if len(results) > len(successful_results):\n",
    "            output.append(f\"âŒ Failed stories: {len(results) - len(successful_results)}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    print(\"ðŸŒ³ Tree of Thoughts Enhanced Multi-Agent User Story Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ”® Features:\")\n",
    "    print(\"  â€¢ Tree of Thoughts exploration for each agent\")\n",
    "    print(\"  â€¢ Few-shot learning with comprehensive examples\")\n",
    "    print(\"  â€¢ Parallel processing for optimal performance\")\n",
    "    print(\"  â€¢ Context preservation throughout pipeline\")\n",
    "    print(\"  â€¢ Timeout controls and fallback mechanisms\")\n",
    "    print(\"  â€¢ Token usage tracking and cost estimation\")\n",
    "    print(\"=\"*70)\n",
    "    pipeline = TreeOfThoughtsUserStoryPipeline()\n",
    "    start_time = time.time()\n",
    "    results = await pipeline.process_multiple_stories(stories_list)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nâ±ï¸ Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(format_output(results))\n",
    "    return results\n",
    "\n",
    "results=asyncio.run(run_pipeline(user_stories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc967319-0033-46c2-8ec7-fa989b6e29a2",
   "metadata": {},
   "source": [
    "- **Main Classes:**\n",
    "\n",
    "> **OptimizedTreeOfThoughts:** Core Tree of Thoughts framework with configurable max_depth (default 2), branching_factor (default 2), timeout_seconds (default 20), breadth-first exploration using deque, thought evaluation scoring, and timeout controls. Key methods: explore_tree(problem, agent_type, context) -> List[str] with context preservation.\n",
    "\n",
    "> **TaskExtractorAgent:** Tree of Thoughts task decomposition using few-shot prompts with concrete examples, breadth-first exploration generating multiple solution paths, thought evaluation based on task structure and action words, and consolidation of unique tasks from all solutions. Key method: decompose(user_story) -> List[str] with fallback mechanisms.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Tree of Thoughts story point estimation with context-aware exploration, few-shot prompts including task context, Fibonacci scale validation [1,2,3,5,8,13], and solution consolidation across multiple thought paths. Key methods: estimate_story_points(user_story, tasks) -> Dict with comprehensive point mapping.\n",
    "\n",
    "> **RequiredSkillsAgent:** Tree of Thoughts skill identification with individual task processing, context-aware exploration preserving task information, few-shot prompts with skill examples, and skill normalization with deduplication. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Tree of Thoughts dependency analysis with context preservation including tasks and story points, few-shot prompts with dependency examples, logical relationship identification, and dependency deduplication across solutions. Key method: analyze_dependencies(user_story, tasks, story_points) -> Dict.\n",
    "\n",
    "> **FormatValidatorAgent:** Output structure validation and formatting with comprehensive error handling, intelligent task ID generation from user story keywords, field validation, and JSON serialization fixes. Key method: validate_and_format(...) -> Dict with structured output generation.\n",
    "\n",
    "> **TreeOfThoughtsUserStoryPipeline:** Enhanced multi-agent orchestrator with Tree of Thoughts integration, parallel processing for story points and skills, comprehensive error handling with graceful degradation, and detailed progress reporting with step-by-step logging.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **explore_tree(initial_problem, agent_type, context):** Core Tree of Thoughts exploration with breadth-first search using deque, timeout controls, thought generation using few-shot prompts, evaluation scoring (0.0-1.0), and solution selection with quality filtering.\n",
    "\n",
    "> **_generate_thoughts(parent_thought, agent_type, context):** Child thought generation with context preservation, few-shot prompt creation based on agent type, temperature variation for diversity (0.4-0.8), and comprehensive error handling with token tracking.\n",
    "\n",
    "> **_create_few_shot_prompt(parent_thought, agent_type, branch_index, context):** Agent-specific few-shot prompt generation with concrete examples, context integration, branching strategy variation, and structured output requirements.\n",
    "\n",
    "> **_evaluate_thought(thought, agent_type):** Thought quality evaluation with agent-specific criteria, content structure analysis, keyword detection, and scoring normalization (0.0-1.0) for solution ranking.\n",
    "\n",
    "> **decompose(user_story: str):** Tree of Thoughts task extraction with multiple solution paths, task consolidation and deduplication, unique task preservation (2-7 tasks), and fallback task generation for robustness.\n",
    "\n",
    "> **estimate_story_points(user_story, tasks):** Context-aware story point estimation with Tree of Thoughts exploration, solution consolidation across thought paths, Fibonacci validation, and comprehensive point mapping with fallback defaults.\n",
    "\n",
    "> **analyze_dependencies(user_story, tasks, story_points):** Tree of Thoughts dependency analysis with context preservation, solution consolidation, dependency deduplication, and logical relationship validation with effort scoring.\n",
    "\n",
    "> **process_story(user_story: str):** Main pipeline orchestration with Tree of Thoughts integration, parallel processing for efficiency, comprehensive error handling, and detailed progress logging with step-by-step reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333948cf-e277-46cb-8e68-4ce47aa9c892",
   "metadata": {},
   "source": [
    "#### 4.4.7 Strategy 7: Self-Consistency\n",
    "- **Definition**\n",
    "> **Running the same prompt multiple times and comparing or combining the results to improve reliability.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Multiple execution runs** with identical prompts\n",
    "\n",
    "> **Result comparison and analysis** across iterations\n",
    "\n",
    "> **Majority voting or consensus** for final output\n",
    "\n",
    "> **Reliability enhancement** through repitition \n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Better Reliability**\n",
    "\n",
    "> **Good balance between effort and performance**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **3-5x higher cost and time investment**\n",
    "\n",
    ">**Requires additional logic to merge/compare results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa4026-6107-484a-aad7-c621d749c9f6",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ce36a33-c1b2-4c44-9255-7ed34f7768d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Pipeline Task Decomposition System ===\n",
      "Pipeline: Story Input â†’ Task Extractor â†’ Story Points & Skills â†’ Dependencies â†’ Validator â†’ Output\n",
      "Features: Self-consistency, context preservation, parallel processing\n",
      "\n",
      "Processing 2 user stories through enhanced pipeline...\n",
      "Processing: As a user, I want to create an account so that I c...\n",
      "  Step 1: Extracting tasks...\n",
      "  Extracted 43 tasks\n",
      "  Steps 2-3: Estimating story points and mapping skills...\n",
      "Processing: As an admin, I want to view analytics dashboard so...\n",
      "  Step 1: Extracting tasks...\n",
      "  Extracted 39 tasks\n",
      "  Steps 2-3: Estimating story points and mapping skills...\n",
      "  Step 4: Analyzing dependencies...\n",
      "  Step 5: Formatting and validating...\n",
      "  Completed: 43 tasks, 231 total story points\n",
      "  Step 4: Analyzing dependencies...\n",
      "  Step 5: Formatting and validating...\n",
      "  Completed: 39 tasks, 228 total story points\n",
      "\n",
      "================================================================================\n",
      "PIPELINE PROCESSING COMPLETE\n",
      "================================================================================\n",
      "================================================================================\n",
      "ENHANCED PIPELINE PROCESSING RESULTS\n",
      "================================================================================\n",
      "\n",
      "--- USER STORY 1 ---\n",
      "Input: As a user, I want to create an account so that I can access personalized features\n",
      "Total Story Points: 231\n",
      "Tasks: 43\n",
      "\n",
      "  Task T_001: Validate user input on the client-side and server-side to prevent errors\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, frontend_development, backend_development, api_integration, javascript, url_handling\n",
      "\n",
      "  Task T_002: Implement password hashing and salting for secure password storage\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development\n",
      "\n",
      "  Task T_003: The user wants to create an account, which implies registration functionality\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, backend_development, api_integration, database_management, error_handling, javascript\n",
      "    Dependencies: T_001 (rework: 2), T_002 (rework: 2), T_005 (rework: 2), T_010 (rework: 2)\n",
      "\n",
      "  Task T_004: The user wants to create an account, which implies a registration process\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, javascript, api_integration, database_management, error_handling, url_handling, general_development\n",
      "    Dependencies: T_001 (rework: 2), T_002 (rework: 2), T_003 (rework: 1), T_005 (rework: 2), T_010 (rework: 2)\n",
      "\n",
      "  Task T_005: The registration process should collect necessary information from the user\n",
      "    Story Points: 5\n",
      "    Skills: database_management, template_design, frontend_development, api_integration\n",
      "    Dependencies: T_004 (rework: 2)\n",
      "\n",
      "  Task T_006: The user's information should be stored securely in a database\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, general_development\n",
      "    Dependencies: T_005 (rework: 3), T_002 (rework: 2), T_011 (rework: 3), T_023 (rework: 3)\n",
      "\n",
      "  Task T_007: The user should be able to log in to their account after registration\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, backend_development, database_management, api_integration, frontend_development, javascript, url_handling\n",
      "    Dependencies: T_006 (rework: 2), T_012 (rework: 3)\n",
      "\n",
      "  Task T_008: The user should have access to personalized features after logging in\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, api_integration, error_handling, javascript, frontend_development, url_handling\n",
      "    Dependencies: T_007 (rework: 2), T_013 (rework: 3)\n",
      "\n",
      "  Task T_009: This involves both frontend and backend implementation\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, frontend_development, backend_development, api_integration, url_handling, database_management\n",
      "    Dependencies: T_008 (rework: 3)\n",
      "\n",
      "  Task T_010: Design and implement a registration form with necessary fields (e.g., username, email, password)\n",
      "    Story Points: 3\n",
      "    Skills: frontend_development, template_design, database_management, backend_development, javascript\n",
      "    Dependencies: T_005 (rework: 2)\n",
      "\n",
      "  Task T_011: Create a database schema to store user information securely\n",
      "    Story Points: 5\n",
      "    Skills: database_management, backend_development\n",
      "    Dependencies: T_006 (rework: 3)\n",
      "\n",
      "  Task T_012: Develop a login functionality that authenticates users and logs them in\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, javascript, api_integration, database_management, error_handling, frontend_development\n",
      "    Dependencies: T_007 (rework: 2), T_011 (rework: 3), T_006 (rework: 3)\n",
      "\n",
      "  Task T_013: Integrate login functionality with personalized features (e.g., profile page, customized dashboard)\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, url_handling, error_handling, frontend_development, javascript\n",
      "    Dependencies: T_012 (rework: 2), T_008 (rework: 3)\n",
      "\n",
      "  Task T_014: Implement session management to maintain user login state\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, javascript\n",
      "    Dependencies: T_012 (rework: 2)\n",
      "\n",
      "  Task T_015: Add error handling for registration and login failures (e.g., invalid credentials, duplicate usernames)\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, error_handling, api_integration, url_handling, database_management\n",
      "    Dependencies: T_003 (rework: 2), T_012 (rework: 2), T_007 (rework: 2)\n",
      "\n",
      "  Task T_016: The user wants to access personalized features, which implies authentication and authorization\n",
      "    Story Points: 8\n",
      "    Skills: error_handling, backend_development, database_management, api_integration\n",
      "    Dependencies: T_008 (rework: 2)\n",
      "\n",
      "  Task T_017: Registration involves collecting user information, such as username, password, and email\n",
      "    Story Points: 2\n",
      "    Skills: backend_development, database_management, frontend_development, error_handling\n",
      "    Dependencies: T_005 (rework: 2)\n",
      "\n",
      "  Task T_018: The system needs to validate and store this information securely\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, database_management, error_handling, api_integration\n",
      "    Dependencies: T_005 (rework: 2), T_002 (rework: 2), T_017 (rework: 2), T_006 (rework: 3)\n",
      "\n",
      "  Task T_019: After registration, the user should be able to log in and access personalized features\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, api_integration, error_handling, url_handling\n",
      "    Dependencies: T_007 (rework: 3), T_008 (rework: 3), T_018 (rework: 2)\n",
      "\n",
      "  Task T_020: This involves implementing authentication and authorization mechanisms\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, error_handling, api_integration, database_management\n",
      "    Dependencies: T_019 (rework: 3), T_008 (rework: 3)\n",
      "\n",
      "  Task T_021: The system should also handle errors and exceptions, such as duplicate usernames or invalid email addresses\n",
      "    Story Points: 5\n",
      "    Skills: error_handling\n",
      "    Dependencies: T_015 (rework: 2), T_020 (rework: 2), T_003 (rework: 2), T_007 (rework: 2)\n",
      "\n",
      "  Task T_022: Design and implement a registration form with fields for username, password, and email\n",
      "    Story Points: 5\n",
      "    Skills: template_design, frontend_development, javascript, database_management\n",
      "    Dependencies: T_005 (rework: 2)\n",
      "\n",
      "  Task T_023: Implement password hashing and salting to store passwords securely\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, backend_development, database_management, error_handling\n",
      "    Dependencies: T_002 (rework: 1)\n",
      "\n",
      "  Task T_024: Create a database schema to store user information\n",
      "    Story Points: 5\n",
      "    Skills: database_management\n",
      "    Dependencies: T_006 (rework: 3)\n",
      "\n",
      "  Task T_025: Develop a registration API endpoint to handle form submissions\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, url_handling, database_management, error_handling\n",
      "    Dependencies: T_003 (rework: 2), T_024 (rework: 2), T_010 (rework: 2), T_011 (rework: 3)\n",
      "\n",
      "  Task T_026: Implement authentication and authorization logic to grant access to personalized features\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, url_handling, general_development\n",
      "    Dependencies: T_020 (rework: 3), T_025 (rework: 3)\n",
      "\n",
      "  Task T_027: Develop a login API endpoint to handle user logins\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, backend_development, database_management, api_integration, url_handling\n",
      "    Dependencies: T_012 (rework: 3), T_026 (rework: 2)\n",
      "\n",
      "  Task T_028: Integrate authentication and authorization with the personalized features\n",
      "    Story Points: 8\n",
      "    Skills: error_handling, backend_development, api_integration, database_management, url_handling\n",
      "    Dependencies: T_026 (rework: 3), T_008 (rework: 2), T_027 (rework: 2)\n",
      "\n",
      "  Task T_029: Handle errors and exceptions, such as duplicate usernames or invalid email addresses\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, javascript, backend_development, frontend_development\n",
      "    Dependencies: T_015 (rework: 2), T_028 (rework: 2), T_003 (rework: 2), T_007 (rework: 2)\n",
      "\n",
      "  Task T_030: Test registration and login functionality for security and correctness\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, url_handling, javascript\n",
      "    Dependencies: T_025 (rework: 2), T_027 (rework: 2), T_029 (rework: 3), T_003 (rework: 2), T_007 (rework: 2)\n",
      "\n",
      "  Task T_031: Registration typically involves providing some personal information\n",
      "    Story Points: 2\n",
      "    Skills: template_design, database_management, frontend_development, backend_development, api_integration, error_handling\n",
      "    Dependencies: T_005 (rework: 2)\n",
      "\n",
      "  Task T_032: The user wants to access personalized features, which means we need to store and associate this information with their account\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, database_management, api_integration\n",
      "    Dependencies: T_006 (rework: 3), T_031 (rework: 2)\n",
      "\n",
      "  Task T_033: We need to ensure the registration process is secure and follows best practices\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, error_handling, api_integration, general_development, guideline_development\n",
      "    Dependencies: T_002 (rework: 3), T_015 (rework: 2), T_032 (rework: 2), T_006 (rework: 3)\n",
      "\n",
      "  Task T_034: We need to handle validation, error cases, and edge scenarios (e.g., duplicate accounts, password strength)\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, javascript, frontend_development\n",
      "    Dependencies: T_015 (rework: 2), T_033 (rework: 3), T_003 (rework: 2), T_007 (rework: 2)\n",
      "\n",
      "  Task T_035: Design and implement a registration form with required fields (e.g., username, email, password)\n",
      "    Story Points: 3\n",
      "    Skills: frontend_development, template_design, javascript, api_integration, database_management, error_handling\n",
      "    Dependencies: T_005 (rework: 2)\n",
      "\n",
      "  Task T_036: Write backend API to handle registration requests and store user data securely\n",
      "    Story Points: 8\n",
      "    Skills: error_handling, backend_development, database_management, api_integration\n",
      "    Dependencies: T_003 (rework: 2), T_035 (rework: 2), T_010 (rework: 2), T_011 (rework: 3)\n",
      "\n",
      "  Task T_037: Create a user database schema to store registration information\n",
      "    Story Points: 5\n",
      "    Skills: database_management\n",
      "    Dependencies: T_006 (rework: 3)\n",
      "\n",
      "  Task T_038: Develop validation logic for registration form inputs (e.g., email format, password strength)\n",
      "    Story Points: 3\n",
      "    Skills: javascript, error_handling, frontend_development, api_integration\n",
      "    Dependencies: T_015 (rework: 2), T_034 (rework: 3), T_035 (rework: 2)\n",
      "\n",
      "  Task T_039: Handle duplicate account registration attempts and display error messages\n",
      "    Story Points: 3\n",
      "    Skills: error_handling, database_management, backend_development, javascript, frontend_development\n",
      "    Dependencies: T_015 (rework: 2), T_038 (rework: 2), T_003 (rework: 2)\n",
      "\n",
      "  Task T_040: Send a verification email to the user's registered email address\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, backend_development, api_integration, url_handling, database_management\n",
      "    Dependencies: T_005 (rework: 2), T_039 (rework: 2), T_003 (rework: 2)\n",
      "\n",
      "  Task T_041: Implement account activation logic upon email verification\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, url_handling, database_management, error_handling\n",
      "    Dependencies: T_040 (rework: 2)\n",
      "\n",
      "  Task T_042: Associate registered user data with their account for personalized features\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, api_integration, error_handling\n",
      "    Dependencies: T_006 (rework: 3), T_041 (rework: 2)\n",
      "\n",
      "  Task T_043: Update the UI to reflect the user's logged-in state and provide access to personalized features\n",
      "    Story Points: 5\n",
      "    Skills: javascript, api_integration, frontend_development, url_handling, error_handling, general_development\n",
      "    Dependencies: T_007 (rework: 3), T_042 (rework: 2)\n",
      "\n",
      "--- USER STORY 2 ---\n",
      "Input: As an admin, I want to view analytics dashboard so that I can monitor system performance\n",
      "Total Story Points: 228\n",
      "Tasks: 39\n",
      "\n",
      "  Task T_001: The admin wants to view an analytics dashboard\n",
      "    Story Points: 5\n",
      "    Skills: database_management, api_integration, frontend_development, error_handling, javascript, backend_development, template_design\n",
      "\n",
      "  Task T_002: The dashboard should display system performance metrics\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, api_integration, database_management, error_handling, backend_development, url_handling\n",
      "    Dependencies: T_004 (rework: 3), T_001 (rework: 2)\n",
      "\n",
      "  Task T_003: Design analytics dashboard layout and wireframes\n",
      "    Story Points: 5\n",
      "    Skills: template_design\n",
      "\n",
      "  Task T_004: This implies that we need to collect and store performance data\n",
      "    Story Points: 5\n",
      "    Skills: database_management, api_integration, backend_development, error_handling\n",
      "    Dependencies: T_002 (rework: 3)\n",
      "\n",
      "  Task T_005: We need to design a user-friendly dashboard to display the data\n",
      "    Story Points: 5\n",
      "    Skills: template_design, frontend_development, url_handling\n",
      "    Dependencies: T_003 (rework: 2)\n",
      "\n",
      "  Task T_006: The dashboard should be accessible to admins only\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, api_integration, general_development, error_handling\n",
      "    Dependencies: T_001 (rework: 3)\n",
      "\n",
      "  Task T_007: We need to consider data visualization, filtering, and drill-down capabilities\n",
      "    Story Points: 8\n",
      "    Skills: api_integration, url_handling, frontend_development, javascript, general_development, database_management, error_handling\n",
      "    Dependencies: T_004 (rework: 4), T_005 (rework: 3)\n",
      "\n",
      "  Task T_008: This involves both backend data processing and frontend visualization tasks\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, backend_development, api_integration, database_management, error_handling, url_handling\n",
      "    Dependencies: T_004 (rework: 5)\n",
      "\n",
      "  Task T_009: Define key performance indicators (KPIs) for system performance\n",
      "    Story Points: 5\n",
      "    Skills: guideline_development, database_management, frontend_development, organizational_knowledge, backend_development, stakeholder_mapping, general_development\n",
      "    Dependencies: T_002 (rework: 2)\n",
      "\n",
      "  Task T_010: Design database schema to store performance metrics data\n",
      "    Story Points: 5\n",
      "    Skills: database_management, backend_development, general_development\n",
      "    Dependencies: T_009 (rework: 2)\n",
      "\n",
      "  Task T_011: Implement data collection mechanism for performance metrics\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, javascript, url_handling\n",
      "    Dependencies: T_010 (rework: 3)\n",
      "\n",
      "  Task T_012: Create data processing pipeline to aggregate and transform data\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, general_development, frontend_development, error_handling\n",
      "    Dependencies: T_011 (rework: 3)\n",
      "\n",
      "  Task T_013: Implement data visualization components (e.g., charts, graphs)\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, javascript, api_integration, error_handling, database_management\n",
      "    Dependencies: T_007 (rework: 4), T_012 (rework: 3)\n",
      "\n",
      "  Task T_014: Develop filtering and drill-down functionality for dashboard\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, frontend_development, api_integration, database_management, javascript, url_handling, general_development\n",
      "    Dependencies: T_007 (rework: 4), T_013 (rework: 3)\n",
      "\n",
      "  Task T_015: Implement access control to restrict dashboard access to admins only\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, backend_development, database_management, api_integration\n",
      "    Dependencies: T_006 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "  Task T_016: Write unit tests and integration tests for data processing pipeline\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, backend_development, api_integration, database_management, frontend_development\n",
      "    Dependencies: T_012 (rework: 2)\n",
      "\n",
      "  Task T_017: Conduct UI testing for dashboard usability and functionality\n",
      "    Story Points: 5\n",
      "    Skills: general_development, frontend_development, observation_tools, observation_protocols\n",
      "    Dependencies: T_005 (rework: 3), T_001 (rework: 2)\n",
      "\n",
      "  Task T_018: The metrics should be relevant and meaningful for system performance monitoring\n",
      "    Story Points: 5\n",
      "    Skills: guideline_development, organizational_knowledge, database_management\n",
      "    Dependencies: T_009 (rework: 2)\n",
      "\n",
      "  Task T_019: The dashboard should be easily accessible and navigable\n",
      "    Story Points: 5\n",
      "    Skills: url_handling, template_design, frontend_development\n",
      "\n",
      "  Task T_020: This involves data collection, processing, and visualization\n",
      "    Story Points: 8\n",
      "    Skills: frontend_development, database_management, api_integration, backend_development, general_development, javascript, observation_tools, material_preparation\n",
      "    Dependencies: T_004 (rework: 4)\n",
      "\n",
      "  Task T_021: Define key performance indicators (KPIs) for system performance monitoring\n",
      "    Story Points: 5\n",
      "    Skills: database_management, frontend_development, general_development, backend_development, stakeholder_mapping, guideline_development, organizational_knowledge\n",
      "    Dependencies: T_009 (rework: 2), T_002 (rework: 2)\n",
      "\n",
      "  Task T_022: Design the analytics dashboard layout and user interface\n",
      "    Story Points: 5\n",
      "    Skills: template_design, frontend_development, backend_development, database_management\n",
      "    Dependencies: T_003 (rework: 2)\n",
      "\n",
      "  Task T_023: Implement data collection mechanisms for system performance metrics\n",
      "    Story Points: 5\n",
      "    Skills: error_handling, database_management, api_integration, backend_development\n",
      "    Dependencies: T_010 (rework: 3), T_011 (rework: 3)\n",
      "\n",
      "  Task T_024: Develop data processing and aggregation logic for dashboard metrics\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, database_management, api_integration, general_development, error_handling\n",
      "    Dependencies: T_012 (rework: 3), T_011 (rework: 3)\n",
      "\n",
      "  Task T_025: Create data visualization components for dashboard charts and graphs\n",
      "    Story Points: 5\n",
      "    Skills: template_design, frontend_development, javascript, api_integration\n",
      "    Dependencies: T_007 (rework: 4), T_013 (rework: 3), T_012 (rework: 3)\n",
      "\n",
      "  Task T_026: Integrate dashboard components with backend data processing\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, backend_development, api_integration, error_handling, database_management, url_handling\n",
      "    Dependencies: T_008 (rework: 5), T_024 (rework: 5), T_012 (rework: 3)\n",
      "\n",
      "  Task T_027: Implement authentication and authorization for admin access to dashboard\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, url_handling\n",
      "    Dependencies: T_006 (rework: 2), T_015 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "  Task T_028: Test and validate dashboard data accuracy and performance\n",
      "    Story Points: 8\n",
      "    Skills: error_handling, database_management, backend_development, api_integration, javascript, frontend_development\n",
      "    Dependencies: T_016 (rework: 4), T_024 (rework: 5), T_001 (rework: 2)\n",
      "\n",
      "  Task T_029: The admin needs to be authenticated and authorized to access the dashboard\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, general_development, error_handling, database_management\n",
      "    Dependencies: T_027 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "  Task T_030: The dashboard should be visually appealing and easy to understand\n",
      "    Story Points: 5\n",
      "    Skills: template_design\n",
      "    Dependencies: T_005 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "  Task T_031: The metrics should be up-to-date and reflect real-time system performance\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, database_management, api_integration, javascript, error_handling\n",
      "    Dependencies: T_004 (rework: 4), T_020 (rework: 5)\n",
      "\n",
      "  Task T_032: This involves backend data processing, frontend visualization, and access control\n",
      "    Story Points: 8\n",
      "    Skills: error_handling, frontend_development, backend_development, api_integration, database_management, url_handling, general_development\n",
      "    Dependencies: T_008 (rework: 8), T_004 (rework: 3)\n",
      "\n",
      "  Task T_033: Implement authentication and authorization for admin access\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, error_handling, api_integration, database_management, url_handling\n",
      "    Dependencies: T_006 (rework: 2), T_015 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "  Task T_034: Develop a data processing pipeline to collect and aggregate system performance metrics\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, general_development, url_handling\n",
      "    Dependencies: T_012 (rework: 3), T_011 (rework: 3)\n",
      "\n",
      "  Task T_035: Create backend API to serve dashboard data\n",
      "    Story Points: 5\n",
      "    Skills: backend_development, api_integration, url_handling, database_management, error_handling\n",
      "    Dependencies: T_034 (rework: 3), T_012 (rework: 3)\n",
      "\n",
      "  Task T_036: Implement frontend visualization components (charts, graphs, etc.) to display metrics\n",
      "    Story Points: 5\n",
      "    Skills: template_design, javascript, frontend_development, api_integration\n",
      "    Dependencies: T_007 (rework: 4), T_025 (rework: 3), T_035 (rework: 3)\n",
      "\n",
      "  Task T_037: Integrate API data with frontend visualization components\n",
      "    Story Points: 5\n",
      "    Skills: frontend_development, api_integration, error_handling, url_handling, javascript, database_management\n",
      "    Dependencies: T_036 (rework: 3), T_035 (rework: 5)\n",
      "\n",
      "  Task T_038: Add real-time data updating mechanism to reflect system performance changes\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, api_integration, database_management, error_handling, url_handling\n",
      "    Dependencies: T_031 (rework: 5), T_004 (rework: 3)\n",
      "\n",
      "  Task T_039: Conduct usability testing and iteration to ensure an intuitive dashboard experience\n",
      "    Story Points: 8\n",
      "    Skills: template_design, observation_protocols, material_preparation, frontend_development, observer_preparation, general_development, guideline_development, training, stakeholder_mapping, protocol_briefing, interview_planning\n",
      "    Dependencies: T_017 (rework: 3), T_030 (rework: 2), T_001 (rework: 2)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total User Stories Processed: 2\n",
      "Total Tasks Generated: 82\n",
      "Total Story Points: 459\n",
      "\n",
      "================================================================================\n",
      "JSON OUTPUT\n",
      "================================================================================\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 231,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Validate user input on the client-side and server-side to prevent errors\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"javascript\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement password hashing and salting for secure password storage\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user wants to create an account, which implies registration functionality\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user wants to create an account, which implies a registration process\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 1\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The registration process should collect necessary information from the user\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"template_design\",\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user's information should be stored securely in a database\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_023\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user should be able to log in to their account after registration\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user should have access to personalized features after logging in\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_013\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\",\n",
      "          \"javascript\",\n",
      "          \"frontend_development\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This involves both frontend and backend implementation\",\n",
      "        \"id\": \"T_009\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design and implement a registration form with necessary fields (e.g., username, email, password)\",\n",
      "        \"id\": \"T_010\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"template_design\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a database schema to store user information securely\",\n",
      "        \"id\": \"T_011\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop a login functionality that authenticates users and logs them in\",\n",
      "        \"id\": \"T_012\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"javascript\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate login functionality with personalized features (e.g., profile page, customized dashboard)\",\n",
      "        \"id\": \"T_013\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"url_handling\",\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement session management to maintain user login state\",\n",
      "        \"id\": \"T_014\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add error handling for registration and login failures (e.g., invalid credentials, duplicate usernames)\",\n",
      "        \"id\": \"T_015\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"error_handling\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user wants to access personalized features, which implies authentication and authorization\",\n",
      "        \"id\": \"T_016\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Registration involves collecting user information, such as username, password, and email\",\n",
      "        \"id\": \"T_017\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"frontend_development\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The system needs to validate and store this information securely\",\n",
      "        \"id\": \"T_018\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_017\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"After registration, the user should be able to log in and access personalized features\",\n",
      "        \"id\": \"T_019\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_018\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This involves implementing authentication and authorization mechanisms\",\n",
      "        \"id\": \"T_020\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_019\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"error_handling\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The system should also handle errors and exceptions, such as duplicate usernames or invalid email addresses\",\n",
      "        \"id\": \"T_021\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_020\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design and implement a registration form with fields for username, password, and email\",\n",
      "        \"id\": \"T_022\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement password hashing and salting to store passwords securely\",\n",
      "        \"id\": \"T_023\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a database schema to store user information\",\n",
      "        \"id\": \"T_024\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop a registration API endpoint to handle form submissions\",\n",
      "        \"id\": \"T_025\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_024\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication and authorization logic to grant access to personalized features\",\n",
      "        \"id\": \"T_026\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_020\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_025\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop a login API endpoint to handle user logins\",\n",
      "        \"id\": \"T_027\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_026\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate authentication and authorization with the personalized features\",\n",
      "        \"id\": \"T_028\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_026\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_027\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Handle errors and exceptions, such as duplicate usernames or invalid email addresses\",\n",
      "        \"id\": \"T_029\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_028\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"javascript\",\n",
      "          \"backend_development\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Test registration and login functionality for security and correctness\",\n",
      "        \"id\": \"T_030\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_025\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_027\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_029\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\",\n",
      "          \"javascript\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Registration typically involves providing some personal information\",\n",
      "        \"id\": \"T_031\",\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"database_management\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The user wants to access personalized features, which means we need to store and associate this information with their account\",\n",
      "        \"id\": \"T_032\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_031\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"We need to ensure the registration process is secure and follows best practices\",\n",
      "        \"id\": \"T_033\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_032\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"api_integration\",\n",
      "          \"general_development\",\n",
      "          \"guideline_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"We need to handle validation, error cases, and edge scenarios (e.g., duplicate accounts, password strength)\",\n",
      "        \"id\": \"T_034\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_033\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"javascript\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design and implement a registration form with required fields (e.g., username, email, password)\",\n",
      "        \"id\": \"T_035\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"template_design\",\n",
      "          \"javascript\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Write backend API to handle registration requests and store user data securely\",\n",
      "        \"id\": \"T_036\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_035\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create a user database schema to store registration information\",\n",
      "        \"id\": \"T_037\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop validation logic for registration form inputs (e.g., email format, password strength)\",\n",
      "        \"id\": \"T_038\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_034\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_035\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Handle duplicate account registration attempts and display error messages\",\n",
      "        \"id\": \"T_039\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_038\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"javascript\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Send a verification email to the user's registered email address\",\n",
      "        \"id\": \"T_040\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_039\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement account activation logic upon email verification\",\n",
      "        \"id\": \"T_041\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_040\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Associate registered user data with their account for personalized features\",\n",
      "        \"id\": \"T_042\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_041\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Update the UI to reflect the user's logged-in state and provide access to personalized features\",\n",
      "        \"id\": \"T_043\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_042\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"javascript\",\n",
      "          \"api_integration\",\n",
      "          \"frontend_development\",\n",
      "          \"url_handling\",\n",
      "          \"error_handling\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----------------------------------------\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 228,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"The admin wants to view an analytics dashboard\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"frontend_development\",\n",
      "          \"error_handling\",\n",
      "          \"javascript\",\n",
      "          \"backend_development\",\n",
      "          \"template_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The dashboard should display system performance metrics\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design analytics dashboard layout and wireframes\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This implies that we need to collect and store performance data\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"backend_development\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"We need to design a user-friendly dashboard to display the data\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"frontend_development\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The dashboard should be accessible to admins only\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"general_development\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"We need to consider data visualization, filtering, and drill-down capabilities\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"general_development\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This involves both backend data processing and frontend visualization tasks\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Define key performance indicators (KPIs) for system performance\",\n",
      "        \"id\": \"T_009\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"guideline_development\",\n",
      "          \"database_management\",\n",
      "          \"frontend_development\",\n",
      "          \"organizational_knowledge\",\n",
      "          \"backend_development\",\n",
      "          \"stakeholder_mapping\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design database schema to store performance metrics data\",\n",
      "        \"id\": \"T_010\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_009\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data collection mechanism for performance metrics\",\n",
      "        \"id\": \"T_011\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"javascript\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create data processing pipeline to aggregate and transform data\",\n",
      "        \"id\": \"T_012\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"general_development\",\n",
      "          \"frontend_development\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data visualization components (e.g., charts, graphs)\",\n",
      "        \"id\": \"T_013\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop filtering and drill-down functionality for dashboard\",\n",
      "        \"id\": \"T_014\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_013\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"javascript\",\n",
      "          \"url_handling\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement access control to restrict dashboard access to admins only\",\n",
      "        \"id\": \"T_015\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Write unit tests and integration tests for data processing pipeline\",\n",
      "        \"id\": \"T_016\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Conduct UI testing for dashboard usability and functionality\",\n",
      "        \"id\": \"T_017\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\",\n",
      "          \"frontend_development\",\n",
      "          \"observation_tools\",\n",
      "          \"observation_protocols\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The metrics should be relevant and meaningful for system performance monitoring\",\n",
      "        \"id\": \"T_018\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_009\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"guideline_development\",\n",
      "          \"organizational_knowledge\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The dashboard should be easily accessible and navigable\",\n",
      "        \"id\": \"T_019\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"url_handling\",\n",
      "          \"template_design\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This involves data collection, processing, and visualization\",\n",
      "        \"id\": \"T_020\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 4\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"backend_development\",\n",
      "          \"general_development\",\n",
      "          \"javascript\",\n",
      "          \"observation_tools\",\n",
      "          \"material_preparation\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Define key performance indicators (KPIs) for system performance monitoring\",\n",
      "        \"id\": \"T_021\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_009\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"database_management\",\n",
      "          \"frontend_development\",\n",
      "          \"general_development\",\n",
      "          \"backend_development\",\n",
      "          \"stakeholder_mapping\",\n",
      "          \"guideline_development\",\n",
      "          \"organizational_knowledge\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design the analytics dashboard layout and user interface\",\n",
      "        \"id\": \"T_022\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement data collection mechanisms for system performance metrics\",\n",
      "        \"id\": \"T_023\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_010\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"backend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop data processing and aggregation logic for dashboard metrics\",\n",
      "        \"id\": \"T_024\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"general_development\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create data visualization components for dashboard charts and graphs\",\n",
      "        \"id\": \"T_025\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_013\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate dashboard components with backend data processing\",\n",
      "        \"id\": \"T_026\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 5\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_024\",\n",
      "            \"rework_effort\": 5\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\",\n",
      "          \"database_management\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication and authorization for admin access to dashboard\",\n",
      "        \"id\": \"T_027\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Test and validate dashboard data accuracy and performance\",\n",
      "        \"id\": \"T_028\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_016\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_024\",\n",
      "            \"rework_effort\": 5\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"database_management\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"javascript\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The admin needs to be authenticated and authorized to access the dashboard\",\n",
      "        \"id\": \"T_029\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_027\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"general_development\",\n",
      "          \"error_handling\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The dashboard should be visually appealing and easy to understand\",\n",
      "        \"id\": \"T_030\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"The metrics should be up-to-date and reflect real-time system performance\",\n",
      "        \"id\": \"T_031\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_020\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"api_integration\",\n",
      "          \"javascript\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"This involves backend data processing, frontend visualization, and access control\",\n",
      "        \"id\": \"T_032\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_008\",\n",
      "            \"rework_effort\": 8\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"error_handling\",\n",
      "          \"frontend_development\",\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"url_handling\",\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement authentication and authorization for admin access\",\n",
      "        \"id\": \"T_033\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_006\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_015\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"error_handling\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Develop a data processing pipeline to collect and aggregate system performance metrics\",\n",
      "        \"id\": \"T_034\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_011\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"general_development\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create backend API to serve dashboard data\",\n",
      "        \"id\": \"T_035\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_034\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_012\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"url_handling\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement frontend visualization components (charts, graphs, etc.) to display metrics\",\n",
      "        \"id\": \"T_036\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_007\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_025\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_035\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"javascript\",\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Integrate API data with frontend visualization components\",\n",
      "        \"id\": \"T_037\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_036\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_035\",\n",
      "            \"rework_effort\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"api_integration\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\",\n",
      "          \"javascript\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add real-time data updating mechanism to reflect system performance changes\",\n",
      "        \"id\": \"T_038\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_031\",\n",
      "            \"rework_effort\": 5\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"api_integration\",\n",
      "          \"database_management\",\n",
      "          \"error_handling\",\n",
      "          \"url_handling\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Conduct usability testing and iteration to ensure an intuitive dashboard experience\",\n",
      "        \"id\": \"T_039\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_017\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_030\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"template_design\",\n",
      "          \"observation_protocols\",\n",
      "          \"material_preparation\",\n",
      "          \"frontend_development\",\n",
      "          \"observer_preparation\",\n",
      "          \"general_development\",\n",
      "          \"guideline_development\",\n",
      "          \"training\",\n",
      "          \"stakeholder_mapping\",\n",
      "          \"protocol_briefing\",\n",
      "          \"interview_planning\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Enhanced Task Extractor with better context preservation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_examples = \"\"\"\n",
    "User Story: As a user researcher, I want to make sure the correct NSF people are invited to user interviews, so that they can observe the interviews and make recommendations accordingly.\n",
    "\n",
    "Reasoning: Let me break this down step by step:\n",
    "1. I need to identify who the \"correct NSF people\" are for different types of interviews\n",
    "2. I need to manage the invitation process for these stakeholders\n",
    "3. I need to set up observation protocols so they can effectively observe\n",
    "4. I need to ensure they can make meaningful recommendations based on what they observe\n",
    "5. This involves stakeholder identification, scheduling, observation setup, and feedback mechanisms\n",
    "\n",
    "Tasks:\n",
    "1. Identify relevant NSF stakeholders for each interview type\n",
    "2. Create interview observation guidelines and protocols\n",
    "3. Schedule stakeholder availability coordination\n",
    "4. Prepare observation materials and note-taking templates\n",
    "5. Brief observers on interview protocols and expectations\n",
    "\n",
    "User Story: As a user, I want to click on the address so that it takes me to a new tab with Google Maps.\n",
    "\n",
    "Reasoning: Let me analyze this step by step:\n",
    "1. The user wants addresses to be interactive/clickable\n",
    "2. Clicking should open Google Maps with that address\n",
    "3. It should open in a new tab for better UX\n",
    "4. I need to handle URL encoding for different address formats\n",
    "5. This is primarily a frontend interaction task\n",
    "\n",
    "Tasks:\n",
    "1. Make address text clickable with proper styling\n",
    "2. Implement click handler to capture address data\n",
    "3. Format address for Google Maps URL with proper encoding\n",
    "4. Open Google Maps in new tab/window\n",
    "5. Add error handling for invalid addresses\n",
    "\"\"\"\n",
    "    \n",
    "    async def decompose(self, user_story: str, num_samples: int = 3) -> List[str]:\n",
    "        \"\"\"Extract tasks using self-consistency across multiple samples\"\"\"\n",
    "        all_task_samples = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            temperature = 0.2 + (i * 0.2)  # 0.2, 0.4, 0.6\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "You are an expert at breaking down user stories into specific, actionable tasks.\n",
    "Each task should be atomic, testable, and focused on a single responsibility.\n",
    "\n",
    "Use Chain of Thought reasoning: First analyze the user story step by step, then provide specific tasks.\n",
    "\n",
    "IMPORTANT: \n",
    "- Always include a \"Tasks:\" section with numbered tasks\n",
    "- Each task should be clear, specific, and actionable\n",
    "- Focus on implementation details, not just high-level concepts\n",
    "- Consider both technical and process-related tasks\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Reasoning: Let me break this down step by step:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=min(temperature, 1.0)\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content.strip()\n",
    "                tasks = self._parse_tasks(content)\n",
    "                if tasks:\n",
    "                    all_task_samples.append(tasks)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Task extraction sample {i+1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_task_samples:\n",
    "            return []\n",
    "        \n",
    "        # Apply self-consistency\n",
    "        return self._apply_consistency(all_task_samples)\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        \"\"\"Enhanced task parsing with better error handling\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        in_tasks_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect tasks section\n",
    "            if (line.lower().startswith('tasks:') or \n",
    "                (re.match(r'^1\\.', line) and not in_tasks_section)):\n",
    "                in_tasks_section = True\n",
    "                if not line.lower().endswith(':'):\n",
    "                    # Extract first task from this line\n",
    "                    task = re.sub(r'^[\\d\\.\\s]+', '', line).strip()\n",
    "                    if task and len(task) > 5:\n",
    "                        tasks.append(task)\n",
    "                continue\n",
    "            \n",
    "            if not in_tasks_section:\n",
    "                continue\n",
    "            \n",
    "            # Parse numbered tasks\n",
    "            if re.match(r'^[\\d\\.\\s]+', line):\n",
    "                task = re.sub(r'^[\\d\\.\\s]+', '', line).strip()\n",
    "                if task and len(task) > 5:\n",
    "                    tasks.append(task)\n",
    "        \n",
    "        return tasks\n",
    "    \n",
    "    def _apply_consistency(self, task_samples: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Apply self-consistency to select most consistent tasks\"\"\"\n",
    "        task_counts = Counter()\n",
    "        task_mapping = {}\n",
    "        \n",
    "        for sample in task_samples:\n",
    "            seen = set()\n",
    "            for task in sample:\n",
    "                normalized = self._normalize_task(task)\n",
    "                if normalized not in seen:\n",
    "                    task_counts[normalized] += 1\n",
    "                    seen.add(normalized)\n",
    "                    if normalized not in task_mapping:\n",
    "                        task_mapping[normalized] = task\n",
    "        \n",
    "        # Select tasks appearing in majority of samples\n",
    "        threshold = max(1, len(task_samples) // 2)\n",
    "        consistent_tasks = []\n",
    "        \n",
    "        for normalized, count in task_counts.items():\n",
    "            if count >= threshold:\n",
    "                consistent_tasks.append(task_mapping[normalized])\n",
    "        \n",
    "        # Sort by frequency\n",
    "        consistent_tasks.sort(key=lambda x: task_counts[self._normalize_task(x)], reverse=True)\n",
    "        return consistent_tasks\n",
    "    \n",
    "    def _normalize_task(self, task: str) -> str:\n",
    "        \"\"\"Normalize task for comparison\"\"\"\n",
    "        normalized = task.lower().strip()\n",
    "        # Remove common action words for better matching\n",
    "        normalized = re.sub(r'^(create|implement|design|build|add|make|ensure|handle)\\s+', '', normalized)\n",
    "        words = normalized.split()\n",
    "        stop_words = {'a', 'an', 'the', 'for', 'to', 'and', 'or', 'with', 'in', 'on', 'at'}\n",
    "        meaningful_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "        return ' '.join(sorted(meaningful_words))\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Story point estimation using Fibonacci scale with consistency\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "        self.few_shot_examples = \"\"\"\n",
    "Task: Identify relevant NSF stakeholders for each interview type\n",
    "\n",
    "Reasoning: Let me assess the complexity of this task:\n",
    "- This requires understanding organizational structure and roles\n",
    "- Need to map different interview types to appropriate stakeholders\n",
    "- Involves research and stakeholder analysis\n",
    "- Some uncertainty in identifying the \"right\" people\n",
    "- Moderate complexity with some unknowns\n",
    "\n",
    "Story Points: 3\n",
    "\n",
    "Task: Implement real-time data synchronization across multiple microservices\n",
    "\n",
    "Reasoning: Let me evaluate this task:\n",
    "- This is a complex distributed systems problem\n",
    "- Requires handling data consistency, network failures, conflict resolution\n",
    "- Multiple architectural decisions and implementation challenges\n",
    "- High technical complexity with many edge cases\n",
    "- Significant unknowns and potential for rework\n",
    "\n",
    "Story Points: 13\n",
    "\n",
    "Task: Add a button to the UI\n",
    "\n",
    "Reasoning: Let me consider this task:\n",
    "- This is a straightforward UI task\n",
    "- Minimal complexity, well-understood requirements\n",
    "- Basic frontend development with clear outcome\n",
    "- Very low complexity and uncertainty\n",
    "\n",
    "Story Points: 1\n",
    "\"\"\"\n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        task_points = {}\n",
    "        for task in tasks:\n",
    "            points = await self._estimate_single_task(task, num_samples=3)\n",
    "            task_points[task] = points\n",
    "        \n",
    "        total_points = sum(task_points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': task_points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "    \n",
    "    async def _estimate_single_task(self, task: str, num_samples: int = 3) -> int:\n",
    "        \"\"\"Estimate story points using self-consistency\"\"\"\n",
    "        estimates = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            temperature = 0.1 + (i * 0.1)  # Lower temperature for estimation consistency\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "Estimate story points for this task using the Fibonacci scale: {self.fibonacci_scale}\n",
    "\n",
    "Consider:\n",
    "- Complexity: How difficult is the implementation?\n",
    "- Uncertainty: How many unknowns are there?\n",
    "- Effort: How much work is required?\n",
    "- Risk: What could go wrong?\n",
    "\n",
    "Use Chain of Thought reasoning to explain your assessment, then provide the story points.\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Reasoning: Let me assess the complexity of this task:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                estimate = self._parse_story_points(response.choices[0].message.content)\n",
    "                if estimate:\n",
    "                    estimates.append(estimate)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Story point estimation failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not estimates:\n",
    "            return 3  # Default moderate estimate\n",
    "        \n",
    "        # Return median for consistency\n",
    "        estimates.sort()\n",
    "        return estimates[len(estimates) // 2]\n",
    "    \n",
    "    def _parse_story_points(self, content: str) -> int:\n",
    "        \"\"\"Extract story points from response\"\"\"\n",
    "        # Look for \"Story Points: X\" pattern\n",
    "        match = re.search(r'story\\s+points?:\\s*(\\d+)', content.lower())\n",
    "        if match:\n",
    "            points = int(match.group(1))\n",
    "            return points if points in self.fibonacci_scale else min(self.fibonacci_scale, key=lambda x: abs(x - points))\n",
    "        \n",
    "        # Look for standalone numbers in Fibonacci scale\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', content)\n",
    "        for num_str in reversed(numbers):  # Check from end to beginning\n",
    "            num = int(num_str)\n",
    "            if num in self.fibonacci_scale:\n",
    "                return num\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Enhanced skills mapping with standardized skill taxonomy\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.skill_taxonomy = {\n",
    "            'stakeholder_mapping': ['stakeholder analysis', 'stakeholder identification', 'organizational mapping'],\n",
    "            'interview_planning': ['interview design', 'interview preparation', 'research planning'],\n",
    "            'organizational_knowledge': ['org structure', 'organizational understanding', 'institutional knowledge'],\n",
    "            'guideline_development': ['documentation', 'procedure creation', 'guideline creation'],\n",
    "            'observation_protocols': ['observation methods', 'research protocols', 'observation procedures'],\n",
    "            'scheduling': ['calendar management', 'scheduling coordination', 'time management'],\n",
    "            'availability_coordination': ['coordination', 'scheduling logistics', 'availability management'],\n",
    "            'material_preparation': ['resource preparation', 'material creation', 'preparation tasks'],\n",
    "            'template_design': ['template creation', 'form design', 'documentation templates'],\n",
    "            'observation_tools': ['research tools', 'observation instruments', 'data collection tools'],\n",
    "            'training': ['instruction', 'education', 'skill transfer'],\n",
    "            'protocol_briefing': ['briefing', 'instruction delivery', 'protocol communication'],\n",
    "            'observer_preparation': ['observer training', 'preparation', 'readiness activities'],\n",
    "            'frontend_development': ['ui development', 'client-side', 'web frontend'],\n",
    "            'backend_development': ['server-side', 'backend', 'api development'],\n",
    "            'database_management': ['data storage', 'database', 'data management'],\n",
    "            'javascript': ['js', 'client scripting', 'web scripting'],\n",
    "            'api_integration': ['api', 'service integration', 'external services'],\n",
    "            'url_handling': ['url manipulation', 'link handling', 'web navigation'],\n",
    "            'error_handling': ['exception handling', 'error management', 'fault tolerance']\n",
    "        }\n",
    "        \n",
    "        self.few_shot_examples = \"\"\"\n",
    "Task: Identify relevant NSF stakeholders for each interview type\n",
    "\n",
    "Reasoning: This task requires:\n",
    "- Understanding organizational structure and roles within NSF\n",
    "- Mapping different types of interviews to appropriate stakeholders\n",
    "- Knowledge of institutional hierarchy and decision-making processes\n",
    "- Analytical skills to determine relevance and appropriateness\n",
    "\n",
    "Required Skills:\n",
    "- stakeholder_mapping\n",
    "- interview_planning  \n",
    "- organizational_knowledge\n",
    "\n",
    "Task: Implement click handler to capture address data\n",
    "\n",
    "Reasoning: This task involves:\n",
    "- Frontend development to handle user interactions\n",
    "- JavaScript programming for event handling\n",
    "- DOM manipulation to capture data from elements\n",
    "- Understanding of web event systems\n",
    "\n",
    "Required Skills:\n",
    "- frontend_development\n",
    "- javascript\n",
    "- event_handling\n",
    "\"\"\"\n",
    "    \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills using consistency and standardized taxonomy\"\"\"\n",
    "        all_skill_samples = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            temperature = 0.3 + (i * 0.15)\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "Identify the specific technical and domain skills required for this task.\n",
    "Use standardized skill names and focus on what expertise is actually needed.\n",
    "\n",
    "Use Chain of Thought reasoning: analyze what the task involves, then identify required skills.\n",
    "\n",
    "Available skill categories: {list(self.skill_taxonomy.keys())}\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Reasoning: This task requires:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                skills = self._parse_and_normalize_skills(response.choices[0].message.content)\n",
    "                if skills:\n",
    "                    all_skill_samples.append(skills)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Skill mapping failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_skill_samples:\n",
    "            return [\"general_development\"]\n",
    "        \n",
    "        return self._apply_skill_consistency(all_skill_samples)\n",
    "    \n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "    def _parse_and_normalize_skills(self, content: str) -> List[str]:\n",
    "        \"\"\"Parse skills and normalize to taxonomy\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        raw_skills = []\n",
    "        in_skills_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if (line.lower().startswith('required skills:') or \n",
    "                line.lower().startswith('skills:')):\n",
    "                in_skills_section = True\n",
    "                continue\n",
    "            \n",
    "            if not in_skills_section:\n",
    "                continue\n",
    "            \n",
    "            # Extract skill from bullet points\n",
    "            skill = re.sub(r'^[\\-\\*\\s]+', '', line).strip()\n",
    "            if skill and len(skill) > 2:\n",
    "                raw_skills.append(skill.lower())\n",
    "        \n",
    "        # Normalize to taxonomy\n",
    "        normalized_skills = []\n",
    "        for raw_skill in raw_skills:\n",
    "            normalized = self._normalize_to_taxonomy(raw_skill)\n",
    "            if normalized and normalized not in normalized_skills:\n",
    "                normalized_skills.append(normalized)\n",
    "        \n",
    "        return normalized_skills\n",
    "    \n",
    "    def _normalize_to_taxonomy(self, raw_skill: str) -> str:\n",
    "        \"\"\"Map raw skill to standardized taxonomy\"\"\"\n",
    "        raw_skill = raw_skill.lower().strip()\n",
    "        \n",
    "        # Direct match\n",
    "        if raw_skill in self.skill_taxonomy:\n",
    "            return raw_skill\n",
    "        \n",
    "        # Find best match in taxonomy\n",
    "        for standard_skill, variations in self.skill_taxonomy.items():\n",
    "            if any(var in raw_skill or raw_skill in var for var in variations):\n",
    "                return standard_skill\n",
    "        \n",
    "        # If no match found, create a reasonable mapping\n",
    "        if any(word in raw_skill for word in ['frontend', 'ui', 'client']):\n",
    "            return 'frontend_development'\n",
    "        elif any(word in raw_skill for word in ['backend', 'server', 'api']):\n",
    "            return 'backend_development'\n",
    "        elif any(word in raw_skill for word in ['stakeholder', 'organization']):\n",
    "            return 'stakeholder_mapping'\n",
    "        elif any(word in raw_skill for word in ['schedule', 'calendar']):\n",
    "            return 'scheduling'\n",
    "        else:\n",
    "            return 'general_development'\n",
    "    \n",
    "    def _apply_skill_consistency(self, skill_samples: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Apply consistency to skill selection\"\"\"\n",
    "        skill_counts = Counter()\n",
    "        \n",
    "        for sample in skill_samples:\n",
    "            for skill in set(sample):  # Deduplicate within sample\n",
    "                skill_counts[skill] += 1\n",
    "        \n",
    "        # Select skills appearing in majority of samples\n",
    "        threshold = max(1, len(skill_samples) // 2)\n",
    "        consistent_skills = [skill for skill, count in skill_counts.items() if count >= threshold]\n",
    "        \n",
    "        # Sort by frequency\n",
    "        consistent_skills.sort(key=lambda x: skill_counts[x], reverse=True)\n",
    "        return consistent_skills\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Enhanced dependency analysis with rework effort estimation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_examples = \"\"\"\n",
    "Tasks:\n",
    "1. Identify relevant NSF stakeholders for each interview type\n",
    "2. Create interview observation guidelines and protocols  \n",
    "3. Schedule stakeholder availability coordination\n",
    "4. Prepare observation materials and note-taking templates\n",
    "5. Brief observers on interview protocols and expectations\n",
    "\n",
    "Reasoning: Let me analyze dependencies step by step:\n",
    "- Task 1 (Identify stakeholders): This is foundational - no dependencies\n",
    "- Task 2 (Create guidelines): Independent of stakeholder identification - no dependencies  \n",
    "- Task 3 (Schedule availability): Must know WHO to schedule (stakeholders) - depends on Task 1\n",
    "- Task 4 (Prepare materials): Should follow the guidelines created - depends on Task 2\n",
    "- Task 5 (Brief observers): Needs both scheduled people and prepared materials - depends on Task 3 and Task 4\n",
    "\n",
    "Dependencies:\n",
    "- Task 3 depends on Task 1 (rework_effort: 2)\n",
    "- Task 4 depends on Task 2 (rework_effort: 2)  \n",
    "- Task 5 depends on Task 3 (rework_effort: 2)\n",
    "- Task 5 depends on Task 4 (rework_effort: 2)\n",
    "\"\"\"\n",
    "    \n",
    "    async def analyze_dependencies(self, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Analyze task dependencies with rework effort estimation\"\"\"\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "        \n",
    "        all_dependency_samples = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            temperature = 0.2 + (i * 0.1)\n",
    "            \n",
    "            tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "            prompt = f\"\"\"\n",
    "Analyze dependencies between these tasks. Identify which tasks must be completed before others can start.\n",
    "\n",
    "Use Chain of Thought reasoning: Think through each task and its logical prerequisites.\n",
    "\n",
    "For dependencies, estimate rework_effort (1-8 story points) if the prerequisite fails or changes:\n",
    "- 1-2: Minimal rework, mostly configuration changes\n",
    "- 3-5: Moderate rework, some logic changes needed  \n",
    "- 6-8: Major rework, significant changes required\n",
    "\n",
    "IMPORTANT: \n",
    "- Only identify actual logical dependencies, not every possible relationship\n",
    "- After reasoning, use format: \"- Task X depends on Task Y (rework_effort: N)\"\n",
    "- Include \"Dependencies:\" section header\n",
    "\n",
    "Example:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Reasoning: Let me analyze dependencies step by step:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                dependencies = self._parse_dependencies(response.choices[0].message.content, tasks)\n",
    "                all_dependency_samples.append(dependencies)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Dependency analysis failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_dependency_samples:\n",
    "            return {}\n",
    "        \n",
    "        return self._apply_dependency_consistency(all_dependency_samples, tasks)\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse dependencies from response\"\"\"\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        in_dependencies = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if (line.lower().startswith('dependencies:') or \n",
    "                'depends on' in line.lower()):\n",
    "                in_dependencies = True\n",
    "                if 'depends on' not in line.lower():\n",
    "                    continue\n",
    "            \n",
    "            if not in_dependencies and 'depends on' not in line.lower():\n",
    "                continue\n",
    "            \n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse \"Task X depends on Task Y (rework_effort: N)\"\n",
    "                    match = re.search(r'task\\s+(\\d+)\\s+depends\\s+on\\s+task\\s+(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_idx = int(match.group(1)) - 1\n",
    "                        prerequisite_idx = int(match.group(2)) - 1\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 0 <= dependent_idx < len(tasks) and 0 <= prerequisite_idx < len(tasks):\n",
    "                            dependent_task = tasks[dependent_idx]\n",
    "                            prerequisite_task = tasks[prerequisite_idx]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                'task_id': f\"T_{prerequisite_idx + 1:03d}\",\n",
    "                                'task_description': prerequisite_task,\n",
    "                                'rework_effort': min(8, max(1, rework_effort))  # Clamp to 1-8\n",
    "                            })\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "    \n",
    "    def _apply_dependency_consistency(self, dependency_samples: List[Dict], tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Apply consistency to dependency relationships\"\"\"\n",
    "        dependency_counts = defaultdict(int)\n",
    "        dependency_details = defaultdict(list)\n",
    "        \n",
    "        for sample in dependency_samples:\n",
    "            for dependent_task, deps in sample.items():\n",
    "                for dep in deps:\n",
    "                    dep_key = f\"{dependent_task} -> {dep['task_description']}\"\n",
    "                    dependency_counts[dep_key] += 1\n",
    "                    dependency_details[dep_key].append(dep['rework_effort'])\n",
    "        \n",
    "        # Select dependencies appearing in majority of samples\n",
    "        threshold = max(1, len(dependency_samples) // 2)\n",
    "        consistent_dependencies = {}\n",
    "        \n",
    "        for dep_key, count in dependency_counts.items():\n",
    "            if count >= threshold:\n",
    "                dependent_task, prerequisite_task = dep_key.split(' -> ')\n",
    "                \n",
    "                # Get median rework effort\n",
    "                efforts = dependency_details[dep_key]\n",
    "                median_effort = sorted(efforts)[len(efforts) // 2]\n",
    "                \n",
    "                # Find prerequisite task index\n",
    "                try:\n",
    "                    prerequisite_idx = tasks.index(prerequisite_task)\n",
    "                    task_id = f\"T_{prerequisite_idx + 1:03d}\"\n",
    "                    \n",
    "                    if dependent_task not in consistent_dependencies:\n",
    "                        consistent_dependencies[dependent_task] = []\n",
    "                    \n",
    "                    consistent_dependencies[dependent_task].append({\n",
    "                        'task_id': task_id,\n",
    "                        'rework_effort': median_effort\n",
    "                    })\n",
    "                    \n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        return consistent_dependencies\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Validation error: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the complete pipeline\"\"\"\n",
    "    print(f\"Processing: {user_story[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction\n",
    "        print(\"  Step 1: Extracting tasks...\")\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        print(f\"  Extracted {len(tasks)} tasks\")\n",
    "        \n",
    "        # Step 2 & 3: Parallel processing of Story Points and Skills\n",
    "        print(\"  Steps 2-3: Estimating story points and mapping skills...\")\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Process all tasks in parallel for efficiency\n",
    "        story_points_tasks = [estimator._estimate_single_task(task) for task in tasks]\n",
    "        skills_tasks = [skills_agent.map_skills(task) for task in tasks]\n",
    "        \n",
    "        story_points_results, skills_results = await asyncio.gather(\n",
    "            asyncio.gather(*story_points_tasks),\n",
    "            asyncio.gather(*skills_tasks)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Dependency Analysis\n",
    "        print(\"  Step 4: Analyzing dependencies...\")\n",
    "        dependency_agent = DependencyAgent()\n",
    "        dependencies = await dependency_agent.analyze_dependencies(tasks)\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        print(\"  Step 5: Formatting and validating...\")\n",
    "        \n",
    "        # Build task data structure\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points_results)\n",
    "        \n",
    "        for i, (task, story_points, skills) in enumerate(zip(tasks, story_points_results, skills_results)):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                task_dependencies = dependencies[task]\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": story_points,\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        # Final validation and formatting\n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        print(f\"  Completed: {len(tasks)} tasks, {total_story_points} total story points\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing user story: {str(e)}\")\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the pipeline\"\"\"\n",
    "    print(f\"Processing {len(user_stories)} user stories through enhanced pipeline...\")\n",
    "    \n",
    "    # Process stories in parallel for efficiency while preventing context loss\n",
    "    tasks = [process_user_story_pipeline(story) for story in user_stories]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Handle any exceptions\n",
    "    final_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            final_results.append({\n",
    "                \"input\": user_stories[i],\n",
    "                \"output\": {\n",
    "                    \"error\": str(result),\n",
    "                    \"story_points\": 0,\n",
    "                    \"tasks\": []\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "def format_pipeline_output(results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Format pipeline results for display\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"ENHANCED PIPELINE PROCESSING RESULTS\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    total_story_points = 0\n",
    "    total_tasks = 0\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- USER STORY {i} ---\")\n",
    "        output.append(f\"Input: {result['input']}\")\n",
    "        \n",
    "        if 'error' in result['output']:\n",
    "            output.append(f\"ERROR: {result['output']['error']}\")\n",
    "            continue\n",
    "        \n",
    "        story_output = result['output']\n",
    "        output.append(f\"Total Story Points: {story_output['story_points']}\")\n",
    "        output.append(f\"Tasks: {len(story_output['tasks'])}\")\n",
    "        \n",
    "        total_story_points += story_output['story_points']\n",
    "        total_tasks += len(story_output['tasks'])\n",
    "        \n",
    "        for task in story_output['tasks']:\n",
    "            output.append(f\"\\n  Task {task['id']}: {task['description']}\")\n",
    "            output.append(f\"    Story Points: {task['story_points']}\")\n",
    "            output.append(f\"    Skills: {', '.join(task['required_skills'])}\")\n",
    "            \n",
    "            if task['depends_on']:\n",
    "                deps = [f\"{dep['task_id']} (rework: {dep['rework_effort']})\" for dep in task['depends_on']]\n",
    "                output.append(f\"    Dependencies: {', '.join(deps)}\")\n",
    "    \n",
    "    output.append(\"\\n\" + \"=\" * 80)\n",
    "    output.append(\"SUMMARY\")\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(f\"Total User Stories Processed: {len(results)}\")\n",
    "    output.append(f\"Total Tasks Generated: {total_tasks}\")\n",
    "    output.append(f\"Total Story Points: {total_story_points}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    print(\"=== Enhanced Pipeline Task Decomposition System ===\")\n",
    "    print(\"Pipeline: Story Input â†’ Task Extractor â†’ Story Points & Skills â†’ Dependencies â†’ Validator â†’ Output\")\n",
    "    print(\"Features: Self-consistency, context preservation, parallel processing\\n\")\n",
    "    results = await process_multiple_user_stories_pipeline(stories_list)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(format_pipeline_output(results))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"JSON OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(\"-\" * 40)\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_pipeline(user_stories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307dc66c-162b-46dd-a0cc-df231e858ba4",
   "metadata": {},
   "source": [
    "- **Dependencies:**\n",
    "  \n",
    "> **collections.Counter:** Frequency counting for self-consistency analysis and task selection\n",
    "\n",
    "> **collections.defaultdict:** Default dictionary for dependency counting and aggregation\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TaskExtractorAgent:** Self-consistency task decomposition using multiple temperature sampling (0.2, 0.4, 0.6) with few-shot examples. Applies consistency analysis across samples using task normalization and frequency counting. Key method: decompose(user_story, num_samples=3) -> List[str] with fallback mechanisms and robust parsing.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Self-consistency story point estimation using Fibonacci scale [1,2,3,5,8,13,21] with multiple temperature sampling for consistency. Processes each task separately with median selection for robustness. Key methods: estimate_story_points(user_story, tasks) -> Dict and _estimate_single_task(task, num_samples=3) -> int.\n",
    "\n",
    "> **RequiredSkillsAgent:** Enhanced skill mapping with standardized skill taxonomy and self-consistency sampling. Contains comprehensive skill_taxonomy with mappings for stakeholder_mapping, interview_planning, organizational_knowledge, frontend_development, backend_development, and specialized domains. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Self-consistency dependency analysis with rework effort estimation using multiple sampling and median selection. Enhanced few-shot examples with detailed reasoning processes. Key method: analyze_dependencies(tasks) -> Dict with consistency application and deduplication.\n",
    "\n",
    "> **FormatValidator:** Output structure validation and formatting with comprehensive error handling and field validation. Ensures data integrity and structure compliance. Key method: validate_and_format(user_story, tasks_data, total_story_points) -> Dict.\n",
    "\n",
    "- **Pipeline Functions:**\n",
    "\n",
    "> **process_user_story_pipeline(user_story):** Main single story processing function with self-consistency integration across all agents, parallel processing for story points and skills, comprehensive error handling, and structured output generation.\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories):** Batch processing function with parallel story processing and individual exception management for robust error handling.\n",
    "\n",
    "> **format_pipeline_output(results):** Enhanced output formatting with detailed statistics, task breakdown, and comprehensive summary reporting.\n",
    "\n",
    "> **run_pipeline(stories_list):** Enhanced async wrapper with complete pipeline execution, formatted output, and detailed progress reporting.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **decompose(user_story: str, num_samples: int = 3):** Self-consistency task extraction with multiple temperature sampling, few-shot examples with concrete reasoning processes, consistency analysis using task normalization, and frequency-based selection with threshold filtering.\n",
    "\n",
    "> **_apply_consistency(task_samples: List[List[str]]):** Task consistency analysis using Counter for frequency tracking, task normalization for similarity detection, majority voting with configurable thresholds, and frequency-based sorting for optimal selection.\n",
    "\n",
    "> **_normalize_task(task: str):** Task normalization for consistency analysis removing common action words, filtering stop words, extracting meaningful terms, and creating sorted canonical representations for comparison.\n",
    "\n",
    "> **_estimate_single_task(task: str, num_samples: int = 3):** Individual task estimation with self-consistency using multiple temperature sampling (0.1, 0.2, 0.3), median selection for robustness, and comprehensive error handling with fallback defaults.\n",
    "\n",
    "> **map_skills(task: str):** Self-consistency skill mapping with standardized taxonomy, multiple temperature sampling, skill normalization and deduplication, and consistency application with threshold-based selection.\n",
    "\n",
    "> **_normalize_to_taxonomy(raw_skill: str):** Advanced skill normalization with comprehensive taxonomy mapping, direct matching and variation detection, fallback categorization for unmapped skills, and intelligent domain classification.\n",
    "\n",
    "> **analyze_dependencies(tasks: List[str]):** Self-consistency dependency analysis with multiple sampling, detailed few-shot examples, consistency application using defaultdict counting, and median rework effort selection for robust estimation.\n",
    "\n",
    "> **_apply_dependency_consistency(dependency_samples, tasks):** Dependency consistency analysis with frequency counting, majority voting for relationship validation, median effort calculation, and comprehensive deduplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdef146-2180-4680-a298-50da2c4f853c",
   "metadata": {},
   "source": [
    "#### 4.4.8 Strategy 8: Reflexion Prompting\n",
    "- **Definition**\n",
    "> **Having the model review and critique its own output, then revise based on its self-assessment.**\n",
    "\n",
    "- **Core Approach**\n",
    "\n",
    "> **Self-review and critique** of initial output\n",
    "\n",
    "> **Self-assessment methodology** for improvement identification\n",
    "\n",
    "> **Iterative revision process** based on internal feedback\n",
    "\n",
    "- âœ… **Advantages**\n",
    "\n",
    "> **Built-in quality control and error correction**\n",
    "\n",
    ">**Helps catch missed dependencies and skills**\n",
    "\n",
    "- âŒ **Disadvantages**\n",
    "\n",
    "> **Requires additional processing time**\n",
    "\n",
    ">**Can get stuck in revision loops**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3711d79-7931-4c32-84d6-2e9db6af8b9c",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87392b0f-073d-430c-9803-5c1293ad685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reflexion-Enhanced Pipeline Task Decomposition System ===\n",
      "Pipeline: Story Input â†’ Task Extractor â†’ Story Points & Skills â†’ Dependencies â†’ Validator â†’ Output\n",
      "Enhanced with: Actor-Evaluator-Self-Reflection framework for iterative improvement\n",
      "Features: Learning from failures, memory-based improvement, robust error handling\n",
      "\n",
      "Processing 2 user stories through Reflexion-enhanced pipeline...\n",
      "\n",
      "--- Processing Story 1/2 ---\n",
      "Processing: As a user, I want to create an account so that I c...\n",
      "  Step 1: Extracting tasks with Reflexion...\n",
      "Extracting tasks: As a user, I want to create an account so that I c...\n",
      "  Starting task_extractor reflexion with 3 episodes...\n",
      "    Episode 1/3\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Extracted 5 tasks\n",
      "  Steps 2-3: Estimating story points and mapping skills with Reflexion...\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.90\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 0.90\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.70\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.70\n",
      "  Step 4: Analyzing dependencies with Reflexion...\n",
      "Analyzing dependencies...\n",
      "  Starting dependency_analyzer reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Step 5: Formatting and validating...\n",
      "  Completed: 5 tasks, 32 total story points\n",
      "\n",
      "--- Processing Story 2/2 ---\n",
      "Processing: As an admin, I want to view analytics dashboard so...\n",
      "  Step 1: Extracting tasks with Reflexion...\n",
      "Extracting tasks: As an admin, I want to view analytics dashboard so...\n",
      "  Starting task_extractor reflexion with 3 episodes...\n",
      "    Episode 1/3\n",
      "      Score: 0.97\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 0.97\n",
      "  Extracted 5 tasks\n",
      "  Steps 2-3: Estimating story points and mapping skills with Reflexion...\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting story_estimator reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Starting skills_mapper reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 0.40\n",
      "    Episode 2/2\n",
      "      Score: 0.40\n",
      "  Best score: 0.40\n",
      "  Step 4: Analyzing dependencies with Reflexion...\n",
      "Analyzing dependencies...\n",
      "  Starting dependency_analyzer reflexion with 2 episodes...\n",
      "    Episode 1/2\n",
      "      Score: 1.00\n",
      "      Early stopping - achieved good score\n",
      "  Best score: 1.00\n",
      "  Step 5: Formatting and validating...\n",
      "  Completed: 5 tasks, 34 total story points\n",
      "\n",
      "================================================================================\n",
      "REFLEXION PIPELINE PROCESSING COMPLETE\n",
      "================================================================================\n",
      "================================================================================\n",
      "REFLEXION-ENHANCED PIPELINE PROCESSING RESULTS\n",
      "================================================================================\n",
      "\n",
      "--- USER STORY 1 ---\n",
      "Input: As a user, I want to create an account so that I can access personalized features\n",
      "Total Story Points: 32\n",
      "Tasks: 5\n",
      "\n",
      "  Task T_001: **Design and implement a user account database schema**\n",
      "    Story Points: 8\n",
      "    Skills: general_development\n",
      "\n",
      "  Task T_002: **Develop a registration form with input validation**\n",
      "    Story Points: 5\n",
      "    Skills: general_development\n",
      "    Dependencies: T_001 (rework: 3)\n",
      "\n",
      "  Task T_003: **Implement username and email uniqueness checks**\n",
      "    Story Points: 3\n",
      "    Skills: general_development\n",
      "    Dependencies: T_001 (rework: 2), T_002 (rework: 1)\n",
      "\n",
      "  Task T_004: **Develop an email confirmation system**\n",
      "    Story Points: 8\n",
      "    Skills: backend_development, database_management, **email_service_integration_(e.g.,_sendgrid_or_mailgun)**:_to_integrate_an_email_service_that_can_send_emails_to_users_with_a_unique_confirmation_link., frontend_development\n",
      "    Dependencies: T_002 (rework: 2)\n",
      "\n",
      "  Task T_005: **Implement login functionality with authentication and authorization**\n",
      "    Story Points: 8\n",
      "    Skills: frontend_development, security, database_management\n",
      "    Dependencies: T_001 (rework: 4), T_003 (rework: 2)\n",
      "\n",
      "--- USER STORY 2 ---\n",
      "Input: As an admin, I want to view analytics dashboard so that I can monitor system performance\n",
      "Total Story Points: 34\n",
      "Tasks: 5\n",
      "\n",
      "  Task T_001: **Design the dashboard layout and wireframes**\n",
      "    Story Points: 5\n",
      "    Skills: general_development\n",
      "\n",
      "  Task T_002: **Develop the backend API for data retrieval**\n",
      "    Story Points: 8\n",
      "    Skills: general_development\n",
      "\n",
      "  Task T_003: **Implement dashboard components and charts**\n",
      "    Story Points: 8\n",
      "    Skills: general_development\n",
      "    Dependencies: T_001 (rework: 2)\n",
      "\n",
      "  Task T_004: **Integrate the API with the dashboard components**\n",
      "    Story Points: 8\n",
      "    Skills: general_development\n",
      "    Dependencies: T_002 (rework: 3), T_003 (rework: 2)\n",
      "\n",
      "  Task T_005: **Test and refine the dashboard**\n",
      "    Story Points: 5\n",
      "    Skills: general_development\n",
      "    Dependencies: T_004 (rework: 4)\n",
      "\n",
      "================================================================================\n",
      "REFLEXION FRAMEWORK SUMMARY\n",
      "================================================================================\n",
      "Total User Stories Processed: 2\n",
      "Total Tasks Generated: 10\n",
      "Total Story Points: 66\n",
      "Framework Benefits: Iterative improvement, learning from failures, self-reflection\n",
      "\n",
      "================================================================================\n",
      "JSON OUTPUT\n",
      "================================================================================\n",
      "{\n",
      "  \"input\": \"As a user, I want to create an account so that I can access personalized features\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 32,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"**Design and implement a user account database schema**\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Develop a registration form with input validation**\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 3\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Implement username and email uniqueness checks**\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 1\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Develop an email confirmation system**\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"backend_development\",\n",
      "          \"database_management\",\n",
      "          \"**email_service_integration_(e.g.,_sendgrid_or_mailgun)**:_to_integrate_an_email_service_that_can_send_emails_to_users_with_a_unique_confirmation_link.\",\n",
      "          \"frontend_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Implement login functionality with authentication and authorization**\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 4\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"security\",\n",
      "          \"database_management\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----------------------------------------\n",
      "{\n",
      "  \"input\": \"As an admin, I want to view analytics dashboard so that I can monitor system performance\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 34,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"**Design the dashboard layout and wireframes**\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Develop the backend API for data retrieval**\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Implement dashboard components and charts**\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Integrate the API with the dashboard components**\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 3\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"**Test and refine the dashboard**\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 4\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    \"\"\"Represents a trajectory (sequence of actions and observations)\"\"\"\n",
    "    input_state: str\n",
    "    actions: List[str] = field(default_factory=list)\n",
    "    observations: List[str] = field(default_factory=list)\n",
    "    final_output: Any = None\n",
    "    rework_score: float = 0.0\n",
    "    episode_id: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ReflexionMemory:\n",
    "    \"\"\"Stores experiences and reflections for learning\"\"\"\n",
    "    short_term_memory: List[Trajectory] = field(default_factory=list)\n",
    "    long_term_memory: List[str] = field(default_factory=list)\n",
    "    performance_history: List[float] = field(default_factory=list)\n",
    "\n",
    "class ImprovedReflexionFramework:\n",
    "    \"\"\"Enhanced Reflexion framework for pipeline agents\"\"\"\n",
    "    \n",
    "    def __init__(self, max_episodes: int = 3):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.memory = ReflexionMemory()\n",
    "        self.current_episode = 0\n",
    "    \n",
    "    async def run_reflexion_loop(self, task: str, agent_type: str, context: Dict = None) -> Any:\n",
    "        \"\"\"Main Reflexion loop optimized for pipeline agents\"\"\"\n",
    "        best_result = None\n",
    "        best_score = 0.0\n",
    "        \n",
    "        print(f\"  Starting {agent_type} reflexion with {self.max_episodes} episodes...\")\n",
    "        \n",
    "        for episode in range(self.max_episodes):\n",
    "            self.current_episode = episode\n",
    "            print(f\"    Episode {episode + 1}/{self.max_episodes}\")\n",
    "            \n",
    "            try:\n",
    "                # Actor generates trajectory based on agent type\n",
    "                trajectory = await self._actor_generate_trajectory(task, agent_type, context)\n",
    "                \n",
    "                # Evaluator scores the trajectory\n",
    "                rework_score = await self._evaluator_score_trajectory(trajectory, agent_type)\n",
    "                trajectory.rework_score = rework_score\n",
    "                trajectory.episode_id = episode\n",
    "                \n",
    "                # Update memory\n",
    "                self.memory.short_term_memory.append(trajectory)\n",
    "                self.memory.performance_history.append(rework_score)\n",
    "                \n",
    "                # Track best result\n",
    "                if rework_score > best_score and trajectory.final_output is not None:\n",
    "                    best_score = rework_score\n",
    "                    best_result = trajectory.final_output\n",
    "                \n",
    "                # Self-reflection for improvement (if not last episode)\n",
    "                if episode < self.max_episodes - 1 and rework_score < 0.8:\n",
    "                    reflection = await self._self_reflection_generate_feedback(trajectory, agent_type)\n",
    "                    self.memory.long_term_memory.append(reflection)\n",
    "                \n",
    "                print(f\"      Score: {rework_score:.2f}\")\n",
    "                \n",
    "                # Early stopping for good results\n",
    "                if rework_score >= 0.8:\n",
    "                    print(f\"      Early stopping - achieved good score\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      Episode {episode + 1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  Best score: {best_score:.2f}\")\n",
    "        return best_result\n",
    "    \n",
    "    async def _actor_generate_trajectory(self, task: str, agent_type: str, context: Dict = None) -> Trajectory:\n",
    "        \"\"\"Actor generates responses based on agent type\"\"\"\n",
    "        memory_context = self._create_memory_context()\n",
    "        \n",
    "        try:\n",
    "            if agent_type == \"task_extractor\":\n",
    "                response_text = await self._actor_extract_tasks(task, memory_context)\n",
    "                final_output = self._parse_task_list(response_text)\n",
    "            elif agent_type == \"story_estimator\":\n",
    "                response_text = await self._actor_estimate_story_points(task, memory_context)\n",
    "                final_output = self._parse_story_points(response_text)\n",
    "            elif agent_type == \"skills_mapper\":\n",
    "                response_text = await self._actor_map_skills(task, memory_context)\n",
    "                final_output = self._parse_skills_list(response_text)\n",
    "            elif agent_type == \"dependency_analyzer\":\n",
    "                tasks_list = context.get('tasks', []) if context else []\n",
    "                response_text = await self._actor_analyze_dependencies(task, tasks_list, memory_context)\n",
    "                final_output = self._parse_dependencies(response_text, tasks_list)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
    "            \n",
    "            # Parse actions and observations\n",
    "            actions, observations = self._parse_reasoning(response_text)\n",
    "            \n",
    "            return Trajectory(\n",
    "                input_state=task,\n",
    "                actions=actions,\n",
    "                observations=observations,\n",
    "                final_output=final_output,\n",
    "                rework_score=0.0,\n",
    "                episode_id=self.current_episode\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        Actor generation failed: {str(e)}\")\n",
    "            return Trajectory(input_state=task, final_output=None)\n",
    "    \n",
    "    async def _actor_extract_tasks(self, user_story: str, memory_context: str) -> str:\n",
    "        \"\"\"Extract tasks from user story with reflexion\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are an expert at breaking down user stories into specific, actionable tasks.\n",
    "\n",
    "USER STORY: {user_story}\n",
    "\n",
    "LEARNING FROM PREVIOUS ATTEMPTS:\n",
    "{memory_context}\n",
    "\n",
    "Think step by step:\n",
    "\n",
    "REASONING: Analyze what this user story requires\n",
    "[Provide detailed analysis of requirements and implementation needs]\n",
    "\n",
    "TASK EXTRACTION: Break into specific, actionable tasks\n",
    "1. [First specific technical task]\n",
    "2. [Second specific technical task]\n",
    "3. [Third specific technical task]\n",
    "4. [Fourth specific technical task]\n",
    "5. [Fifth specific technical task]\n",
    "\n",
    "Requirements for each task:\n",
    "- Specific and implementable by a developer\n",
    "- Clear scope and deliverables\n",
    "- Technically focused\n",
    "- Testable outcome\n",
    "\n",
    "Provide at least 4-5 concrete tasks.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def _actor_estimate_story_points(self, task: str, memory_context: str) -> str:\n",
    "        \"\"\"Estimate story points using Fibonacci scale\"\"\"\n",
    "        fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Estimate story points for this task using the Fibonacci scale: {fibonacci_scale}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "LEARNING FROM PREVIOUS ATTEMPTS:\n",
    "{memory_context}\n",
    "\n",
    "Consider these factors:\n",
    "\n",
    "COMPLEXITY ANALYSIS:\n",
    "- Technical complexity: How difficult is the implementation?\n",
    "- Unknown factors: How many uncertainties exist?\n",
    "- Integration points: How many systems/components involved?\n",
    "- Risk factors: What could go wrong?\n",
    "\n",
    "EFFORT ESTIMATION:\n",
    "- Development time required\n",
    "- Testing and validation effort\n",
    "- Documentation needs\n",
    "- Potential rework\n",
    "\n",
    "STORY POINTS: [Select from Fibonacci scale: 1, 2, 3, 5, 8, 13, 21]\n",
    "\n",
    "Reasoning: [Explain your assessment]\n",
    "Final Estimate: [X] story points\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=600\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def _actor_map_skills(self, task: str, memory_context: str) -> str:\n",
    "        \"\"\"Map required skills for task\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Identify the specific technical skills required to complete this task.\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "LEARNING FROM PREVIOUS ATTEMPTS:\n",
    "{memory_context}\n",
    "\n",
    "Think step by step:\n",
    "\n",
    "TECHNICAL ANALYSIS: What technical work is involved?\n",
    "[Analyze the technical requirements and implementation needs]\n",
    "\n",
    "SKILL IDENTIFICATION: What specific skills are needed?\n",
    "- [Technical skill 1]\n",
    "- [Technical skill 2] \n",
    "- [Technical skill 3]\n",
    "- [Technical skill 4]\n",
    "\n",
    "Focus on concrete technical skills:\n",
    "- Programming languages (JavaScript, Python, etc.)\n",
    "- Frameworks and libraries (React, Django, etc.)\n",
    "- Technologies (databases, APIs, cloud services)\n",
    "- Methodologies (testing, deployment, security)\n",
    "- Domain expertise (UI/UX, DevOps, data analysis)\n",
    "\n",
    "Provide 3-5 specific skills.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=600\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def _actor_analyze_dependencies(self, context: str, tasks: List[str], memory_context: str) -> str:\n",
    "        \"\"\"Analyze dependencies between tasks\"\"\"\n",
    "        if len(tasks) <= 1:\n",
    "            return \"No dependencies found - only one task.\"\n",
    "        \n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Analyze dependencies between these tasks. Identify which tasks must be completed before others can start.\n",
    "\n",
    "TASKS:\n",
    "{tasks_str}\n",
    "\n",
    "LEARNING FROM PREVIOUS ATTEMPTS:\n",
    "{memory_context}\n",
    "\n",
    "Think step by step:\n",
    "\n",
    "DEPENDENCY ANALYSIS: Review each task for prerequisites\n",
    "[Analyze logical dependencies and prerequisite relationships]\n",
    "\n",
    "DEPENDENCIES:\n",
    "- Task [X] depends on Task [Y] (rework_effort: [1-8])\n",
    "- Task [A] depends on Task [B] (rework_effort: [1-8])\n",
    "\n",
    "Guidelines:\n",
    "- Only identify actual logical dependencies\n",
    "- Estimate rework_effort (1-8 story points) if prerequisite changes\n",
    "- Consider: data flow, logical sequence, shared components\n",
    "- Focus on \"must be completed before\" relationships\n",
    "\n",
    "Format: \"Task X depends on Task Y (rework_effort: N)\"\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _parse_reasoning(self, response: str) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Extract reasoning actions and observations\"\"\"\n",
    "        lines = response.split('\\n')\n",
    "        actions = []\n",
    "        observations = []\n",
    "        \n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect reasoning sections\n",
    "            if any(keyword in line.upper() for keyword in ['REASONING:', 'ANALYSIS:', 'COMPLEXITY ANALYSIS:', 'TECHNICAL ANALYSIS:']):\n",
    "                current_section = 'observations'\n",
    "                obs = re.sub(r'^[A-Z\\s:]+', '', line).strip()\n",
    "                if obs:\n",
    "                    observations.append(obs)\n",
    "                continue\n",
    "            elif any(keyword in line.upper() for keyword in ['TASK EXTRACTION:', 'ESTIMATION:', 'SKILL IDENTIFICATION:', 'DEPENDENCIES:']):\n",
    "                current_section = 'actions'\n",
    "                actions.append(f\"Execute {line.lower().replace(':', '')}\")\n",
    "                continue\n",
    "            \n",
    "            # Collect content based on section\n",
    "            if current_section == 'observations' and len(line) > 10:\n",
    "                observations.append(line)\n",
    "            elif current_section == 'actions' and len(line) > 5:\n",
    "                actions.append(f\"Process: {line}\")\n",
    "        \n",
    "        return actions or [\"Execute task\"], observations or [\"Analysis completed\"]\n",
    "    \n",
    "    def _parse_task_list(self, response: str) -> List[str]:\n",
    "        \"\"\"Parse task list from response\"\"\"\n",
    "        lines = response.split('\\n')\n",
    "        tasks = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Look for numbered items\n",
    "            if re.match(r'^\\d+\\.', line):\n",
    "                task = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "                if task and len(task) > 10:\n",
    "                    tasks.append(task)\n",
    "        \n",
    "        return tasks\n",
    "    \n",
    "    def _parse_story_points(self, response: str) -> int:\n",
    "        \"\"\"Parse story points from response\"\"\"\n",
    "        fibonacci_scale = [1, 2, 3, 5, 8, 13, 21]\n",
    "        \n",
    "        # Look for \"Final Estimate: X\" or \"STORY POINTS: X\"\n",
    "        for pattern in [r'final estimate:\\s*(\\d+)', r'story points?:\\s*(\\d+)', r'estimate:\\s*(\\d+)']:\n",
    "            match = re.search(pattern, response.lower())\n",
    "            if match:\n",
    "                points = int(match.group(1))\n",
    "                return points if points in fibonacci_scale else min(fibonacci_scale, key=lambda x: abs(x - points))\n",
    "        \n",
    "        # Look for any Fibonacci number in the response\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        for num_str in reversed(numbers):\n",
    "            num = int(num_str)\n",
    "            if num in fibonacci_scale:\n",
    "                return num\n",
    "        \n",
    "        return 3  # Default moderate estimate\n",
    "    \n",
    "    def _parse_skills_list(self, response: str) -> List[str]:\n",
    "        \"\"\"Parse skills list from response\"\"\"\n",
    "        lines = response.split('\\n')\n",
    "        skills = []\n",
    "        in_skills_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect skills section\n",
    "            if 'SKILL IDENTIFICATION:' in line.upper():\n",
    "                in_skills_section = True\n",
    "                continue\n",
    "            \n",
    "            if in_skills_section and line.startswith('-'):\n",
    "                skill = line.lstrip('- ').strip()\n",
    "                if skill and len(skill) > 3:\n",
    "                    skills.append(skill)\n",
    "        \n",
    "        return self._normalize_skills(skills) if skills else [\"general_development\"]\n",
    "    \n",
    "    def _parse_dependencies(self, response: str, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse dependencies from response\"\"\"\n",
    "        dependencies = {}\n",
    "        lines = response.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"depends on\" in line.lower():\n",
    "                try:\n",
    "                    # Extract \"Task X depends on Task Y (rework_effort: N)\"\n",
    "                    match = re.search(r'task\\s+(\\d+)\\s+depends\\s+on\\s+task\\s+(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_idx = int(match.group(1)) - 1\n",
    "                        prerequisite_idx = int(match.group(2)) - 1\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        if 0 <= dependent_idx < len(tasks) and 0 <= prerequisite_idx < len(tasks):\n",
    "                            dependent_task = tasks[dependent_idx]\n",
    "                            prerequisite_task = tasks[prerequisite_idx]\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                'task_id': f\"T_{prerequisite_idx + 1:03d}\",\n",
    "                                'rework_effort': min(8, max(1, rework_effort))\n",
    "                            })\n",
    "                            \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "    \n",
    "    def _normalize_skills(self, skills: List[str]) -> List[str]:\n",
    "        \"\"\"Normalize skills to standard taxonomy\"\"\"\n",
    "        skill_taxonomy = {\n",
    "            'frontend_development': ['frontend', 'front-end', 'ui', 'user interface', 'react', 'angular', 'vue', 'javascript', 'html', 'css'],\n",
    "            'backend_development': ['backend', 'back-end', 'server', 'api', 'rest', 'microservices', 'node.js', 'python', 'java'],\n",
    "            'database_management': ['database', 'db', 'sql', 'nosql', 'mongodb', 'postgresql', 'mysql', 'data storage'],\n",
    "            'cloud_services': ['cloud', 'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'deployment'],\n",
    "            'testing': ['testing', 'unit test', 'integration test', 'qa', 'quality assurance'],\n",
    "            'security': ['security', 'authentication', 'authorization', 'encryption', 'oauth'],\n",
    "            'devops': ['devops', 'ci/cd', 'deployment', 'infrastructure', 'monitoring'],\n",
    "            'data_analysis': ['data analysis', 'analytics', 'reporting', 'visualization', 'etl'],\n",
    "            'project_management': ['project management', 'agile', 'scrum', 'planning', 'coordination'],\n",
    "            'communication': ['communication', 'documentation', 'stakeholder', 'requirements']\n",
    "        }\n",
    "        \n",
    "        normalized = []\n",
    "        seen = set()\n",
    "        \n",
    "        for skill in skills:\n",
    "            skill_lower = skill.lower().strip()\n",
    "            \n",
    "            # Find matching standard skill\n",
    "            mapped_skill = None\n",
    "            for standard_skill, variations in skill_taxonomy.items():\n",
    "                if any(var in skill_lower for var in variations):\n",
    "                    mapped_skill = standard_skill\n",
    "                    break\n",
    "            \n",
    "            final_skill = mapped_skill or skill_lower.replace(' ', '_')\n",
    "            if final_skill not in seen and len(final_skill) > 2:\n",
    "                normalized.append(final_skill)\n",
    "                seen.add(final_skill)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    async def _evaluator_score_trajectory(self, trajectory: Trajectory, agent_type: str) -> float:\n",
    "        \"\"\"Enhanced evaluator for different agent types\"\"\"\n",
    "        try:\n",
    "            base_score = 0.0\n",
    "            \n",
    "            if trajectory.final_output is None:\n",
    "                return 0.1\n",
    "            \n",
    "            # Agent-specific scoring\n",
    "            if agent_type == \"task_extractor\":\n",
    "                tasks = trajectory.final_output\n",
    "                if isinstance(tasks, list):\n",
    "                    # Score based on number and quality of tasks\n",
    "                    base_score += min(0.4, len(tasks) * 0.08)  # Up to 0.4 for 5+ tasks\n",
    "                    \n",
    "                    # Quality indicators\n",
    "                    total_length = sum(len(task) for task in tasks)\n",
    "                    base_score += min(0.3, total_length / 1000)  # Length quality\n",
    "                    \n",
    "                    # Check for action words\n",
    "                    action_words = ['implement', 'create', 'design', 'develop', 'build', 'configure']\n",
    "                    action_count = sum(1 for task in tasks for word in action_words if word.lower() in task.lower())\n",
    "                    base_score += min(0.2, action_count * 0.05)\n",
    "            \n",
    "            elif agent_type == \"story_estimator\":\n",
    "                if isinstance(trajectory.final_output, int):\n",
    "                    base_score += 0.6  # Got a valid number\n",
    "                    # Reasonable range bonus\n",
    "                    if 1 <= trajectory.final_output <= 21:\n",
    "                        base_score += 0.3\n",
    "            \n",
    "            elif agent_type == \"skills_mapper\":\n",
    "                skills = trajectory.final_output\n",
    "                if isinstance(skills, list):\n",
    "                    base_score += min(0.4, len(skills) * 0.1)  # Up to 0.4 for 4+ skills\n",
    "                    \n",
    "                    # Check for technical terms\n",
    "                    tech_terms = ['development', 'programming', 'database', 'api', 'framework']\n",
    "                    tech_count = sum(1 for skill in skills for term in tech_terms if term in skill.lower())\n",
    "                    base_score += min(0.3, tech_count * 0.1)\n",
    "            \n",
    "            elif agent_type == \"dependency_analyzer\":\n",
    "                deps = trajectory.final_output\n",
    "                if isinstance(deps, dict):\n",
    "                    base_score += min(0.4, len(deps) * 0.2)  # Dependencies found\n",
    "                    \n",
    "                    # Check for proper structure\n",
    "                    for task, dep_list in deps.items():\n",
    "                        if isinstance(dep_list, list):\n",
    "                            for dep in dep_list:\n",
    "                                if isinstance(dep, dict) and 'task_id' in dep and 'rework_effort' in dep:\n",
    "                                    base_score += 0.1\n",
    "            \n",
    "            # Reasoning quality bonus\n",
    "            if len(trajectory.observations) > 0:\n",
    "                base_score += 0.1\n",
    "            if len(trajectory.actions) > 0:\n",
    "                base_score += 0.1\n",
    "            \n",
    "            return min(1.0, base_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        Evaluation failed: {str(e)}\")\n",
    "            return 0.1\n",
    "    \n",
    "    async def _self_reflection_generate_feedback(self, trajectory: Trajectory, agent_type: str) -> str:\n",
    "        \"\"\"Generate improvement feedback\"\"\"\n",
    "        feedback_parts = []\n",
    "        \n",
    "        if trajectory.rework_score < 0.3:\n",
    "            feedback_parts.append(\"CRITICAL: Output quality is very low. Focus on structured, detailed responses.\")\n",
    "        \n",
    "        if agent_type == \"task_extractor\":\n",
    "            if not isinstance(trajectory.final_output, list) or len(trajectory.final_output) < 3:\n",
    "                feedback_parts.append(\"TASKS: Generate at least 4-5 specific, actionable tasks with clear deliverables.\")\n",
    "        \n",
    "        elif agent_type == \"story_estimator\":\n",
    "            if not isinstance(trajectory.final_output, int):\n",
    "                feedback_parts.append(\"ESTIMATION: Must provide a clear numeric estimate using Fibonacci scale.\")\n",
    "        \n",
    "        elif agent_type == \"skills_mapper\":\n",
    "            if not isinstance(trajectory.final_output, list) or len(trajectory.final_output) < 2:\n",
    "                feedback_parts.append(\"SKILLS: Identify 3-5 specific technical skills needed for implementation.\")\n",
    "        \n",
    "        elif agent_type == \"dependency_analyzer\":\n",
    "            if not isinstance(trajectory.final_output, dict):\n",
    "                feedback_parts.append(\"DEPENDENCIES: Use structured format 'Task X depends on Task Y (rework_effort: N)'.\")\n",
    "        \n",
    "        return \" \".join(feedback_parts) if feedback_parts else \"Continue improving output quality and structure.\"\n",
    "    \n",
    "    def _create_memory_context(self) -> str:\n",
    "        \"\"\"Create learning context from memory\"\"\"\n",
    "        if not self.memory.performance_history:\n",
    "            return \"No previous experience. Focus on clear, structured output.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        # Performance trend analysis\n",
    "        if len(self.memory.performance_history) >= 2:\n",
    "            recent_scores = self.memory.performance_history[-2:]\n",
    "            if recent_scores[-1] < recent_scores[-2]:\n",
    "                context_parts.append(\"Previous attempt declined in quality. Focus on improvement.\")\n",
    "        \n",
    "        # Best practices from successful trajectories\n",
    "        if self.memory.short_term_memory:\n",
    "            best_trajectory = max(self.memory.short_term_memory, key=lambda t: t.rework_score)\n",
    "            if best_trajectory.rework_score > 0.3:\n",
    "                context_parts.append(f\"Best practice: Follow structured format and provide detailed output.\")\n",
    "        \n",
    "        # Recent feedback\n",
    "        if self.memory.long_term_memory:\n",
    "            context_parts.append(f\"Previous feedback: {self.memory.long_term_memory[-1]}\")\n",
    "        \n",
    "        return \" \".join(context_parts) if context_parts else \"Focus on clear, structured output with specific details.\"\n",
    "\n",
    "\n",
    "# Pipeline Agents\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Extract tasks using Reflexion framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reflexion_framework = ImprovedReflexionFramework(max_episodes=3)\n",
    "        \n",
    "    async def decompose(self, user_story: str) -> List[str]:\n",
    "        print(f\"Extracting tasks: {user_story[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            result = await self.reflexion_framework.run_reflexion_loop(user_story, \"task_extractor\")\n",
    "            return result if result else []\n",
    "        except Exception as e:\n",
    "            print(f\"  Task extraction failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Estimate story points using Reflexion framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reflexion_framework = ImprovedReflexionFramework(max_episodes=2)\n",
    "\n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        task_points = {}\n",
    "        for task in tasks:\n",
    "            points = await self._estimate_single_task(task)\n",
    "            task_points[task] = points\n",
    "        \n",
    "        total_points = sum(task_points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': task_points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "        \n",
    "    async def _estimate_single_task(self, task: str) -> int:\n",
    "        try:\n",
    "            result = await self.reflexion_framework.run_reflexion_loop(task, \"story_estimator\")\n",
    "            return result if isinstance(result, int) else 3\n",
    "        except Exception as e:\n",
    "            print(f\"  Story point estimation failed: {str(e)}\")\n",
    "            return 3\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Map skills using Reflexion framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reflexion_framework = ImprovedReflexionFramework(max_episodes=2)\n",
    "        \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        try:\n",
    "            result = await self.reflexion_framework.run_reflexion_loop(task, \"skills_mapper\")\n",
    "            return result if isinstance(result, list) else [\"general_development\"]\n",
    "        except Exception as e:\n",
    "            print(f\"  Skill mapping failed: {str(e)}\")\n",
    "            return [\"general_development\"]\n",
    "    \n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Required method for evaluation system\"\"\"\n",
    "        skills_map = {}\n",
    "        for task in tasks:\n",
    "            skills = await self.map_skills(task)\n",
    "            skills_map[task] = skills\n",
    "        \n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        return skills_map\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Analyze dependencies using Reflexion framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reflexion_framework = ImprovedReflexionFramework(max_episodes=2)\n",
    "        \n",
    "    async def analyze_dependencies(self, tasks: List[str]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        if len(tasks) <= 1:\n",
    "            return {}\n",
    "        \n",
    "        print(\"Analyzing dependencies...\")\n",
    "        \n",
    "        try:\n",
    "            context = {\"tasks\": tasks}\n",
    "            result = await self.reflexion_framework.run_reflexion_loop(\"\", \"dependency_analyzer\", context)\n",
    "            return result if isinstance(result, dict) else {}\n",
    "        except Exception as e:\n",
    "            print(f\"  Dependency analysis failed: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Validation error: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the Reflexion-enhanced pipeline\"\"\"\n",
    "    print(f\"Processing: {user_story[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction\n",
    "        print(\"  Step 1: Extracting tasks with Reflexion...\")\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        print(f\"  Extracted {len(tasks)} tasks\")\n",
    "        \n",
    "        # Step 2 & 3: Parallel processing of Story Points and Skills\n",
    "        print(\"  Steps 2-3: Estimating story points and mapping skills with Reflexion...\")\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Process all tasks in parallel\n",
    "        story_points_tasks = [estimator._estimate_single_task(task) for task in tasks]\n",
    "        skills_tasks = [skills_agent.map_skills(task) for task in tasks]\n",
    "        \n",
    "        story_points_results, skills_results = await asyncio.gather(\n",
    "            asyncio.gather(*story_points_tasks),\n",
    "            asyncio.gather(*skills_tasks)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Dependency Analysis\n",
    "        print(\"  Step 4: Analyzing dependencies with Reflexion...\")\n",
    "        dependency_agent = DependencyAgent()\n",
    "        dependencies = await dependency_agent.analyze_dependencies(tasks)\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        print(\"  Step 5: Formatting and validating...\")\n",
    "        \n",
    "        # Build task data structure\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points_results)\n",
    "        \n",
    "        for i, (task, story_points, skills) in enumerate(zip(tasks, story_points_results, skills_results)):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                task_dependencies = dependencies[task]\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"story_points\": story_points,\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        # Final validation and formatting\n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        print(f\"  Completed: {len(tasks)} tasks, {total_story_points} total story points\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing user story: {str(e)}\")\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the Reflexion-enhanced pipeline\"\"\"\n",
    "    print(f\"Processing {len(user_stories)} user stories through Reflexion-enhanced pipeline...\")\n",
    "    \n",
    "    # Process stories sequentially to maintain Reflexion learning benefits\n",
    "    # (parallel processing would lose the learning context)\n",
    "    results = []\n",
    "    \n",
    "    for i, story in enumerate(user_stories, 1):\n",
    "        print(f\"\\n--- Processing Story {i}/{len(user_stories)} ---\")\n",
    "        try:\n",
    "            result = await process_user_story_pipeline(story)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"input\": story,\n",
    "                \"output\": {\n",
    "                    \"error\": str(e),\n",
    "                    \"story_points\": 0,\n",
    "                    \"tasks\": []\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_pipeline_output(results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Format pipeline results for display\"\"\"\n",
    "    output = []\n",
    "    \n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(\"REFLEXION-ENHANCED PIPELINE PROCESSING RESULTS\")\n",
    "    output.append(\"=\" * 80)\n",
    "    \n",
    "    total_story_points = 0\n",
    "    total_tasks = 0\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        output.append(f\"\\n--- USER STORY {i} ---\")\n",
    "        output.append(f\"Input: {result['input']}\")\n",
    "        \n",
    "        if 'error' in result['output']:\n",
    "            output.append(f\"ERROR: {result['output']['error']}\")\n",
    "            continue\n",
    "        \n",
    "        story_output = result['output']\n",
    "        output.append(f\"Total Story Points: {story_output['story_points']}\")\n",
    "        output.append(f\"Tasks: {len(story_output['tasks'])}\")\n",
    "        \n",
    "        total_story_points += story_output['story_points']\n",
    "        total_tasks += len(story_output['tasks'])\n",
    "        \n",
    "        for task in story_output['tasks']:\n",
    "            output.append(f\"\\n  Task {task['id']}: {task['description']}\")\n",
    "            output.append(f\"    Story Points: {task['story_points']}\")\n",
    "            output.append(f\"    Skills: {', '.join(task['required_skills'])}\")\n",
    "            \n",
    "            if task['depends_on']:\n",
    "                deps = [f\"{dep['task_id']} (rework: {dep['rework_effort']})\" for dep in task['depends_on']]\n",
    "                output.append(f\"    Dependencies: {', '.join(deps)}\")\n",
    "    \n",
    "    output.append(\"\\n\" + \"=\" * 80)\n",
    "    output.append(\"REFLEXION FRAMEWORK SUMMARY\")\n",
    "    output.append(\"=\" * 80)\n",
    "    output.append(f\"Total User Stories Processed: {len(results)}\")\n",
    "    output.append(f\"Total Tasks Generated: {total_tasks}\")\n",
    "    output.append(f\"Total Story Points: {total_story_points}\")\n",
    "    output.append(f\"Framework Benefits: Iterative improvement, learning from failures, self-reflection\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "async def run_pipeline(stories_list):\n",
    "    print(\"=== Reflexion-Enhanced Pipeline Task Decomposition System ===\")\n",
    "    print(\"Pipeline: Story Input â†’ Task Extractor â†’ Story Points & Skills â†’ Dependencies â†’ Validator â†’ Output\")\n",
    "    print(\"Enhanced with: Actor-Evaluator-Self-Reflection framework for iterative improvement\")\n",
    "    print(\"Features: Learning from failures, memory-based improvement, robust error handling\\n\")\n",
    "    results = await process_multiple_user_stories_pipeline(stories_list)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REFLEXION PIPELINE PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(format_pipeline_output(results))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"JSON OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(\"-\" * 40)\n",
    "    return results\n",
    "    \n",
    "results = asyncio.run(run_pipeline(user_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2857305-1166-4c99-a0a9-ff14953cad0a",
   "metadata": {},
   "source": [
    "- **Core Data Structures:**\n",
    "\n",
    "> **Trajectory (@dataclass):** Represents a trajectory (sequence of actions and observations) with input_state, actions list, observations list, final_output, rework_score (0.0-1.0), and episode_id for tracking learning episodes.\n",
    "\n",
    "> **ReflexionMemory (@dataclass):** Stores experiences and reflections for learning with short_term_memory (List[Trajectory]), long_term_memory (List[str] for feedback), and performance_history (List[float]) for tracking improvement.\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **ImprovedReflexionFramework:** Core Reflexion framework with configurable max_episodes (default 3), Actor-Evaluator-Self-Reflection loop, trajectory generation and scoring, memory-based learning, and early stopping mechanisms. Key method: run_reflexion_loop(task, agent_type, context) -> Any with comprehensive learning integration.\n",
    "\n",
    "> **TaskExtractorAgent:** Reflexion-enhanced task decomposition with ImprovedReflexionFramework integration (max_episodes=3), structured prompts with learning context, trajectory-based improvement, and fallback mechanisms. Key method: decompose(user_story) -> List[str] with iterative learning.\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Reflexion-enhanced story point estimation with framework integration (max_episodes=2), individual task processing, Fibonacci scale validation, and memory-based improvement. Key methods: estimate_story_points(user_story, tasks) -> Dict and _estimate_single_task(task) -> int.\n",
    "\n",
    "> **RequiredSkillsAgent:** Reflexion-enhanced skill mapping with framework integration (max_episodes=2), standardized skill taxonomy normalization, trajectory-based learning, and comprehensive skill categorization. Key methods: map_skills(task) -> List[str] and identify_skills(user_story, tasks) -> Dict[str, List[str]].\n",
    "\n",
    "> **DependencyAgent:** Reflexion-enhanced dependency analysis with framework integration (max_episodes=2), context-aware processing with task information, trajectory evaluation, and structured dependency parsing. Key method: analyze_dependencies(tasks) -> Dict with learning-based improvement.\n",
    "\n",
    "> **FormatValidator:** Output structure validation and formatting with comprehensive error handling, field validation, and graceful degradation. Key method: validate_and_format(user_story, tasks_data, total_story_points) -> Dict.\n",
    "\n",
    "- **Pipeline Functions:**\n",
    "\n",
    "> **process_user_story_pipeline(user_story):** Main single story processing function with Reflexion integration across all agents, parallel processing for story points and skills, comprehensive error handling, and structured output generation with learning benefits.\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories):** Sequential processing function preserving Reflexion learning context, individual exception management, and cumulative learning benefits across stories.\n",
    "\n",
    "> **format_pipeline_output(results):** Enhanced output formatting with Reflexion framework statistics, learning benefits summary, and comprehensive results reporting.\n",
    "\n",
    "> **run_pipeline(stories_list):** Reflexion-enhanced async wrapper with complete pipeline execution, learning-based improvement, and detailed progress reporting.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **run_reflexion_loop(task, agent_type, context):** Core Reflexion loop with Actor-Evaluator-Self-Reflection pattern, configurable episodes (1-3 per agent), trajectory generation and evaluation, memory-based learning, and early stopping for efficiency.\n",
    "\n",
    "> **_actor_generate_trajectory(task, agent_type, context):** Actor component generating responses with memory context integration, agent-specific prompt generation, reasoning parsing for actions/observations, and comprehensive error handling with trajectory creation.\n",
    "\n",
    "> **_evaluator_score_trajectory(trajectory, agent_type):** Enhanced evaluator with agent-specific scoring criteria, output quality assessment, reasoning quality bonuses, and normalized scoring (0.0-1.0) for consistent evaluation.\n",
    "\n",
    "> **_self_reflection_generate_feedback(trajectory, agent_type):** Self-reflection component generating improvement feedback based on trajectory performance, agent-specific recommendations, structured feedback generation, and learning-oriented suggestions.\n",
    "\n",
    "> **_create_memory_context():** Memory context creation from ReflexionMemory with performance trend analysis, best practice extraction from successful trajectories, recent feedback integration, and structured learning guidance.\n",
    "\n",
    "> **_actor_extract_tasks(user_story, memory_context):** Task extraction with Reflexion learning context, structured prompts with reasoning requirements, comprehensive task generation guidelines, and memory-informed improvement.\n",
    "\n",
    "> **_actor_estimate_story_points(task, memory_context):** Story point estimation with learning context, Fibonacci scale enforcement, complexity analysis framework, and memory-based improvement guidance.\n",
    "\n",
    "> **_actor_map_skills(task, memory_context):** Skill mapping with learning context, standardized skill taxonomy, technical skill focus, and memory-informed skill identification.\n",
    "\n",
    "> **_actor_analyze_dependencies(context, tasks, memory_context):** Dependency analysis with learning context, structured dependency format, logical relationship identification, and memory-based improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833176a-cf77-4210-b8e1-aeb5a2c5c42b",
   "metadata": {},
   "source": [
    "#### 4.4.9 Summary\n",
    "\n",
    "\n",
    "| Strategy | Tokens | Cost | Time | Quality | Best For |\n",
    "|----------|--------|------|------|---------|----------|\n",
    "| Zero-Shot | 3211 | 0.038 | Fastest | Basic | Simple tasks |\n",
    "| Few-Shot | 3434-6918 | 0.044-0.075 | Fast | Good | Most tasks |\n",
    "| Chain-of-Thought | 21438 | 0.270 | Slow | High | Complex reasoning |\n",
    "| Few-Shot-COT | 35976 | 0.536 | Very Slow | Highest | Most complex |\n",
    "| Tree-of-Thoughts | 14298 | 0.177 | Slow | High | Ambiguous problems |\n",
    "| Reflexion | 10000 | 0.120 | Slow | High | Quality control |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412d3fa-6aaf-4264-ad65-6b7a0f71417c",
   "metadata": {},
   "source": [
    "### 4.5 Prompt Strategies evaluation Across agents\n",
    "\n",
    "#### 4.5.1 LLM Evaluation Overview :\n",
    "\n",
    "LLM evaluations are quality checks that help make sure your AI product does what it's supposed to, whether that's writing code or handling support tickets. These evaluations serve as systematic assessments to ensure consistent performance and reliability across different use cases and prompt strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15dcb2a-a1bf-4b62-aafd-889cfebb75fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.5.2 How LLM Evaluations Work :\n",
    "The evaluation process follows a structured workflow:\n",
    "\n",
    "- **Inputs:** Test cases and scenarios are fed into the system\n",
    "- **LLM Application:** The model processes inputs using specific prompt strategies\n",
    "- **Outputs:** Generated responses from the model\n",
    "- **Reference Examples (Optional):** Ground truth or expected outputs for comparison\n",
    "- **Evaluator:** Assessment mechanism that analyzes model performance\n",
    "- **Score:** Quantitative or qualitative metrics indicating success\n",
    "\n",
    "This systematic approach allows for consistent measurement of prompt strategy effectiveness across different agents and use cases.\n",
    "\n",
    "![LLM Multi-Agents Pipeline](evalsworkmethodology.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f65a9-e247-4bc0-a958-0874e99a978b",
   "metadata": {},
   "source": [
    "#### 4.5.3 LLM Evaluation Methods :\n",
    "\n",
    "When evaluating LLM performance, we have two fundamental approaches depending on whether we have access to \"correct\" answers or not:!\n",
    "\n",
    "- **Reference-Based Evaluation :** compares model outputs against known correct answers or gold standard examples. This approach is ideal when you have labeled datasets or established ground truth.\n",
    "  \n",
    "- **Reference-Free Evaluation :** assesses output quality using intrinsic properties, constraints, or learned patterns without needing predefined correct answers. This is useful for creative tasks or when multiple valid outputs exist.\n",
    "\n",
    "![LLM Multi-Agents Pipeline](llmevalsmethods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff71fc4-782a-47e3-926e-4b2a4c1ae913",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.5.4 Choosing the Right Evaluation Method :\n",
    "\n",
    "The selection process follows a decision tree approach:\n",
    "\n",
    "![LLM Multi-Agents Pipeline](right_method.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2b993-269d-4d31-adab-7f0e8d9ad352",
   "metadata": {},
   "source": [
    "#### 4.5.5 Task Extractor Agent Evaluation Implementation :\n",
    "\n",
    "- **Evaluation Framework:**\n",
    "The task extractor agent evaluation combines semantic analysis, lexical overlap metrics (BLEU, ROUGE, METEOR), and statistical analysis of task counts and distributions\n",
    "\n",
    "> **Overlap-Based Metrics :** reflect how many shared symbols, words, or word sequences there are between the reference and the generated response.\n",
    "\n",
    "![LLM Multi-Agents Pipeline](overlap.png)\n",
    "\n",
    "> **Semantic Similarity Assessment :** help compare meaning instead of words. They use pre-trained models like BERT, many of which are open-source. These models turn text into vectors that capture the context and relationships between words. By comparing the distance between these vectors, you can get a sense of how similar the texts really are.\n",
    "\n",
    "![LLM Multi-Agents Pipeline](similarity.png)\n",
    "\n",
    ">**Task Count Accuracy :** track the number of the tasks the model predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12fb30-2adc-4649-a440-74c58b1cdf68",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9421ec-c175-4f2c-9e61-5c954de842fe",
   "metadata": {},
   "source": [
    "(See the code in the multi_evaluation file within the evaluation directory)\n",
    "- **Dependencies:**\n",
    "  \n",
    "> **numpy:** Numerical computing for statistical analysis and metric calculations\n",
    "\n",
    "> **sentence-transformers:** SentenceTransformer for semantic similarity analysis using embedding models\n",
    "\n",
    "> **sklearn.metrics.pairwise:** Cosine similarity computation for semantic evaluation\n",
    "\n",
    "> **collections.Counter:** Frequency counting for comparative analysis and statistics\n",
    "\n",
    "> **nltk:** Natural Language Toolkit for BLEU, METEOR scores and linguistic analysis\n",
    "\n",
    "> **warnings:** Warning suppression for cleaner output during evaluation\n",
    "\n",
    "> **importlib.util:** Dynamic module loading for technique registration and execution\n",
    "\n",
    "> **traceback:** Error tracking and debugging during technique evaluation\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TechniqueLoader:** Dynamic technique management system with technique registration, file-based loading, class detection, and error handling. Supports auto-registration from directories, interactive registration, and custom technique paths. Key methods: register_technique(name, file_path, class_name) and load_technique(name) with dynamic module loading.\n",
    "\n",
    "> **ReferenceBasedEvaluator:** Comprehensive evaluation framework using SentenceTransformer embeddings (all-MiniLM-L6-v2), semantic similarity analysis, overlap-based metrics (BLEU, ROUGE, METEOR), task statistics computation, and quality categorization. Key method: evaluate_single_technique(test_data, predicted_tasks, technique_name) -> Dict with detailed metrics.\n",
    "\n",
    "> **MultiTechniqueEvaluationPipeline:** Complete evaluation orchestrator with technique loading, test data management, parallel evaluation execution, interactive mode support, and comprehensive result generation. Key methods: run_evaluation(technique_names, output_dir, interactive) -> Dict and evaluate_multiple_techniques(technique_names, test_data) -> Tuple.\n",
    "\n",
    "- **Core Functions:**\n",
    "\n",
    "> **create_sample_testset():** Sample test data generation with new testset format including structured task objects with descriptions, IDs, story points, dependencies, and required skills for evaluation testing.\n",
    "\n",
    "> **main():** Command-line interface with argument parsing for testset paths, output directories, technique selection, interactive mode, custom registration, and comprehensive help system.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **register_technique(name: str, file_path: str, class_name: str):** Dynamic technique registration with file validation, path storage, and class name specification. Supports custom class names beyond TaskExtractorAgent for flexibility.\n",
    "\n",
    "> **load_technique(name: str):** Dynamic module loading using importlib.util with spec creation, module execution, class instantiation, and comprehensive error handling with detailed feedback.\n",
    "\n",
    "> **_extract_tasks_from_testset(test_item: Dict):** Flexible task extraction supporting new testset format with structured output containing task arrays with description fields, fallback to legacy formats, and error handling.\n",
    "\n",
    "> **evaluate_single_technique(test_data, predicted_tasks, technique_name):** Comprehensive evaluation with semantic similarity computation, overlap metrics analysis, task statistics calculation, and summary generation with detailed scoring.\n",
    "\n",
    "> **_compute_semantic_similarity(test_data, predicted_tasks):** Semantic analysis using SentenceTransformer embeddings, cosine similarity matrices, precision/recall-like scoring, F1-like calculation, and quality categorization (excellent/good/fair/poor).\n",
    "\n",
    "> **_compute_overlap_metrics(test_data, predicted_tasks):** Traditional NLP metrics including BLEU scores with smoothing, ROUGE-like word overlap, METEOR scoring, and Jaccard similarity with comprehensive statistical analysis.\n",
    "\n",
    "> **_compute_task_statistics(test_data, predicted_tasks):** Task-level analysis including count comparisons, ratio calculations, over/under-prediction detection, and statistical summaries with mean, std, min, max values.\n",
    "\n",
    "> **generate_predictions_for_technique(technique_name, test_data):** Technique execution with error handling, progress tracking, prediction generation, expected vs predicted comparison, and rate limiting (30-second delays).\n",
    "\n",
    "> **evaluate_multiple_techniques(technique_names, test_data):** Parallel technique evaluation with individual error isolation, comprehensive result aggregation, and comparison analysis across all techniques.\n",
    "\n",
    "- **Results :**\n",
    "![LLM Multi-Agents Pipeline](task_extractor_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32eb4e-9e92-43e1-8adf-6f0d54cfd68c",
   "metadata": {},
   "source": [
    "#### 4.5.6 Story point Estimation Agent Evaluation \n",
    "\n",
    "Story point estimation presents unique evaluation challenges because:\n",
    "\n",
    "- **Numerical Nature:** Story points are numbers, requiring regression-based evaluation\n",
    "- **Fibonacci Constraint:** Valid story points must follow the Fibonacci sequence\n",
    "- **Categorical Aspect:** Each Fibonacci number represents a distinct complexity category\n",
    "- **Relative Accuracy:** Close estimates (e.g., predicting 5 instead of 3) are better than distant ones\n",
    "\n",
    "Hence the evaluation employs :\n",
    "- **Mean Absolute Error (MAE):**\n",
    "  > **Definition:** Average absolute difference between predicted and actual story points\n",
    "\n",
    "  > **Formula:** MAE = (1/n) * Î£|predicted - actual|\n",
    "\n",
    "  > **Interpretation:** Lower values indicate better accuracy\n",
    "\n",
    "- **RÂ² (R-Squared):**\n",
    "  > **Definition:** Proportion of variance in story points explained by the model\n",
    "\n",
    "  > **Formula:** RÂ² = 1 - (SS_res / SS_tot)\n",
    "\n",
    "  > **Interpretation:** higher values indicate better prediction\n",
    "\n",
    "- **Perfect Match Percentage:**\n",
    "  > **Definition:** Percentage of predictions that are valid Fibonacci numbers\n",
    "\n",
    "  > **Formula:** (Valid Fibonacci Predictions / Total Predictions) Ã— 100\n",
    "\n",
    "  > **Interpretation:** higher values indicate better precision\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b3b24-0865-40ea-afed-9161a3504a75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Implementation \n",
    "(See the code in the storypointestimator_evaluation file within the evaluation directory)\n",
    "- **Dependencies:**\n",
    "  \n",
    "> **numpy:** Numerical computing for statistical analysis and metric calculations\n",
    "\n",
    "> **sklearn.metrics:** Machine learning metrics including mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "> **warnings:** Warning suppression for cleaner output during evaluation\n",
    "\n",
    "> **importlib.util:** Dynamic module loading for estimator registration and execution\n",
    "\n",
    "> **traceback:** Error tracking and debugging during estimator evaluation\n",
    "\n",
    "> **pandas:** Data analysis and CSV export for detailed results\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TaskStoryPointEvaluator:** Comprehensive evaluation framework for story point estimation at task level with Fibonacci sequence validation [1,2,3,5,8,13,21,34,55,89], individual task extraction from testsets, accuracy categorization, and detailed performance metrics. Key methods: extract_tasks_from_testset(testset_path) and evaluate_estimator_on_tasks(estimator, estimator_name, tasks).\n",
    "\n",
    "> **EstimatorLoader:** Dynamic estimator management system with automatic registration from techniques directory, file-based loading, class detection for StoryPointEstimatorAgent, and error handling. Key methods: register_estimator(name, file_path, class_name) and load_estimator(name) with dynamic module loading.\n",
    "\n",
    "- **Core Functions:**\n",
    "\n",
    "> **evaluate_all_techniques():** Main evaluation orchestrator processing all available estimators with comprehensive comparison, individual technique evaluation, result aggregation, and combined analysis reporting.\n",
    "\n",
    "> **print_comparison_summary(all_results):** Multi-technique comparison with performance ranking, best performer identification, and comprehensive metric comparison across MAE, RÂ², perfect predictions, and Fibonacci compliance.\n",
    "\n",
    "> **save_combined_results(all_results):** Result aggregation and export with JSON serialization, CSV comparison tables, and structured file organization with timestamp-based naming.\n",
    "\n",
    "> **main():** Command-line interface with argument parsing for single estimator evaluation, file specification, listing mode, and comprehensive help system.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **extract_tasks_from_testset(testset_path: str):** Task extraction from structured testsets with support for new format including task descriptions, IDs, story points, required skills, and dependencies. Returns individual task objects for evaluation.\n",
    "\n",
    "> **evaluate_estimator_on_tasks(estimator, estimator_name, tasks):** Comprehensive individual task evaluation with estimator method detection (_estimate_single_task vs estimate_story_points), error handling with fallback defaults, rate limiting (2-15 second delays), and detailed progress tracking.\n",
    "\n",
    "> **_calculate_metrics(predictions, actual_values, detailed_results, estimator_name):** Statistical analysis with mean absolute error (MAE), root mean squared error (RMSE), RÂ² coefficient calculation, relative error analysis, accuracy categorization, and Fibonacci compliance checking.\n",
    "\n",
    "> **_categorize_accuracy(relative_error: float):** Accuracy classification system with perfect (0% error), excellent (â‰¤10% error), good (â‰¤20% error), acceptable (â‰¤50% error), and poor (>50% error) categories.\n",
    "\n",
    "> **print_evaluation_summary(results):** Comprehensive result presentation with overall performance metrics, accuracy breakdown percentages, Fibonacci compliance analysis, summary statistics, and performance-by-story-point-value tables.\n",
    "\n",
    "> **save_detailed_results(results, output_dir):** Result persistence with JSON export for complete results, CSV export for detailed analysis, and structured file organization with estimator-specific naming.\n",
    "\n",
    "> **auto_register_estimators():** Automatic estimator discovery with directory scanning for Python files, content analysis for StoryPointEstimatorAgent class detection, and automatic registration with error handling.\n",
    "\n",
    "- **Results :**\n",
    "  \n",
    "![LLM Multi-Agents Pipeline](story_point_estimation_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96093fc5-b6a3-4f48-bb94-d3583cb0754c",
   "metadata": {},
   "source": [
    "#### 4.5.7 Required Skills agent evaluation \n",
    "Skills evaluation presents unique challenges because:\n",
    "\n",
    "- **Set-Based Nature:** Skills are typically sets of items, not single values\n",
    "- **Semantic Relationships:** Similar skills may be expressed differently (e.g., \"JavaScript\" vs \"JS\")\n",
    "- **Partial Matches Matter:** Identifying some correct skills is better than none\n",
    "\n",
    "Hence the evaluation imploys beside of semantic and overlap metrics explained above :\n",
    "- **Precision:**\n",
    "  > **Definition:** Proportion of predicted skills that are actually correct\n",
    "\n",
    "  > **Formula:** precision = len(intersection) / len(predicted_skills_set)\n",
    "\n",
    "  > **Interpretation:** higher precision indicates few irrelevant skills predicted\n",
    "\n",
    " - **Recall:**\n",
    "\n",
    "  > **Definition:** Proportion of actual required skills that were correctly identified \n",
    "\n",
    "  > **Formula:** recall = len(intersection) / len(actual_skills)\n",
    "\n",
    "  > **Interpretation:** higher recall indicates few required skills missed\n",
    "\n",
    " - **F1 Score:**\n",
    "   \n",
    "  > **Definition:** Harmonic mean of precision and recall, providing a balanced measure of overall performance \n",
    "\n",
    "  > **Formula:** f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "  > **Interpretation:** reflects performance\n",
    "\n",
    " - **Coverage:**\n",
    "   \n",
    "  > **Definition:** Proportion of test cases where at least one skill was correctly identified \n",
    "\n",
    "  > **Formula:** Coverage = (Tasks with â‰¥1 Correct Skill) / Total Tasks\n",
    "\n",
    "  > **Interpretation:** higher coverage shows practical success rate of the agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171405d-44cf-4687-919d-557c28fc8e43",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "(See the code in the required_skill_agent_evaluation file within the evaluation directory)\n",
    "- **Dependencies:**\n",
    "  \n",
    "> **numpy:** Numerical computing for statistical analysis and metric calculations\n",
    "\n",
    "> **sentence-transformers:** SentenceTransformer for semantic similarity analysis using embedding models\n",
    "\n",
    "> **sklearn.metrics.pairwise:** Cosine similarity computation for semantic evaluation\n",
    "\n",
    "> **sklearn.metrics:** Machine learning metrics including jaccard_score, f1_score, precision_score, recall_score\n",
    "\n",
    "> **collections.Counter:** Frequency counting for skill analysis and distribution statistics\n",
    "\n",
    "> **nltk:** Natural Language Toolkit for BLEU, METEOR scores and linguistic analysis\n",
    "\n",
    "> **warnings:** Warning suppression for cleaner output during evaluation\n",
    "\n",
    "> **importlib.util:** Dynamic module loading for agent registration and execution\n",
    "\n",
    "> **traceback:** Error tracking and debugging during agent evaluation\n",
    "\n",
    "> **pandas:** Data analysis and CSV export for detailed results\n",
    "- **Main Classes:**\n",
    "\n",
    "> **TaskSkillsEvaluator:** Comprehensive evaluation framework for required skills identification at task level with SentenceTransformer embedding model (all-MiniLM-L6-v2), skill category analysis, set-based metrics (F1, precision, recall, Jaccard), and detailed performance assessment. Key methods: extract_tasks_from_testset(testset_path) and evaluate_agent_on_tasks(agent, agent_name, tasks).\n",
    "\n",
    "> **SkillsAgentLoader:** Dynamic agent management system with automatic registration from techniques directory, file-based loading, class detection for RequiredSkillsAgent and map_skills methods, and error handling. Key methods: register_agent(name, file_path, class_name) and load_agent(name) with dynamic module loading.\n",
    "\n",
    "- **Core Functions:**\n",
    "\n",
    "> **evaluate_all_agents():** Main evaluation orchestrator processing all available skills agents with comprehensive comparison, individual agent evaluation, result aggregation, and combined analysis reporting.\n",
    "\n",
    "> **print_comparison_summary(all_results):** Multi-agent comparison with performance ranking by F1 score, best performer identification across F1, precision, recall, coverage, and exact matches.\n",
    "\n",
    "> **save_combined_results(all_results):** Result aggregation and export with JSON serialization, CSV comparison tables, and structured file organization with timestamp-based naming.\n",
    "\n",
    "> **main():** Command-line interface with argument parsing for single agent evaluation, file specification, task limiting, listing mode, and comprehensive help system.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **extract_tasks_from_testset(testset_path: str):** Task extraction from structured testsets with support for new format including task descriptions, IDs, required skills, story points, and dependencies. Returns individual task objects for skills evaluation.\n",
    "\n",
    "> **evaluate_agent_on_tasks(agent, agent_name, tasks):** Comprehensive individual task evaluation with set-based skill comparison, F1/precision/recall calculation, exact match detection, error handling with empty prediction fallbacks, and rate limiting (2-15 second delays).\n",
    "\n",
    "> **_calculate_metrics(predictions, actual_values, detailed_results, agent_name):** Statistical analysis with F1 score calculation, precision/recall computation, Jaccard similarity measurement, exact match rate, accuracy categorization, skill-level analysis, and semantic similarity computation.\n",
    "\n",
    "> **_analyze_category_performance(detailed_results):** Category-specific performance analysis with predefined skill categories (frontend, backend, devops, security, data, mobile, design), category-filtered metrics, and performance distribution assessment.\n",
    "\n",
    "> **_categorize_accuracy(f1_score: float):** Accuracy classification system with excellent (â‰¥0.9), good (â‰¥0.7), acceptable (â‰¥0.5), poor (â‰¥0.3), and very_poor (<0.3) categories based on F1 scores.\n",
    "\n",
    "> **print_evaluation_summary(results):** Comprehensive result presentation with overall performance metrics, accuracy breakdown percentages, skill analysis with coverage/precision rates, overlap metrics (BLEU/ROUGE/METEOR), semantic similarity, and category performance ranking.\n",
    "\n",
    "> **save_detailed_results(results, output_dir):** Result persistence with JSON export for complete results, CSV export for detailed analysis, and structured file organization with agent-specific naming.\n",
    "\n",
    "> **auto_register_agents():** Automatic agent discovery with directory scanning for Python files, content analysis for RequiredSkillsAgent class or map_skills method detection, and automatic registration with error handling.\n",
    "\n",
    "- **Results:**\n",
    "  \n",
    "![LLM Multi-Agents Pipeline](required_skills_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d3113-76b5-4b1a-ad5c-4326e9393400",
   "metadata": {},
   "source": [
    "#### 4.5.8 Dependency Detection Agent Evaluation\n",
    "\n",
    "Dependency detection presents unique challenges because:\n",
    "- **Graph-Based Nature:** Dependencies form directed relationships between tasks, creating complex graph structures\n",
    "- **Sequential Relationships:** Task order and prerequisite chains are critical for project execution\n",
    "- **Partial Correctness:** Identifying some dependencies is better than missing all relationships\n",
    "\n",
    "Hence the evaluation employs beside of semantic, overlap metrics, f1 score and coverage explained above:\n",
    "\n",
    "- **Accuracy:**\n",
    " > **Definition:** Proportion of all dependency predictions (positive and negative) that are correct\n",
    "\n",
    " > **Formula:** accuracy = correct_dependencies / (total_expected_dependencies + false_dependencies)\n",
    "\n",
    " > **Interpretation:** Higher accuracy shows overall correctness of dependency identification\n",
    "\n",
    "- **Detection Rate:**\n",
    " > **Definition:** Ratio of predicted dependencies to expected dependencies\n",
    "\n",
    " > **Formula:** detection_rate = total_predicted_dependencies / total_expected_dependencies\n",
    "\n",
    " > **Interpretation:** Values >1 indicate over-prediction, values <1 indicate under-prediction\n",
    "\n",
    "- **Perfect Match Rate:**\n",
    " > **Definition:** Percentage of user stories where dependency graph is perfectly predicted\n",
    "\n",
    " > **Formula:** perfect_match_rate = stories_with_perfect_dependencies / total_stories_with_dependencies\n",
    "\n",
    " > **Interpretation:** Higher rate shows agent's ability to capture complete dependency structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73a088-e3d3-43d6-8596-a21cf0170717",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "(See the code in the dependencies_detection_evaluation file within the evaluation directory)\n",
    "\n",
    "- **Dependencies:**\n",
    "\n",
    "> **numpy:** Numerical computing for statistical analysis and metric calculations\n",
    "\n",
    "> **collections.Counter, defaultdict:** Data structures for frequency counting and dependency analysis\n",
    "\n",
    "> **warnings:** Warning suppression for cleaner output during evaluation\n",
    "\n",
    "> **importlib.util:** Dynamic module loading for agent registration and execution\n",
    "\n",
    "> **traceback:** Error tracking and debugging during agent evaluation\n",
    "\n",
    "> **inspect:** Method signature inspection for adaptive agent interface handling\n",
    "\n",
    "- **Main Classes:**\n",
    "\n",
    "> **DependencyEvaluator:** Comprehensive evaluation framework for dependency detection with task extraction from testsets, dependency normalization for comparison, accuracy computation with precision/recall/F1 metrics, and detailed statistical analysis. Key methods: evaluate_dependency_detection(test_data, predictions, agent_name) and _normalize_dependencies(dependencies, task_descriptions).\n",
    "\n",
    "> **DependencyDetectionPipeline:** Complete evaluation orchestrator with automatic agent discovery, test data loading, multi-interface agent support, and comprehensive result management. Key methods: run_evaluation(technique_name, output_dir, limit_stories) and evaluate_all_agents(test_data).\n",
    "\n",
    "> **MockDependencyAgent:** Testing utility providing mock dependency analysis compatible with multiple interfaces, sequential dependency generation, and configurable rework effort for evaluation framework testing.\n",
    "\n",
    "- **Core Functions:**\n",
    "\n",
    "> **main():** Command-line interface with argument parsing for technique selection, output configuration, story limiting, agent listing, and mock testing mode with comprehensive help system.\n",
    "\n",
    "- **Important Methods:**\n",
    "\n",
    "> **_extract_tasks_from_testset(test_item: Dict):** Comprehensive task extraction from structured testsets including complete task objects, task descriptions, dependency mappings, dependency graphs, and task ID to description mappings with error handling for malformed data.\n",
    "\n",
    "> **_normalize_dependencies(dependencies, task_descriptions):** Dependency normalization for comparison with closest task matching, prerequisite set creation, and structured dependency mapping enabling consistent evaluation across different agent output formats.\n",
    "\n",
    "> **_find_closest_task_match(target: str, task_list: List[str]):** Intelligent task matching with exact match priority, partial substring matching, and keyword-based scoring for robust task identification across varied description formats.\n",
    "\n",
    "> **evaluate_dependency_detection(test_data, predictions, agent_name):** Comprehensive evaluation orchestrator computing dependency accuracy, detection statistics, precision/recall metrics, and summary analysis with detailed result aggregation.\n",
    "\n",
    "> **_compute_dependency_accuracy(test_data, predictions):** Statistical analysis with true positive/false positive/false negative calculation, overall precision/recall/F1 computation, accuracy measurement, and detailed per-story comparison tracking.\n",
    "\n",
    "> **_compute_detection_statistics(test_data, predictions):** General detection analysis including story coverage, dependency distribution, rework effort statistics, and detection pattern analysis with comprehensive statistical summaries.\n",
    "\n",
    "> **_compute_precision_recall(test_data, predictions):** Detailed precision/recall computation with perfect/partial/no match categorization, dependency pair analysis, and statistical distribution measurement across all test cases.\n",
    "\n",
    "> **generate_dependency_predictions(dependency_agent, test_data, agent_name):** Agent execution with adaptive interface detection, multiple calling pattern support, comprehensive error handling, and prediction aggregation with progress tracking.\n",
    "\n",
    "> **_discover_agents():** Automatic agent discovery with file system scanning for pipeline files (paste.py, paste-2.py, paste-3.py), dynamic module loading, and DependencyAgent class detection with error handling.\n",
    "\n",
    "- **Results :**\n",
    "\n",
    "![LLM Multi-Agents Pipeline](dependency_detection_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66cc77-7ef3-4cec-8100-7cb21d265f79",
   "metadata": {},
   "source": [
    "#### 4.6 Optimal Prompting Strategy Configuration\n",
    "Based on comprehensive evaluation results, the optimized prompting techniques for each agent:\n",
    "\n",
    "| Agent | Optimal Technique | Justification |\n",
    "|-------|------------------|---------------|\n",
    "| **Task Extractor** | Self-Consistency | Best semantic similarity performance |\n",
    "| **Story Point Estimator** | Few-Shot | Lowest MAE and highest Fibonacci compliance |\n",
    "| **Required Skills** | Zero-Shot | Optimal performance/efficiency balance with lower token usage |\n",
    "| **Dependency Detection** | Few-Shot | Superior F1 score and perfect match percentage |+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66f58a-b95d-4c75-8ba4-65ea969dd5eb",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037667b2-945a-4de8-8396-436b3b8c0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”” Interactive environment detected!\n",
      "ðŸ’¡ Use one of these methods:\n",
      "1. await process_stories_interactive(['Your user story 1', 'Your user story 2'])\n",
      "2. results = await process_multiple_user_stories_pipeline(['Your stories'])\n",
      "3. graph = create_graph_from_json(results)  # If you have existing results\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Set, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except:\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        self.token_usage = {\n",
    "            'task_extraction': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'story_point_estimation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'required_skills': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'dependency_analysis': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'format_validation': {'input': 0, 'output': 0, 'total': 0},\n",
    "            'total_consumed': 0\n",
    "        }\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer:\n",
    "            return len(self.tokenizer.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def track_api_call(self, category: str, input_text: str, output_text: str):\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        self.token_usage[category]['input'] += input_tokens\n",
    "        self.token_usage[category]['output'] += output_tokens\n",
    "        self.token_usage[category]['total'] += total_tokens\n",
    "        self.token_usage['total_consumed'] += total_tokens\n",
    "        \n",
    "        print(f\"[{category.upper()}] Tokens - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'breakdown': self.token_usage,\n",
    "            'cost_estimate': self.estimate_cost(),\n",
    "            'efficiency_metrics': self.calculate_efficiency()\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> Dict[str, float]:\n",
    "        input_rate = 0.00001\n",
    "        output_rate = 0.00002\n",
    "        \n",
    "        total_input = sum(cat['input'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        total_output = sum(cat['output'] for cat in self.token_usage.values() if isinstance(cat, dict))\n",
    "        \n",
    "        input_cost = total_input * input_rate\n",
    "        output_cost = total_output * output_rate\n",
    "        \n",
    "        return {\n",
    "            'input_cost': input_cost,\n",
    "            'output_cost': output_cost,\n",
    "            'total_cost': input_cost + output_cost\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency(self) -> Dict[str, Any]:\n",
    "        total_tokens = self.token_usage['total_consumed']\n",
    "        if total_tokens == 0:\n",
    "            return {'efficiency': 'No data'}\n",
    "        \n",
    "        categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "        \n",
    "        return {\n",
    "            'tokens_per_category': {\n",
    "                cat: self.token_usage[cat]['total'] \n",
    "                for cat in categories\n",
    "            },\n",
    "            'percentage_breakdown': {\n",
    "                cat: (self.token_usage[cat]['total'] / total_tokens) * 100 \n",
    "                for cat in categories\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Global token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "\n",
    "class TaskExtractorAgent:\n",
    "    \"\"\"Enhanced Task Extractor with better context preservation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.few_shot_examples = \"\"\"\n",
    "User Story: As a user researcher, I want to make sure the correct NSF people are invited to user interviews, so that they can observe the interviews and make recommendations accordingly.\n",
    "\n",
    "Tasks:\n",
    "1. Identify relevant NSF stakeholders for each interview type\n",
    "2. Create interview observation guidelines\n",
    "3. Schedule stakeholder availability coordination\n",
    "4. Prepare observation materials and templates\n",
    "5. Brief observers on interview protocols\n",
    "\n",
    "User Story: As a user, I want to click on the address so that it takes me to a new tab with Google Maps.\n",
    "\n",
    "Tasks:\n",
    "1. Create clickable address styling\n",
    "2. Implement click handler for address data\n",
    "3. Format address for Google Maps URL\n",
    "4. Configure new tab opening functionality\n",
    "5. Add error handling for invalid addresses\n",
    "\n",
    "User Story: As an admin, I want to manage inventory so that I can track stock levels.\n",
    "\n",
    "Tasks:\n",
    "1. Design inventory management interface\n",
    "2. Implement inventory CRUD operations\n",
    "3. Create stock level tracking system\n",
    "4. Build inventory history logging\n",
    "5. Set up low stock alert notifications\n",
    "\"\"\"\n",
    "    \n",
    "    async def decompose(self, user_story: str, num_samples: int = 3) -> List[str]:\n",
    "        \"\"\"Extract tasks using self-consistency across multiple samples\"\"\"\n",
    "        all_task_samples = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            temperature = 0.2 + (i * 0.2)  # 0.2, 0.4, 0.6\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "You are an expert at breaking down user stories into specific, actionable tasks.\n",
    "Each task should be atomic, testable, and focused on a single responsibility.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Generate MAXIMUM 8 tasks per user story (fewer is better)\n",
    "- Each task must be CONCISE and ATOMIC (one clear action)\n",
    "- ALWAYS start with imperative verbs (Create, Implement, Design, Test, Build, Add, Update, etc.)\n",
    "- NEVER use explanatory language or user perspectives\n",
    "- NO tasks starting with \"The user...\", \"They need...\", \"Be displayed...\", \"The system should...\"\n",
    "- Focus on CONCRETE ACTIONS only\n",
    "\n",
    "REQUIRED FORMAT: [VERB] + [OBJECT/ACTION]\n",
    "\n",
    "GOOD task examples:\n",
    "âœ“ \"Create search interface\"\n",
    "âœ“ \"Implement search algorithm\" \n",
    "âœ“ \"Design results display\"\n",
    "âœ“ \"Test search functionality\"\n",
    "âœ“ \"Update inventory levels\"\n",
    "âœ“ \"Track inventory history\"\n",
    "âœ“ \"Set notification alerts\"\n",
    "\n",
    "BAD task examples (NEVER generate these):\n",
    "âœ— \"The user wants to search for restaurants\"\n",
    "âœ— \"They need to be able to update inventory\"\n",
    "âœ— \"Be displayed in a user-friendly format\"\n",
    "âœ— \"The user may want to filter results\"\n",
    "âœ— \"The system should validate input\"\n",
    "âœ— \"This implies that location detection is needed\"\n",
    "\n",
    "Examples:\n",
    "{self.few_shot_examples}\n",
    "\n",
    "User Story: {user_story}\n",
    "\n",
    "Tasks:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=min(temperature, 1.0)\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content.strip()\n",
    "                token_tracker.track_api_call('task_extraction', prompt, content)\n",
    "                \n",
    "                tasks = self._parse_tasks(content)\n",
    "                if tasks:\n",
    "                    all_task_samples.append(tasks)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Task extraction sample {i+1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_task_samples:\n",
    "            return []\n",
    "        \n",
    "        # Apply self-consistency and limit to 8 tasks\n",
    "        final_tasks = self._apply_consistency(all_task_samples)\n",
    "        \n",
    "        # Limit to maximum 8 tasks and clean descriptions\n",
    "        if len(final_tasks) > 8:\n",
    "            # Keep the most frequent/important tasks\n",
    "            final_tasks = final_tasks[:8]\n",
    "        \n",
    "        # Clean task descriptions and filter out empty ones\n",
    "        cleaned_tasks = []\n",
    "        for task in final_tasks:\n",
    "            cleaned_task = self._clean_task_description(task)\n",
    "            if cleaned_task and len(cleaned_task.strip()) > 0:  # Only keep non-empty tasks\n",
    "                cleaned_tasks.append(cleaned_task)\n",
    "        \n",
    "        # Ensure we still have tasks after cleaning\n",
    "        if not cleaned_tasks:\n",
    "            # Fallback: keep original tasks if all were filtered out\n",
    "            cleaned_tasks = final_tasks[:5]  # At least keep some basic tasks\n",
    "        \n",
    "        print(f\"âœ“ Extracted {len(cleaned_tasks)} tasks using self-consistency\")\n",
    "        return cleaned_tasks\n",
    "    \n",
    "    def _clean_task_description(self, task: str) -> str:\n",
    "        \"\"\"Clean and format task descriptions\"\"\"\n",
    "        task = task.strip()\n",
    "        \n",
    "        # Remove common problematic patterns that aren't real tasks\n",
    "        problematic_patterns = [\n",
    "            r'^the\\s+user\\s+wants?\\s+to\\s+.+',      # \"The user wants to...\"\n",
    "            r'^the\\s+user\\s+may\\s+want\\s+to\\s+.+',  # \"The user may want to...\"\n",
    "            r'^they\\s+need\\s+to\\s+be\\s+able\\s+to\\s+.+', # \"They need to be able to...\"\n",
    "            r'^they\\s+need\\s+to\\s+.+',              # \"They need to...\"\n",
    "            r'^be\\s+displayed\\s+.+',                # \"Be displayed...\"\n",
    "            r'^this\\s+implies?\\s+that\\s+.+',        # \"This implies that...\"\n",
    "            r'^this\\s+means?\\s+that\\s+.+',          # \"This means that...\"\n",
    "            r'^this\\s+involves?\\s+.+',              # \"This involves...\"\n",
    "            r'^this\\s+includes?\\s+.+',              # \"This includes...\"\n",
    "            r'^this\\s+requires?\\s+.+',              # \"This requires...\"\n",
    "            r'^the\\s+\\w+\\s+needs?\\s+to\\s+.+',       # \"The developer needs to...\"\n",
    "            r'^the\\s+system\\s+should\\s+.+',         # \"The system should...\"\n",
    "            r'^it\\s+is\\s+important\\s+to\\s+.+',      # \"It is important to...\"\n",
    "            r'^we\\s+need\\s+to\\s+ensure\\s+.+',       # \"We need to ensure...\"\n",
    "            r'^we\\s+should\\s+make\\s+sure\\s+.+',     # \"We should make sure...\"\n",
    "        ]\n",
    "        \n",
    "        # Check if this is a problematic pattern that should be converted\n",
    "        for pattern in problematic_patterns:\n",
    "            if re.match(pattern, task, re.IGNORECASE):\n",
    "                # Try to extract the core action from these patterns\n",
    "                task = self._extract_core_action(task)\n",
    "                break\n",
    "        \n",
    "        # Remove common prefixes that make tasks verbose\n",
    "        prefixes_to_remove = [\n",
    "            r'^they\\s+need\\s+to\\s+be\\s+able\\s+to\\s+',  # \"They need to be able to\"\n",
    "            r'^they\\s+need\\s+to\\s+',                   # \"They need to\"\n",
    "            r'^the\\s+user\\s+may\\s+want\\s+to\\s+',       # \"The user may want to\"\n",
    "            r'^the\\s+user\\s+wants?\\s+to\\s+',           # \"The user wants to\"\n",
    "            r'^be\\s+displayed\\s+',                     # \"Be displayed\"\n",
    "            r'^we\\s+need\\s+to\\s+',\n",
    "            r'^we\\s+should\\s+',\n",
    "            r'^we\\s+may\\s+also\\s+want\\s+to\\s+',\n",
    "            r'^we\\s+could\\s+',\n",
    "            r'^this\\s+involves\\s+',\n",
    "            r'^this\\s+includes\\s+',\n",
    "            r'^this\\s+requires\\s+',\n",
    "            r'^the\\s+developer\\s+needs?\\s+to\\s+',\n",
    "            r'^the\\s+developer\\s+should\\s+',\n",
    "            r'^the\\s+system\\s+should\\s+',\n",
    "            r'^it\\s+is\\s+important\\s+to\\s+',\n",
    "            r'^it\\s+would\\s+be\\s+good\\s+to\\s+',\n",
    "            r'^we\\s+might\\s+want\\s+to\\s+',\n",
    "            r'^we\\s+can\\s+',\n",
    "            r'^to\\s+do\\s+this,?\\s+',\n",
    "            r'^for\\s+this,?\\s+',\n",
    "            r'^in\\s+order\\s+to\\s+.*?,\\s*',\n",
    "        ]\n",
    "        \n",
    "        for prefix in prefixes_to_remove:\n",
    "            task = re.sub(prefix, '', task, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Skip tasks that are just explanations or assumptions\n",
    "        skip_patterns = [\n",
    "            r'^this\\s+is\\s+',\n",
    "            r'^this\\s+implies\\s+',\n",
    "            r'^the\\s+\\w+\\s+wants\\s+',\n",
    "            r'^assumption\\s*:',\n",
    "            r'^note\\s*:',\n",
    "        ]\n",
    "        \n",
    "        for pattern in skip_patterns:\n",
    "            if re.match(pattern, task, re.IGNORECASE):\n",
    "                return \"\"  # Return empty string to skip this task\n",
    "        \n",
    "        # Ensure task starts with imperative verb\n",
    "        if task and not self._starts_with_imperative(task):\n",
    "            # Try to convert to imperative form\n",
    "            task = self._convert_to_imperative(task)\n",
    "        \n",
    "        # Capitalize first letter\n",
    "        if task:\n",
    "            task = task[0].upper() + task[1:] if len(task) > 1 else task.upper()\n",
    "        \n",
    "        # Remove trailing periods\n",
    "        task = task.rstrip('.')\n",
    "        \n",
    "        return task\n",
    "    \n",
    "    def _extract_core_action(self, task: str) -> str:\n",
    "        \"\"\"Extract core actionable task from explanatory sentences\"\"\"\n",
    "        # Patterns to extract actionable tasks from explanatory text\n",
    "        extraction_patterns = [\n",
    "            # \"They need to be able to update inventory levels\" -> \"Update inventory levels\"\n",
    "            (r'.*need\\s+to\\s+be\\s+able\\s+to\\s+(.+)', r'Update \\1'),\n",
    "            (r'.*need\\s+to\\s+(.+)', r'Implement \\1'),\n",
    "            \n",
    "            # \"The user may want to filter or sort the results\" -> \"Implement filtering and sorting\"\n",
    "            (r'.*user\\s+may\\s+want\\s+to\\s+filter\\s+or\\s+sort.*', 'Implement filtering and sorting'),\n",
    "            (r'.*user\\s+may\\s+want\\s+to\\s+(.+)', r'Implement \\1'),\n",
    "            (r'.*user\\s+wants?\\s+to\\s+(.+)', r'Implement \\1'),\n",
    "            \n",
    "            # \"Be displayed in a user-friendly format\" -> \"Display results in user-friendly format\"\n",
    "            (r'^be\\s+displayed\\s+(.+)', r'Display results \\1'),\n",
    "            \n",
    "            # \"They need to be able to track inventory history\" -> \"Track inventory history\"\n",
    "            (r'.*track\\s+inventory\\s+history.*', 'Track inventory history'),\n",
    "            (r'.*set\\s+alerts\\s+or\\s+notifications.*', 'Set alerts and notifications'),\n",
    "            \n",
    "            # Location detection patterns\n",
    "            (r'.*location\\s+needs?\\s+to\\s+be\\s+detected.*', 'Detect user location'),\n",
    "            (r'.*location\\s+.*detected.*', 'Detect user location'),\n",
    "            (r'.*geolocation.*', 'Implement geolocation'),\n",
    "            \n",
    "            # Search and interface patterns\n",
    "            (r'.*user\\s+interface\\s+component.*', 'Develop user interface component'),\n",
    "            (r'.*search\\s+results.*interface.*', 'Develop search results interface'),\n",
    "            (r'.*display.*results.*', 'Display search results'),\n",
    "            \n",
    "            # Skip already implemented functionality\n",
    "            (r'.*functionality\\s+is\\s+already\\s+implemented.*', ''),\n",
    "            (r'.*already\\s+implemented.*', ''),\n",
    "            \n",
    "            # Testing patterns\n",
    "            (r'.*developer\\s+wants\\s+to\\s+test.*search.*', 'Test search functionality'),\n",
    "            (r'.*test.*search.*facility.*', 'Test search functionality'),\n",
    "            \n",
    "            # Generic patterns\n",
    "            (r'.*needs?\\s+to\\s+be\\s+(\\w+).*', r'Implement \\1'),\n",
    "            (r'.*should\\s+be\\s+(\\w+).*', r'Implement \\1'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in extraction_patterns:\n",
    "            if re.match(pattern, task, re.IGNORECASE):\n",
    "                if replacement == '':\n",
    "                    return ''  # Skip this task\n",
    "                return replacement\n",
    "        \n",
    "        return task\n",
    "    \n",
    "    def _starts_with_imperative(self, task: str) -> bool:\n",
    "        \"\"\"Check if task starts with an imperative verb\"\"\"\n",
    "        imperative_verbs = [\n",
    "            'create', 'implement', 'design', 'build', 'develop', 'test', 'write',\n",
    "            'add', 'configure', 'setup', 'install', 'deploy', 'validate', 'verify',\n",
    "            'integrate', 'connect', 'establish', 'define', 'identify', 'research',\n",
    "            'analyze', 'review', 'update', 'modify', 'refactor', 'optimize',\n",
    "            'document', 'prepare', 'plan', 'schedule', 'coordinate', 'organize'\n",
    "        ]\n",
    "        \n",
    "        first_word = task.split()[0].lower() if task.split() else ''\n",
    "        return first_word in imperative_verbs\n",
    "    \n",
    "    def _convert_to_imperative(self, task: str) -> str:\n",
    "        \"\"\"Convert task to imperative form\"\"\"\n",
    "        # Simple conversion patterns\n",
    "        conversions = [\n",
    "            (r'^the (\\w+) (needs?|requires?|should|must) (to\\s+)?(.+)', r'\\4'),\n",
    "            (r'^(\\w+ing)\\s+(.+)', r'Implement \\2'),  # \"Creating user form\" -> \"Implement user form\"\n",
    "            (r'^(.+) (is|are) needed', r'Create \\1'),\n",
    "            (r'^(.+) should be (.+)', r'Make \\1 \\2'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in conversions:\n",
    "            if re.match(pattern, task, re.IGNORECASE):\n",
    "                task = re.sub(pattern, replacement, task, flags=re.IGNORECASE)\n",
    "                break\n",
    "        \n",
    "        return task\n",
    "    \n",
    "    def _parse_tasks(self, content: str) -> List[str]:\n",
    "        \"\"\"Enhanced task parsing with better error handling\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        tasks = []\n",
    "        in_tasks_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Detect tasks section\n",
    "            if (line.lower().startswith('tasks:') or \n",
    "                (re.match(r'^1\\.', line) and not in_tasks_section)):\n",
    "                in_tasks_section = True\n",
    "                if not line.lower().endswith(':'):\n",
    "                    # Extract first task from this line\n",
    "                    task = re.sub(r'^[\\d\\.\\s]+', '', line).strip()\n",
    "                    if task and len(task) > 5:\n",
    "                        tasks.append(task)\n",
    "                continue\n",
    "            \n",
    "            if not in_tasks_section:\n",
    "                continue\n",
    "            \n",
    "            # Parse numbered tasks\n",
    "            if re.match(r'^[\\d\\.\\s]+', line):\n",
    "                task = re.sub(r'^[\\d\\.\\s]+', '', line).strip()\n",
    "                if task and len(task) > 5:\n",
    "                    tasks.append(task)\n",
    "        \n",
    "        return tasks\n",
    "    \n",
    "    def _apply_consistency(self, task_samples: List[List[str]]) -> List[str]:\n",
    "        \"\"\"Apply self-consistency to select most consistent tasks\"\"\"\n",
    "        task_counts = Counter()\n",
    "        task_mapping = {}\n",
    "        \n",
    "        for sample in task_samples:\n",
    "            seen = set()\n",
    "            for task in sample:\n",
    "                normalized = self._normalize_task(task)\n",
    "                if normalized not in seen:\n",
    "                    task_counts[normalized] += 1\n",
    "                    seen.add(normalized)\n",
    "                    if normalized not in task_mapping:\n",
    "                        task_mapping[normalized] = task\n",
    "        \n",
    "        # Select tasks appearing in majority of samples\n",
    "        threshold = max(1, len(task_samples) // 2)\n",
    "        consistent_tasks = []\n",
    "        \n",
    "        for normalized, count in task_counts.items():\n",
    "            if count >= threshold:\n",
    "                consistent_tasks.append(task_mapping[normalized])\n",
    "        \n",
    "        # Sort by frequency\n",
    "        consistent_tasks.sort(key=lambda x: task_counts[self._normalize_task(x)], reverse=True)\n",
    "        return consistent_tasks\n",
    "    \n",
    "    def _normalize_task(self, task: str) -> str:\n",
    "        \"\"\"Normalize task for comparison\"\"\"\n",
    "        normalized = task.lower().strip()\n",
    "        # Remove common action words for better matching\n",
    "        normalized = re.sub(r'^(create|implement|design|build|add|make|ensure|handle)\\s+', '', normalized)\n",
    "        words = normalized.split()\n",
    "        stop_words = {'a', 'an', 'the', 'for', 'to', 'and', 'or', 'with', 'in', 'on', 'at'}\n",
    "        meaningful_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "        return ' '.join(sorted(meaningful_words))\n",
    "\n",
    "\n",
    "class StoryPointEstimatorAgent:\n",
    "    \"\"\"Step 2: Estimate story points for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def estimate_story_points(self, user_story: str, tasks: List[str]) -> Dict[str, Any]:\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a story point estimation expert. Estimate story points for each task using the Fibonacci sequence (1, 2, 3, 5, 8, 13).\n",
    "\n",
    "Consider complexity, time, risk, and uncertainty.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Tasks and Estimates:\n",
    "1. Design user registration form interface (3 points)\n",
    "2. Implement email validation and verification system (5 points)\n",
    "3. Create password strength requirements and validation (3 points)\n",
    "4. Build user profile creation workflow (5 points)\n",
    "5. Add account activation process (3 points)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Tasks and Estimates:\n",
    "1. Design analytics dashboard layout and components (5 points)\n",
    "2. Implement data collection and aggregation system (8 points)\n",
    "3. Create real-time performance metrics display (5 points)\n",
    "4. Add filtering and date range selection features (3 points)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Tasks and Estimates:\n",
    "1. Design search interface with filters (3 points)\n",
    "2. Implement search algorithm and indexing (8 points)\n",
    "3. Create search results display with pagination (3 points)\n",
    "4. Add search history and suggestions feature (5 points)\n",
    "\n",
    "Now estimate points for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: X points\n",
    "Task 2: Y points\n",
    "Task 3: Z points\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('story_point_estimation', prompt, output_text)\n",
    "        \n",
    "        points = self._parse_story_points(output_text, tasks)\n",
    "        print(f\"âœ“ Estimated story points for {len(points)} tasks\")\n",
    "        total_points = sum(points.values())\n",
    "        return {\n",
    "            'total_story_points': total_points,\n",
    "            'task_points': points,\n",
    "            'estimated_sum': total_points\n",
    "        }\n",
    "    \n",
    "    def _parse_story_points(self, content: str, tasks: List[str]) -> Dict[str, int]:\n",
    "        points = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    # Extract task number and points\n",
    "                    parts = line.split(':')\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    points_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Extract points\n",
    "                            points_match = re.search(r'(\\d+)', points_part)\n",
    "                            if points_match:\n",
    "                                story_points = int(points_match.group(1))\n",
    "                                # Validate Fibonacci sequence\n",
    "                                valid_points = [1, 2, 3, 5, 8, 13]\n",
    "                                if story_points not in valid_points:\n",
    "                                    # Find closest valid point\n",
    "                                    story_points = min(valid_points, key=lambda x: abs(x - story_points))\n",
    "                                \n",
    "                                task_desc = tasks[task_num - 1]\n",
    "                                points[task_desc] = story_points\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse story points line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default points\n",
    "        for task in tasks:\n",
    "            if task not in points:\n",
    "                points[task] = 3  # Default moderate complexity\n",
    "        \n",
    "        return points\n",
    "\n",
    "\n",
    "class RequiredSkillsAgent:\n",
    "    \"\"\"Step 2b: Identify required skills for each task\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def map_skills(self, task: str) -> List[str]:\n",
    "        \"\"\"Map skills for individual task\"\"\"\n",
    "        user_story = \"General task completion\"\n",
    "        tasks = [task]\n",
    "        tasks_str = \"1. \" + task\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify the specific skills required for each task.\n",
    "\n",
    "Consider:\n",
    "- Programming languages\n",
    "- Frameworks and tools\n",
    "- Domain expertise\n",
    "- Technical disciplines (frontend, backend, database, etc.)\n",
    "- Soft skills when relevant\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task 1: skill1, skill2, skill3\n",
    "\n",
    "Use concise skill names like: javascript, react, database_design, api_development, user_research, etc.\n",
    "\"\"\"\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "    \n",
    "        # Parse for single task\n",
    "        skills_map = self._parse_skills(output_text, tasks)\n",
    "        return skills_map.get(task, [\"general_development\"])\n",
    "\n",
    "    async def identify_skills(self, user_story: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Identify skills for all tasks in a single API call for efficiency\"\"\"\n",
    "        if not tasks:\n",
    "            return {}\n",
    "        \n",
    "        # Process all tasks in one API call instead of individual calls\n",
    "        tasks_str = \"\\n\".join([f\"{i+1}. {task}\" for i, task in enumerate(tasks)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a technical skills analyst. Identify the specific skills required for each task.\n",
    "\n",
    "Consider:\n",
    "- Programming languages\n",
    "- Frameworks and tools\n",
    "- Domain expertise\n",
    "- Technical disciplines (frontend, backend, database, etc.)\n",
    "- Soft skills when relevant\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format for ALL tasks:\n",
    "Task 1: skill1, skill2, skill3\n",
    "Task 2: skill1, skill2, skill3\n",
    "Task 3: skill1, skill2, skill3\n",
    "...\n",
    "\n",
    "Use concise skill names like: javascript, react, database_design, api_development, user_research, etc.\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        output_text = response.choices[0].message.content.strip()\n",
    "        token_tracker.track_api_call('required_skills', prompt, output_text)\n",
    "        \n",
    "        # Parse skills for all tasks\n",
    "        skills_map = self._parse_skills(output_text, tasks)\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = [\"general_development\"]\n",
    "        \n",
    "        print(f\"âœ“ Identified skills for {len(skills_map)} tasks in 1 API call\")\n",
    "        return skills_map\n",
    "\n",
    "    def _parse_skills(self, content: str, tasks: List[str]) -> Dict[str, List[str]]:\n",
    "        skills_map = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'task' in line.lower() and ':' in line:\n",
    "                try:\n",
    "                    parts = line.split(':', 1)\n",
    "                    task_part = parts[0].strip().lower()\n",
    "                    skills_part = parts[1].strip()\n",
    "                    \n",
    "                    # Extract task number\n",
    "                    task_num_match = re.search(r'task\\s*(\\d+)', task_part)\n",
    "                    if task_num_match:\n",
    "                        task_num = int(task_num_match.group(1))\n",
    "                        if 1 <= task_num <= len(tasks):\n",
    "                            # Clean and parse skills - remove any task description remnants\n",
    "                            skills_raw = skills_part.split(',')\n",
    "                            skills = []\n",
    "                            \n",
    "                            for skill in skills_raw:\n",
    "                                skill = skill.strip()\n",
    "                                \n",
    "                                # Remove task description if it somehow got included\n",
    "                                # Look for patterns like \"Task description - skill1, skill2\"\n",
    "                                if ' - ' in skill:\n",
    "                                    skill = skill.split(' - ')[-1].strip()\n",
    "                                \n",
    "                                # Skip if it looks like a task description (contains common task words)\n",
    "                                task_words = ['test', 'create', 'implement', 'develop', 'design', 'build', 'need to', 'display', 'find']\n",
    "                                if any(word in skill.lower() for word in task_words) and len(skill.split()) > 2:\n",
    "                                    continue\n",
    "                                    \n",
    "                                # Only keep if it looks like a real skill\n",
    "                                if skill and len(skill) > 1 and not skill.startswith('I '):\n",
    "                                    skills.append(skill)\n",
    "                            \n",
    "                            # If no valid skills found, add default\n",
    "                            if not skills:\n",
    "                                skills = self._get_default_skills_for_task(tasks[task_num - 1])\n",
    "                            \n",
    "                            task_desc = tasks[task_num - 1]\n",
    "                            skills_map[task_desc] = skills\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse skills line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        # Fill in missing tasks with default skills\n",
    "        for task in tasks:\n",
    "            if task not in skills_map:\n",
    "                skills_map[task] = self._get_default_skills_for_task(task)\n",
    "        \n",
    "        return skills_map\n",
    "    \n",
    "    def _get_default_skills_for_task(self, task: str) -> List[str]:\n",
    "        \"\"\"Get default skills based on task content\"\"\"\n",
    "        task_lower = task.lower()\n",
    "        \n",
    "        # Pattern matching for common task types\n",
    "        if 'test' in task_lower:\n",
    "            return [\"testing\", \"unit_testing\", \"automation_testing\"]\n",
    "        elif 'database' in task_lower:\n",
    "            return [\"database_design\", \"sql\", \"data_modeling\"]\n",
    "        elif 'location' in task_lower or 'geolocation' in task_lower:\n",
    "            return [\"geolocation\", \"javascript\", \"frontend_development\"]\n",
    "        elif 'search' in task_lower or 'algorithm' in task_lower:\n",
    "            return [\"algorithm_design\", \"backend_development\", \"data_structures\"]\n",
    "        elif 'interface' in task_lower or 'display' in task_lower or 'ui' in task_lower:\n",
    "            return [\"frontend_development\", \"javascript\", \"ui_design\"]\n",
    "        elif 'api' in task_lower:\n",
    "            return [\"api_development\", \"backend_development\", \"rest_api\"]\n",
    "        else:\n",
    "            return [\"general_development\"]\n",
    "\n",
    "\n",
    "class DependencyAgent:\n",
    "    \"\"\"Step 3: Analyze dependencies between tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    async def analyze_dependencies(self, user_story: str, tasks: List[str], story_points: Dict[str, int]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Analyze dependencies using self-consistency approach\"\"\"\n",
    "        num_samples = 3\n",
    "        all_dependency_samples = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            temperature = 0.2 + (i * 0.2)  # 0.2, 0.4, 0.6\n",
    "            \n",
    "            tasks_with_points = []\n",
    "            for j, task in enumerate(tasks):\n",
    "                points = story_points.get(task, 3)\n",
    "                tasks_with_points.append(f\"{j+1}. {task} ({points} points)\")\n",
    "            \n",
    "            tasks_str = \"\\n\".join(tasks_with_points)\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "You are a dependency analysis expert. Identify which tasks must be completed before others can begin.\n",
    "\n",
    "Consider logical workflow order and technical dependencies.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "User Story: \"As a user, I want to create an account so that I can access personalized features\"\n",
    "Dependencies:\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "Task 4 depends on Task 2 (rework_effort: 3)\n",
    "Task 5 depends on Task 2 (rework_effort: 2)\n",
    "Task 5 depends on Task 4 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As an admin, I want to view analytics dashboard so that I can monitor system performance\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a customer, I want to search for products so that I can find what I need quickly\"\n",
    "Dependencies:\n",
    "Task 3 depends on Task 2 (rework_effort: 3)\n",
    "Task 4 depends on Task 2 (rework_effort: 2)\n",
    "\n",
    "User Story: \"As a developer, I want to set up CI/CD pipeline so that deployments are automated\"\n",
    "Dependencies:\n",
    "Task 2 depends on Task 1 (rework_effort: 2)\n",
    "Task 3 depends on Task 1 (rework_effort: 2)\n",
    "\n",
    "rework_effort scale:\n",
    "- 1: Low effort if prerequisite changes\n",
    "- 2: Moderate rework needed  \n",
    "- 3: High rework effort required\n",
    "\n",
    "Now analyze dependencies for this user story:\n",
    "\n",
    "User Story Context: {user_story}\n",
    "\n",
    "Tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Return ONLY this format:\n",
    "Task X depends on Task Y (rework_effort: Z)\n",
    "\n",
    "Only include REAL dependencies. Don't create artificial ones.\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-70b-8192\",\n",
    "                    temperature=min(temperature, 1.0)\n",
    "                )\n",
    "                \n",
    "                output_text = response.choices[0].message.content.strip()\n",
    "                token_tracker.track_api_call('dependency_analysis', prompt, output_text)\n",
    "                \n",
    "                dependencies = self._parse_dependencies(output_text, tasks)\n",
    "                if dependencies:\n",
    "                    all_dependency_samples.append(dependencies)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Dependency analysis sample {i+1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_dependency_samples:\n",
    "            print(f\"âœ“ No dependencies found for tasks\")\n",
    "            return {}\n",
    "        \n",
    "        # Apply self-consistency\n",
    "        final_dependencies = self._apply_dependency_consistency(all_dependency_samples)\n",
    "        print(f\"âœ“ Analyzed dependencies for {len(final_dependencies)} tasks\")\n",
    "        return final_dependencies\n",
    "    \n",
    "    def _parse_dependencies(self, content: str, tasks: List[str]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        dependencies = {}\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'depends on' in line.lower():\n",
    "                try:\n",
    "                    # Parse: Task X depends on Task Y (rework_effort: Z)\n",
    "                    match = re.search(r'task\\s*(\\d+)\\s*depends\\s*on\\s*task\\s*(\\d+).*rework_effort:\\s*(\\d+)', line.lower())\n",
    "                    if match:\n",
    "                        dependent_num = int(match.group(1))\n",
    "                        prerequisite_num = int(match.group(2))\n",
    "                        rework_effort = int(match.group(3))\n",
    "                        \n",
    "                        # Validate task numbers\n",
    "                        if 1 <= dependent_num <= len(tasks) and 1 <= prerequisite_num <= len(tasks):\n",
    "                            dependent_task = tasks[dependent_num - 1]\n",
    "                            prerequisite_task = tasks[prerequisite_num - 1]\n",
    "                            \n",
    "                            # Validate rework_effort\n",
    "                            if rework_effort not in [1, 2, 3]:\n",
    "                                rework_effort = 2  # Default\n",
    "                            \n",
    "                            if dependent_task not in dependencies:\n",
    "                                dependencies[dependent_task] = []\n",
    "                            \n",
    "                            dependencies[dependent_task].append({\n",
    "                                \"task_id\": prerequisite_task,\n",
    "                                \"rework_effort\": rework_effort\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Couldn't parse dependency line: {line}\")\n",
    "                    continue\n",
    "        \n",
    "        return dependencies\n",
    "    \n",
    "    def _apply_dependency_consistency(self, dependency_samples: List[Dict[str, List[Dict[str, any]]]]) -> Dict[str, List[Dict[str, any]]]:\n",
    "        \"\"\"Apply self-consistency to dependency analysis\"\"\"\n",
    "        dependency_counts = defaultdict(Counter)\n",
    "        \n",
    "        # Count occurrences of each dependency\n",
    "        for sample in dependency_samples:\n",
    "            for dependent_task, prerequisites in sample.items():\n",
    "                for prereq in prerequisites:\n",
    "                    key = (dependent_task, prereq['task_id'])\n",
    "                    dependency_counts[dependent_task][key] += 1\n",
    "        \n",
    "        # Select dependencies appearing in majority of samples\n",
    "        threshold = max(1, len(dependency_samples) // 2)\n",
    "        final_dependencies = {}\n",
    "        \n",
    "        for dependent_task, prereq_counts in dependency_counts.items():\n",
    "            consistent_prereqs = []\n",
    "            for (dep_task, prereq_task), count in prereq_counts.items():\n",
    "                if count >= threshold:\n",
    "                    # Find the most common rework_effort for this dependency\n",
    "                    effort_counts = Counter()\n",
    "                    for sample in dependency_samples:\n",
    "                        if dep_task in sample:\n",
    "                            for prereq in sample[dep_task]:\n",
    "                                if prereq['task_id'] == prereq_task:\n",
    "                                    effort_counts[prereq['rework_effort']] += 1\n",
    "                    \n",
    "                    most_common_effort = effort_counts.most_common(1)[0][0] if effort_counts else 2\n",
    "                    consistent_prereqs.append({\n",
    "                        \"task_id\": prereq_task,\n",
    "                        \"rework_effort\": most_common_effort\n",
    "                    })\n",
    "            \n",
    "            if consistent_prereqs:\n",
    "                final_dependencies[dependent_task] = consistent_prereqs\n",
    "        \n",
    "        return final_dependencies\n",
    "\n",
    "\n",
    "class TaskMergerAgent:\n",
    "    \"\"\"Merges similar tasks across user stories using semantic similarity\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold: float = 0.4):  # Lowered further from 0.5 to 0.4\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 3),  # Increased to capture more phrases\n",
    "            max_features=1500,  # Increased features for better similarity detection\n",
    "            lowercase=True,\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'  # Better tokenization\n",
    "        )\n",
    "    \n",
    "    def merge_similar_tasks(self, all_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Merge similar tasks across all user stories\"\"\"\n",
    "        print(f\"ðŸ”„ Merging similar tasks across {len(all_results)} user stories...\")\n",
    "        \n",
    "        # Extract all tasks with metadata\n",
    "        all_tasks = []\n",
    "        task_metadata = {}\n",
    "        \n",
    "        for story_idx, result in enumerate(all_results):\n",
    "            if 'error' in result['output']:\n",
    "                continue\n",
    "                \n",
    "            story_input = result['input']\n",
    "            for task_data in result['output']['tasks']:\n",
    "                task_key = f\"story_{story_idx}_{task_data['id']}\"\n",
    "                \n",
    "                all_tasks.append({\n",
    "                    'key': task_key,\n",
    "                    'description': task_data['description'],\n",
    "                    'story_idx': story_idx,\n",
    "                    'original_id': task_data['id'],\n",
    "                    'story_input': story_input\n",
    "                })\n",
    "                \n",
    "                task_metadata[task_key] = {\n",
    "                    'story_points': task_data['story_points'],\n",
    "                    'required_skills': task_data['required_skills'],\n",
    "                    'depends_on': task_data['depends_on'],\n",
    "                    'story_input': story_input\n",
    "                }\n",
    "        \n",
    "        if len(all_tasks) < 2:\n",
    "            print(f\"âœ“ No merging needed - only {len(all_tasks)} tasks found\")\n",
    "            return all_results\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        task_descriptions = [task['description'] for task in all_tasks]\n",
    "        similarity_matrix = self._calculate_similarity_matrix(task_descriptions)\n",
    "        \n",
    "        # Find clusters of similar tasks\n",
    "        clusters = self._find_task_clusters(all_tasks, similarity_matrix)\n",
    "        \n",
    "        # Merge tasks within clusters\n",
    "        merged_results = self._merge_task_clusters(all_results, clusters, task_metadata)\n",
    "        \n",
    "        print(f\"âœ“ Merged {len(all_tasks)} tasks into {sum(len(r['output']['tasks']) for r in merged_results if 'error' not in r['output'])} unique tasks\")\n",
    "        return merged_results\n",
    "    \n",
    "    def _calculate_similarity_matrix(self, descriptions: List[str]) -> np.ndarray:\n",
    "        \"\"\"Calculate TF-IDF similarity matrix for task descriptions\"\"\"\n",
    "        try:\n",
    "            # Preprocess descriptions for better similarity detection\n",
    "            processed_descriptions = []\n",
    "            for desc in descriptions:\n",
    "                # Normalize task descriptions for better similarity matching\n",
    "                processed = desc.lower()\n",
    "                # Extract key terms\n",
    "                processed = re.sub(r'^(test|create|implement|design|build|develop|write|add)', '', processed)\n",
    "                processed = re.sub(r'\\s+', ' ', processed).strip()\n",
    "                processed_descriptions.append(processed)\n",
    "            \n",
    "            # Vectorize descriptions\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(processed_descriptions)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "            \n",
    "            return similarity_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating similarity matrix: {e}\")\n",
    "            # Return identity matrix as fallback\n",
    "            n = len(descriptions)\n",
    "            return np.eye(n)\n",
    "    \n",
    "    def _find_task_clusters(self, all_tasks: List[Dict], similarity_matrix: np.ndarray) -> List[List[int]]:\n",
    "        \"\"\"Find clusters of similar tasks using similarity threshold\"\"\"\n",
    "        n_tasks = len(all_tasks)\n",
    "        visited = [False] * n_tasks\n",
    "        clusters = []\n",
    "        \n",
    "        for i in range(n_tasks):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            \n",
    "            # Start new cluster\n",
    "            cluster = [i]\n",
    "            visited[i] = True\n",
    "            \n",
    "            # Find similar tasks\n",
    "            for j in range(i + 1, n_tasks):\n",
    "                if not visited[j] and similarity_matrix[i][j] >= self.similarity_threshold:\n",
    "                    cluster.append(j)\n",
    "                    visited[j] = True\n",
    "            \n",
    "            clusters.append(cluster)\n",
    "        \n",
    "        # Log cluster information\n",
    "        merged_clusters = [c for c in clusters if len(c) > 1]\n",
    "        if merged_clusters:\n",
    "            print(f\"  ðŸ“Š Found {len(merged_clusters)} clusters with similar tasks:\")\n",
    "            for i, cluster in enumerate(merged_clusters):\n",
    "                print(f\"    Cluster {i+1}: {len(cluster)} similar tasks\")\n",
    "                for task_idx in cluster[:2]:  # Show first 2 tasks as examples\n",
    "                    print(f\"      - {all_tasks[task_idx]['description'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"  ðŸ“Š No similar tasks found (threshold: {self.similarity_threshold})\")\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _merge_task_clusters(self, all_results: List[Dict[str, Any]], clusters: List[List[int]], \n",
    "                           task_metadata: Dict[str, Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Merge tasks within clusters and update dependencies\"\"\"\n",
    "        \n",
    "        # Create mapping from old task keys to new merged task info\n",
    "        task_key_mapping = {}\n",
    "        all_tasks = []\n",
    "        \n",
    "        # Rebuild all_tasks list for mapping\n",
    "        for story_idx, result in enumerate(all_results):\n",
    "            if 'error' in result['output']:\n",
    "                continue\n",
    "            for task_data in result['output']['tasks']:\n",
    "                task_key = f\"story_{story_idx}_{task_data['id']}\"\n",
    "                all_tasks.append({\n",
    "                    'key': task_key,\n",
    "                    'description': task_data['description'],\n",
    "                    'story_idx': story_idx,\n",
    "                    'original_id': task_data['id']\n",
    "                })\n",
    "        \n",
    "        # Process clusters to create merged tasks\n",
    "        merged_task_counter = 1\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            if len(cluster) == 1:\n",
    "                # Single task - no merging needed\n",
    "                task_idx = cluster[0]\n",
    "                old_key = all_tasks[task_idx]['key']\n",
    "                new_id = f\"T_{merged_task_counter:03d}\"\n",
    "                task_key_mapping[old_key] = {\n",
    "                    'new_id': new_id,\n",
    "                    'description': all_tasks[task_idx]['description'],\n",
    "                    'story_idx': all_tasks[task_idx]['story_idx']\n",
    "                }\n",
    "                merged_task_counter += 1\n",
    "            else:\n",
    "                # Multiple similar tasks - merge them\n",
    "                cluster_tasks = [all_tasks[i] for i in cluster]\n",
    "                \n",
    "                # Choose the most comprehensive description and clean it\n",
    "                best_description = max(cluster_tasks, key=lambda t: len(t['description']))['description']\n",
    "                \n",
    "                # Clean the merged task description\n",
    "                cleaned_description = self._clean_merged_task_description(best_description)\n",
    "                \n",
    "                # Merge skills and story points\n",
    "                all_skills = set()\n",
    "                total_story_points = 0\n",
    "                source_stories = set()\n",
    "                \n",
    "                for task_idx in cluster:\n",
    "                    old_key = all_tasks[task_idx]['key']\n",
    "                    metadata = task_metadata[old_key]\n",
    "                    all_skills.update(metadata['required_skills'])\n",
    "                    total_story_points = max(total_story_points, metadata['story_points'])  # Use max story points\n",
    "                    source_stories.add(metadata['story_input'])\n",
    "                \n",
    "                new_id = f\"T_{merged_task_counter:03d}\"\n",
    "                \n",
    "                # Map all old keys to the new merged task\n",
    "                for task_idx in cluster:\n",
    "                    old_key = all_tasks[task_idx]['key']\n",
    "                    task_key_mapping[old_key] = {\n",
    "                        'new_id': new_id,\n",
    "                        'description': cleaned_description,\n",
    "                        'merged': True,\n",
    "                        'merged_skills': list(all_skills),\n",
    "                        'merged_story_points': total_story_points,\n",
    "                        'source_stories': list(source_stories)\n",
    "                    }\n",
    "                \n",
    "                merged_task_counter += 1\n",
    "        \n",
    "        # Rebuild results with merged tasks\n",
    "        return self._rebuild_results_with_merged_tasks(all_results, task_key_mapping, task_metadata)\n",
    "    \n",
    "    def _rebuild_results_with_merged_tasks(self, all_results: List[Dict[str, Any]], \n",
    "                                         task_key_mapping: Dict[str, Dict], \n",
    "                                         task_metadata: Dict[str, Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Rebuild results with merged tasks and updated dependencies\"\"\"\n",
    "        \n",
    "        # Create global merged task registry\n",
    "        global_tasks = {}\n",
    "        \n",
    "        # First pass: create merged tasks\n",
    "        for story_idx, result in enumerate(all_results):\n",
    "            if 'error' in result['output']:\n",
    "                continue\n",
    "                \n",
    "            for task_data in result['output']['tasks']:\n",
    "                old_key = f\"story_{story_idx}_{task_data['id']}\"\n",
    "                mapping = task_key_mapping[old_key]\n",
    "                new_id = mapping['new_id']\n",
    "                \n",
    "                if new_id not in global_tasks:\n",
    "                    if mapping.get('merged', False):\n",
    "                        # This is a merged task\n",
    "                        global_tasks[new_id] = {\n",
    "                            'description': mapping['description'],\n",
    "                            'id': new_id,\n",
    "                            'story_points': mapping['merged_story_points'],\n",
    "                            'required_skills': mapping['merged_skills'],\n",
    "                            'depends_on': [],  # Will be populated in second pass\n",
    "                            'source_stories': mapping['source_stories']\n",
    "                        }\n",
    "                    else:\n",
    "                        # This is a single task\n",
    "                        metadata = task_metadata[old_key]\n",
    "                        global_tasks[new_id] = {\n",
    "                            'description': mapping['description'],\n",
    "                            'id': new_id,\n",
    "                            'story_points': metadata['story_points'],\n",
    "                            'required_skills': metadata['required_skills'],\n",
    "                            'depends_on': [],  # Will be populated in second pass\n",
    "                            'source_stories': [metadata['story_input']]\n",
    "                        }\n",
    "        \n",
    "        # Second pass: resolve dependencies\n",
    "        for story_idx, result in enumerate(all_results):\n",
    "            if 'error' in result['output']:\n",
    "                continue\n",
    "                \n",
    "            for task_data in result['output']['tasks']:\n",
    "                old_key = f\"story_{story_idx}_{task_data['id']}\"\n",
    "                new_id = task_key_mapping[old_key]['new_id']\n",
    "                \n",
    "                # Convert old dependencies to new dependencies\n",
    "                for old_dep in task_data['depends_on']:\n",
    "                    old_dep_description = old_dep['task_id']\n",
    "                    \n",
    "                    # Find the new task ID for this dependency\n",
    "                    new_dep_id = None\n",
    "                    for check_story_idx, check_result in enumerate(all_results):\n",
    "                        if 'error' in check_result['output']:\n",
    "                            continue\n",
    "                        for check_task in check_result['output']['tasks']:\n",
    "                            if (check_task['description'] == old_dep_description or \n",
    "                                check_task['id'] == old_dep_description):\n",
    "                                check_old_key = f\"story_{check_story_idx}_{check_task['id']}\"\n",
    "                                if check_old_key in task_key_mapping:\n",
    "                                    new_dep_id = task_key_mapping[check_old_key]['new_id']\n",
    "                                    break\n",
    "                        if new_dep_id:\n",
    "                            break\n",
    "                    \n",
    "                    # Add dependency if found and not self-referential\n",
    "                    if new_dep_id and new_dep_id != new_id:\n",
    "                        existing_deps = [d['task_id'] for d in global_tasks[new_id]['depends_on']]\n",
    "                        if new_dep_id not in existing_deps:\n",
    "                            global_tasks[new_id]['depends_on'].append({\n",
    "                                'task_id': new_dep_id,\n",
    "                                'rework_effort': old_dep['rework_effort']\n",
    "                            })\n",
    "        \n",
    "        # Create new results structure with merged tasks\n",
    "        new_results = []\n",
    "        \n",
    "        # First, identify which tasks are merged vs single-story tasks\n",
    "        merged_tasks = {}  # Tasks that come from multiple user stories\n",
    "        single_story_tasks = defaultdict(list)  # Tasks that come from single user stories, grouped by story\n",
    "        \n",
    "        for task_id, task_data in global_tasks.items():\n",
    "            if len(task_data['source_stories']) > 1:\n",
    "                # This is a merged task\n",
    "                merged_tasks[task_id] = task_data\n",
    "            else:\n",
    "                # This is a single-story task\n",
    "                source_story = task_data['source_stories'][0]\n",
    "                single_story_tasks[source_story].append((task_id, task_data))\n",
    "        \n",
    "        # Assign each merged task to one of its contributing user stories\n",
    "        # We'll assign it to the first contributing story that appears in our results\n",
    "        merged_task_assignments = {}  # story_input -> list of merged tasks\n",
    "        \n",
    "        for task_id, task_data in merged_tasks.items():\n",
    "            # Find the first story in our results that contributed to this merged task\n",
    "            assigned_story = None\n",
    "            for result in all_results:\n",
    "                if 'error' not in result['output'] and result['input'] in task_data['source_stories']:\n",
    "                    assigned_story = result['input']\n",
    "                    break\n",
    "            \n",
    "            if assigned_story:\n",
    "                if assigned_story not in merged_task_assignments:\n",
    "                    merged_task_assignments[assigned_story] = []\n",
    "                merged_task_assignments[assigned_story].append((task_id, task_data))\n",
    "        \n",
    "        # Process each original result\n",
    "        for story_idx, result in enumerate(all_results):\n",
    "            if 'error' in result['output']:\n",
    "                new_results.append(result)\n",
    "                continue\n",
    "            \n",
    "            story_input = result['input']\n",
    "            formatted_tasks = []\n",
    "            total_story_points = 0\n",
    "            \n",
    "            # Add merged tasks assigned to this story\n",
    "            assigned_merged_tasks = merged_task_assignments.get(story_input, [])\n",
    "            for task_id, task_data in assigned_merged_tasks:\n",
    "                formatted_task = {\n",
    "                    'description': task_data['description'],\n",
    "                    'id': task_id,\n",
    "                    'user_stories': task_data['source_stories'],  # All contributing user stories\n",
    "                    'story_points': task_data['story_points'],\n",
    "                    'depends_on': task_data['depends_on'],\n",
    "                    'required_skills': task_data['required_skills']\n",
    "                }\n",
    "                formatted_tasks.append(formatted_task)\n",
    "                total_story_points += task_data['story_points']\n",
    "            \n",
    "            # Add this story's own single-story tasks\n",
    "            story_tasks = single_story_tasks.get(story_input, [])\n",
    "            for task_id, task_data in story_tasks:\n",
    "                formatted_task = {\n",
    "                    'description': task_data['description'],\n",
    "                    'id': task_id,\n",
    "                    'user_stories': task_data['source_stories'],  # Single user story\n",
    "                    'story_points': task_data['story_points'],\n",
    "                    'depends_on': task_data['depends_on'],\n",
    "                    'required_skills': task_data['required_skills']\n",
    "                }\n",
    "                formatted_tasks.append(formatted_task)\n",
    "                total_story_points += task_data['story_points']\n",
    "            \n",
    "            if formatted_tasks:\n",
    "                new_result = {\n",
    "                    'input': story_input,\n",
    "                    'output': {\n",
    "                        'story_points': total_story_points,\n",
    "                        'tasks': formatted_tasks\n",
    "                    }\n",
    "                }\n",
    "                new_results.append(new_result)\n",
    "            else:\n",
    "                # This story has no tasks (shouldn't happen, but just in case)\n",
    "                new_result = {\n",
    "                    'input': story_input,\n",
    "                    'output': {\n",
    "                        'story_points': 0,\n",
    "                        'tasks': [],\n",
    "                        'note': 'No tasks found for this user story'\n",
    "                    }\n",
    "                }\n",
    "                new_results.append(new_result)\n",
    "        \n",
    "        return new_results\n",
    "    \n",
    "    def _clean_merged_task_description(self, description: str) -> str:\n",
    "        \"\"\"Clean merged task descriptions to be concise and clear\"\"\"\n",
    "        # Remove verbose prefixes and unnecessary words\n",
    "        prefixes_to_remove = [\n",
    "            r'^we need to\\s+',\n",
    "            r'^we should\\s+',\n",
    "            r'^we may also want to\\s+',\n",
    "            r'^this involves\\s+',\n",
    "            r'^this includes\\s+',\n",
    "            r'^the developer needs to\\s+',\n",
    "            r'^the system should\\s+',\n",
    "            r'^it is important to\\s+',\n",
    "        ]\n",
    "        \n",
    "        for prefix in prefixes_to_remove:\n",
    "            description = re.sub(prefix, '', description, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Ensure it starts with an imperative verb\n",
    "        if description and not description.split()[0].lower() in ['create', 'implement', 'design', 'build', 'develop', 'test', 'write', 'add', 'configure', 'setup', 'validate', 'verify', 'integrate', 'define', 'identify', 'research', 'analyze', 'review', 'update']:\n",
    "            # Add imperative verb based on context\n",
    "            if 'test' in description.lower():\n",
    "                description = 'Test ' + description\n",
    "            elif 'create' in description.lower() or 'form' in description.lower():\n",
    "                description = 'Create ' + description\n",
    "            elif 'implement' in description.lower():\n",
    "                description = 'Implement ' + description\n",
    "            elif 'design' in description.lower():\n",
    "                description = 'Design ' + description\n",
    "            else:\n",
    "                description = 'Implement ' + description\n",
    "        \n",
    "        # Capitalize and clean\n",
    "        description = description.strip()\n",
    "        if description:\n",
    "            description = description[0].upper() + description[1:] if len(description) > 1 else description.upper()\n",
    "        \n",
    "        return description.rstrip('.')\n",
    "\n",
    "\n",
    "class FormatValidator:\n",
    "    \"\"\"Validates and formats final output structure\"\"\"\n",
    "    \n",
    "    def validate_and_format(self, user_story: str, tasks_data: List[Dict], \n",
    "                          total_story_points: int) -> Dict[str, Any]:\n",
    "        \"\"\"Validate and format the final output structure\"\"\"\n",
    "        print(f\"âœ… Validating and formatting final output...\")\n",
    "        \n",
    "        try:\n",
    "            # Validate required fields\n",
    "            for task in tasks_data:\n",
    "                required_fields = ['description', 'id', 'user_stories', 'story_points', 'depends_on', 'required_skills']\n",
    "                for field in required_fields:\n",
    "                    if field not in task:\n",
    "                        raise ValueError(f\"Missing required field '{field}' in task\")\n",
    "            \n",
    "            # Format output\n",
    "            output = {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Validate JSON serialization\n",
    "            json.dumps(output)\n",
    "            print(f\"  âœ… Output validated successfully\")\n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Validation error: {str(e)}\")\n",
    "            return {\n",
    "                \"input\": user_story,\n",
    "                \"output\": {\n",
    "                    \"story_points\": total_story_points,\n",
    "                    \"tasks\": tasks_data,\n",
    "                    \"validation_error\": str(e)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "class SimpleGraphVisualizer:\n",
    "    \"\"\"Simple visualizer that only interprets JSON results without adding extra attributes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_dependency_graph(self, pipeline_results: List[Dict[str, Any]]):\n",
    "        \"\"\"Create a simple dependency graph from JSON results\"\"\"\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            import plotly.graph_objects as go\n",
    "            import math\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸ Could not import visualization libraries.\")\n",
    "            print(\"ðŸ’¡ Please install: pip install plotly networkx\")\n",
    "            return None\n",
    "        \n",
    "        # Create NetworkX graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Extract all tasks from JSON results\n",
    "        all_tasks = []\n",
    "        task_id_to_idx = {}\n",
    "        \n",
    "        for result in pipeline_results:\n",
    "            if 'error' in result['output']:\n",
    "                continue\n",
    "                \n",
    "            for task_data in result['output']['tasks']:\n",
    "                task_idx = len(all_tasks)\n",
    "                all_tasks.append(task_data)\n",
    "                task_id_to_idx[task_data['id']] = task_idx\n",
    "                \n",
    "                # Add node with only JSON attributes\n",
    "                G.add_node(task_idx, \n",
    "                          task_id=task_data['id'],\n",
    "                          description=task_data['description'],\n",
    "                          story_points=task_data['story_points'],\n",
    "                          user_stories=\"; \".join(task_data['user_stories']),\n",
    "                          required_skills=\", \".join(task_data['required_skills']))\n",
    "        \n",
    "        # Add edges from dependencies in JSON\n",
    "        edge_data = []\n",
    "        for task_idx, task_data in enumerate(all_tasks):\n",
    "            for dep in task_data['depends_on']:\n",
    "                dep_task_id = dep['task_id']\n",
    "                if dep_task_id in task_id_to_idx:\n",
    "                    dep_idx = task_id_to_idx[dep_task_id]\n",
    "                    rework_effort = dep['rework_effort']\n",
    "                    \n",
    "                    G.add_edge(dep_idx, task_idx, rework_effort=rework_effort)\n",
    "                    \n",
    "                    edge_data.append({\n",
    "                        'from_idx': dep_idx,\n",
    "                        'to_idx': task_idx,\n",
    "                        'from_task': all_tasks[dep_idx]['description'],\n",
    "                        'to_task': task_data['description'],\n",
    "                        'rework_effort': rework_effort\n",
    "                    })\n",
    "        \n",
    "        return self._create_plotly_graph(G, edge_data)\n",
    "    \n",
    "    def _create_plotly_graph(self, G, edge_data):\n",
    "        \"\"\"Create Plotly visualization\"\"\"\n",
    "        import networkx as nx\n",
    "        import plotly.graph_objects as go\n",
    "        import math\n",
    "        \n",
    "        # Layout\n",
    "        try:\n",
    "            pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "        except:\n",
    "            pos = nx.circular_layout(G)\n",
    "        \n",
    "        # Create arrow traces\n",
    "        arrow_traces = []\n",
    "        midpoint_traces = []\n",
    "        \n",
    "        for edge_info in edge_data:\n",
    "            from_idx = edge_info['from_idx']\n",
    "            to_idx = edge_info['to_idx']\n",
    "            \n",
    "            x0, y0 = pos[from_idx]\n",
    "            x1, y1 = pos[to_idx]\n",
    "            \n",
    "            # Calculate arrow\n",
    "            dx = x1 - x0\n",
    "            dy = y1 - y0\n",
    "            length = math.sqrt(dx**2 + dy**2)\n",
    "            \n",
    "            if length == 0:\n",
    "                continue\n",
    "            \n",
    "            # Normalize direction\n",
    "            dx_norm = dx / length\n",
    "            dy_norm = dy / length\n",
    "            \n",
    "            # Offset points\n",
    "            offset = 0.05\n",
    "            start_x = x0 + dx_norm * offset\n",
    "            start_y = y0 + dy_norm * offset\n",
    "            end_x = x1 - dx_norm * offset\n",
    "            end_y = y1 - dy_norm * offset\n",
    "            \n",
    "            # Arrow line\n",
    "            arrow_trace = go.Scatter(\n",
    "                x=[start_x, end_x, None],\n",
    "                y=[start_y, end_y, None],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color='gray'),\n",
    "                hoverinfo='skip',\n",
    "                showlegend=False\n",
    "            )\n",
    "            arrow_traces.append(arrow_trace)\n",
    "            \n",
    "            # Arrowhead\n",
    "            arrow_size = 0.02\n",
    "            perp_x = -dy_norm * arrow_size\n",
    "            perp_y = dx_norm * arrow_size\n",
    "            \n",
    "            arrowhead_x = [\n",
    "                end_x,\n",
    "                end_x - dx_norm * arrow_size * 2 + perp_x,\n",
    "                end_x - dx_norm * arrow_size * 2 - perp_x,\n",
    "                end_x,\n",
    "                None\n",
    "            ]\n",
    "            arrowhead_y = [\n",
    "                end_y,\n",
    "                end_y - dy_norm * arrow_size * 2 + perp_y,\n",
    "                end_y - dy_norm * arrow_size * 2 - perp_y,\n",
    "                end_y,\n",
    "                None\n",
    "            ]\n",
    "            \n",
    "            arrowhead_trace = go.Scatter(\n",
    "                x=arrowhead_x,\n",
    "                y=arrowhead_y,\n",
    "                mode='lines',\n",
    "                fill='toself',\n",
    "                fillcolor='gray',\n",
    "                line=dict(width=1, color='gray'),\n",
    "                hoverinfo='skip',\n",
    "                showlegend=False\n",
    "            )\n",
    "            arrow_traces.append(arrowhead_trace)\n",
    "            \n",
    "            # Midpoint circle for dependency info\n",
    "            mid_x = (start_x + end_x) / 2\n",
    "            mid_y = (start_y + end_y) / 2\n",
    "            \n",
    "            hover_text = (\n",
    "                f\"<b>Dependency:</b> {edge_info['from_task']} â†’ {edge_info['to_task']}<br>\" +\n",
    "                f\"<b>rework Effort:</b> {edge_info['rework_effort']}\"\n",
    "            )\n",
    "            \n",
    "            midpoint_trace = go.Scatter(\n",
    "                x=[mid_x],\n",
    "                y=[mid_y],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color='orange',\n",
    "                    line=dict(width=2, color='white'),\n",
    "                    opacity=0.8\n",
    "                ),\n",
    "                hoverinfo='text',\n",
    "                hovertext=hover_text,\n",
    "                showlegend=False,\n",
    "                name='Dependencies'\n",
    "            )\n",
    "            midpoint_traces.append(midpoint_trace)\n",
    "        \n",
    "        # Create node trace\n",
    "        node_x = [pos[node][0] for node in G.nodes()]\n",
    "        node_y = [pos[node][1] for node in G.nodes()]\n",
    "        node_labels = [f\"{G.nodes[node]['task_id']}\" for node in G.nodes()]\n",
    "        node_sizes = [max(20, min(50, 20 + G.nodes[node]['story_points'] * 3)) for node in G.nodes()]\n",
    "        \n",
    "        node_hover = []\n",
    "        for node in G.nodes():\n",
    "            node_data = G.nodes[node]\n",
    "            hover_text = (f\"<b>Task:</b> {node_data['description']}<br>\" +\n",
    "                         f\"<b>Story Points:</b> {node_data['story_points']}<br>\" +\n",
    "                         f\"<b>User Stories:</b> {node_data['user_stories']}<br>\" +\n",
    "                         f\"<b>Skills:</b> {node_data['required_skills']}\")\n",
    "            node_hover.append(hover_text)\n",
    "        \n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=node_sizes, color='lightblue',\n",
    "                       line=dict(width=2, color='white'), opacity=0.9),\n",
    "            text=node_labels,\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=8, color='black', family='Arial'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=node_hover,\n",
    "            name='Tasks'\n",
    "        )\n",
    "        \n",
    "        # Create figure\n",
    "        all_traces = [node_trace] + arrow_traces + midpoint_traces\n",
    "        fig = go.Figure(data=all_traces)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=\"ðŸ”— Task Dependencies Graph\",\n",
    "                x=0.5,\n",
    "                font=dict(size=20, family='Arial', color='#2C3E50')\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20,l=5,r=5,t=60),\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            plot_bgcolor='rgba(248,249,250,0.8)',\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "async def process_user_story_pipeline(user_story: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single user story through the hybrid pipeline\"\"\"\n",
    "    print(f\"\\nðŸ”„ Processing: {user_story[:60]}...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Task Extraction using Self-Consistency\n",
    "        print(\"ðŸ“ Step 1: Extracting tasks with Self-Consistency...\")\n",
    "        extractor = TaskExtractorAgent()\n",
    "        tasks = await extractor.decompose(user_story, num_samples=3)\n",
    "        \n",
    "        if not tasks:\n",
    "            raise ValueError(\"No tasks extracted from user story\")\n",
    "        \n",
    "        # Step 2: Story Points using Few-Shot\n",
    "        print(\"ðŸ“Š Step 2: Estimating story points with Few-Shot examples...\")\n",
    "        estimator = StoryPointEstimatorAgent()\n",
    "        story_points_results = await estimator.estimate_story_points(user_story, tasks)\n",
    "        story_points = story_points_results['task_points']\n",
    "        \n",
    "        # Step 3: Skills using Zero-Shot (parallel with dependencies)\n",
    "        print(\"ðŸ› ï¸ Step 3: Identifying skills with Zero-Shot...\")\n",
    "        skills_agent = RequiredSkillsAgent()\n",
    "        \n",
    "        # Step 4: Dependencies using Self-Consistency (parallel with skills)\n",
    "        print(\"ðŸ”— Step 4: Analyzing dependencies with Self-Consistency...\")\n",
    "        dependency_agent = DependencyAgent()\n",
    "        \n",
    "        # Run skills and dependencies in parallel\n",
    "        skills, dependencies = await asyncio.gather(\n",
    "            skills_agent.identify_skills(user_story, tasks),\n",
    "            dependency_agent.analyze_dependencies(user_story, tasks, story_points)\n",
    "        )\n",
    "        \n",
    "        # Step 5: Format and Validate\n",
    "        print(\"ðŸ“‹ Step 5: Formatting and validating output...\")\n",
    "        \n",
    "        # Create task ID mapping\n",
    "        task_to_id = {}\n",
    "        for i, task in enumerate(tasks):\n",
    "            task_to_id[task] = f\"T_{i+1:03d}\"\n",
    "        \n",
    "        # Build task data structure\n",
    "        tasks_data = []\n",
    "        total_story_points = sum(story_points.values())\n",
    "        \n",
    "        for i, task in enumerate(tasks):\n",
    "            task_id = f\"T_{i+1:03d}\"\n",
    "            \n",
    "            # Get dependencies for this task and convert task descriptions to task IDs\n",
    "            task_dependencies = []\n",
    "            if task in dependencies:\n",
    "                for dep in dependencies[task]:\n",
    "                    # Convert task description to task ID\n",
    "                    dep_task_desc = dep[\"task_id\"]\n",
    "                    dep_task_id = task_to_id.get(dep_task_desc, dep_task_desc)  # fallback to description if not found\n",
    "                    \n",
    "                    task_dependencies.append({\n",
    "                        \"task_id\": dep_task_id,\n",
    "                        \"rework_effort\": dep[\"rework_effort\"]\n",
    "                    })\n",
    "            \n",
    "            task_data = {\n",
    "                \"description\": task,\n",
    "                \"id\": task_id,\n",
    "                \"user_stories\": [user_story],  # Add user_stories field for single story processing\n",
    "                \"story_points\": story_points.get(task, 3),\n",
    "                \"depends_on\": task_dependencies,\n",
    "                \"required_skills\": skills.get(task, [\"general_development\"])\n",
    "            }\n",
    "            tasks_data.append(task_data)\n",
    "        \n",
    "        \n",
    "        validator = FormatValidator()\n",
    "        result = validator.validate_and_format(user_story, tasks_data, total_story_points)\n",
    "        \n",
    "        print(\"ðŸŽ‰ Story processing complete!\")\n",
    "        print(\"=\"*80)\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing user story: {str(e)}\")\n",
    "        return {\n",
    "            \"input\": user_story,\n",
    "            \"output\": {\n",
    "                \"error\": str(e),\n",
    "                \"story_points\": 0,\n",
    "                \"tasks\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "async def process_multiple_user_stories_pipeline(user_stories: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple user stories through the hybrid pipeline with task merging\"\"\"\n",
    "    print(f\"\\nðŸš€ Starting Hybrid Multi-Agent Pipeline with Task Merging\")\n",
    "    print(f\"ðŸ“Š Processing {len(user_stories)} user stories...\")\n",
    "    print(\"ðŸ”§ Techniques: Self-Consistency (Tasks & Dependencies) + Few-Shot (Story Points) + Zero-Shot (Skills) + Semantic Merging\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reset token tracker\n",
    "    global token_tracker\n",
    "    token_tracker = TokenTracker()\n",
    "    \n",
    "    # Process all stories individually first\n",
    "    individual_results = []\n",
    "    for i, story in enumerate(user_stories, 1):\n",
    "        print(f\"\\nðŸ“– Story {i}/{len(user_stories)}\")\n",
    "        result = await process_user_story_pipeline(story)\n",
    "        individual_results.append(result)\n",
    "    \n",
    "    print(f\"\\nðŸ”— Step 6: Merging similar tasks across user stories...\")\n",
    "    \n",
    "    # Merge similar tasks across all stories\n",
    "    merger = TaskMergerAgent(similarity_threshold=0.7)\n",
    "    merged_results = merger.merge_similar_tasks(individual_results)\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline completed! Processed {len(user_stories)} stories with cross-story task merging\")\n",
    "    return merged_results\n",
    "\n",
    "\n",
    "def print_token_usage():\n",
    "    \"\"\"Print comprehensive token usage statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOKEN USAGE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary = token_tracker.get_summary()\n",
    "    breakdown = summary['breakdown']\n",
    "    cost_estimate = summary['cost_estimate']\n",
    "    efficiency = summary['efficiency_metrics']\n",
    "    \n",
    "    print(f\"TOTAL TOKENS CONSUMED: {breakdown['total_consumed']:,}\")\n",
    "    print(f\"ESTIMATED COST: ${cost_estimate['total_cost']:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"BREAKDOWN BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    categories = ['task_extraction', 'story_point_estimation', 'required_skills', 'dependency_analysis', 'format_validation']\n",
    "    \n",
    "    for category in categories:\n",
    "        if category in breakdown:\n",
    "            cat_data = breakdown[category]\n",
    "            percentage = efficiency['percentage_breakdown'].get(category, 0)\n",
    "            print(f\"{category.replace('_', ' ').title():<25}: {cat_data['total']:>6,} tokens ({percentage:>5.1f}%)\")\n",
    "            print(f\"  {'Input':<23}: {cat_data['input']:>6,} tokens\")\n",
    "            print(f\"  {'Output':<23}: {cat_data['output']:>6,} tokens\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"INPUT COST:  ${cost_estimate['input_cost']:.6f}\")\n",
    "    print(f\"OUTPUT COST: ${cost_estimate['output_cost']:.6f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    print(\"Enter user stories (one per line, press Enter twice to finish):\")\n",
    "    user_stories = []\n",
    "    while True:\n",
    "        story = input().strip()\n",
    "        if not story:\n",
    "            if user_stories:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter at least one user story\")\n",
    "                continue\n",
    "        user_stories.append(story)\n",
    "    \n",
    "    # Process through pipeline\n",
    "    results = await process_multiple_user_stories_pipeline(user_stories)\n",
    "    \n",
    "    # FIRST: Output clean JSON results\n",
    "    print(\"\\nRESULTS:\")\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print()\n",
    "    \n",
    "    # Print token usage\n",
    "    print_token_usage()\n",
    "    \n",
    "    # THEN: Create and show the graph\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¨ GENERATING DEPENDENCY GRAPH FROM JSON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Initialize simple visualizer\n",
    "        visualizer = SimpleGraphVisualizer()\n",
    "        \n",
    "        # Generate graph directly from JSON results\n",
    "        print(\"ðŸ“Š Creating dependency graph from JSON results...\")\n",
    "        dependency_graph = visualizer.create_dependency_graph(results)\n",
    "        \n",
    "        if dependency_graph:\n",
    "            dependency_graph.show()\n",
    "            print(\"âœ… Dependency graph opened in browser!\")\n",
    "            \n",
    "            # Optionally save the graph\n",
    "            save_graph = input(\"\\nðŸ’¾ Save graph as HTML file? (y/n): \").lower().strip()\n",
    "            if save_graph == 'y':\n",
    "                dependency_graph.write_html(\"dependency_graph.html\")\n",
    "                print(\"âœ… Graph saved as 'dependency_graph.html'\")\n",
    "        else:\n",
    "            print(\"âŒ Could not create graph\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error creating visualization: {e}\")\n",
    "        print(\"ðŸ“‹ JSON results are still available above\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# CONVENIENCE FUNCTIONS\n",
    "async def process_stories_interactive(user_stories: List[str]):\n",
    "    \"\"\"Interactive function for Jupyter notebooks\"\"\"\n",
    "    # Process through pipeline\n",
    "    results = await process_multiple_user_stories_pipeline(user_stories)\n",
    "    \n",
    "    # Output JSON first\n",
    "    print(\"\\nRESULTS:\")\n",
    "    for result in results:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        print()\n",
    "    \n",
    "    # Print token usage\n",
    "    print_token_usage()\n",
    "    \n",
    "    # Create graph\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¨ GENERATING DEPENDENCY GRAPH FROM JSON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        visualizer = SimpleGraphVisualizer()\n",
    "        dependency_graph = visualizer.create_dependency_graph(results)\n",
    "        \n",
    "        if dependency_graph:\n",
    "            dependency_graph.show()\n",
    "            dependency_graph.write_html(\"dependency_graph.html\")\n",
    "            print(\"âœ… Graph created and saved!\")\n",
    "        \n",
    "        return results, dependency_graph\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error creating visualization: {e}\")\n",
    "        return results, None\n",
    "\n",
    "\n",
    "def create_graph_from_json(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Create graph from existing JSON results\"\"\"\n",
    "    try:\n",
    "        visualizer = SimpleGraphVisualizer()\n",
    "        return visualizer.create_dependency_graph(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating graph: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    \"\"\"Check if code is running in an interactive environment\"\"\"\n",
    "    try:\n",
    "        __IPYTHON__\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not is_interactive():\n",
    "        asyncio.run(main())\n",
    "    else:\n",
    "        print(\"ðŸ”” Interactive environment detected!\")\n",
    "        print(\"ðŸ’¡ Use one of these methods:\")\n",
    "        print(\"1. await process_stories_interactive(['Your user story 1', 'Your user story 2'])\")\n",
    "        print(\"2. results = await process_multiple_user_stories_pipeline(['Your stories'])\")\n",
    "        print(\"3. graph = create_graph_from_json(results)  # If you have existing results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601fa36a-0702-4f73-a99c-0bd7e3ecc037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting Hybrid Multi-Agent Pipeline with Task Merging\n",
      "ðŸ“Š Processing 2 user stories...\n",
      "ðŸ”§ Techniques: Self-Consistency (Tasks & Dependencies) + Few-Shot (Story Points) + Zero-Shot (Skills) + Semantic Merging\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Story 1/2\n",
      "\n",
      "ðŸ”„ Processing: As a user, I want to search for products...\n",
      "================================================================================\n",
      "ðŸ“ Step 1: Extracting tasks with Self-Consistency...\n",
      "[TASK_EXTRACTION] Tokens - Input: 501, Output: 59, Total: 560\n",
      "[TASK_EXTRACTION] Tokens - Input: 501, Output: 59, Total: 560\n",
      "[TASK_EXTRACTION] Tokens - Input: 501, Output: 59, Total: 560\n",
      "âœ“ Extracted 7 tasks using self-consistency\n",
      "ðŸ“Š Step 2: Estimating story points with Few-Shot examples...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 380, Output: 65, Total: 445\n",
      "âœ“ Estimated story points for 7 tasks\n",
      "ðŸ› ï¸ Step 3: Identifying skills with Zero-Shot...\n",
      "ðŸ”— Step 4: Analyzing dependencies with Self-Consistency...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 188, Output: 148, Total: 336\n",
      "âœ“ Identified skills for 7 tasks in 1 API call\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 459, Output: 153, Total: 612\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 459, Output: 131, Total: 590\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 459, Output: 131, Total: 590\n",
      "âœ“ Analyzed dependencies for 6 tasks\n",
      "ðŸ“‹ Step 5: Formatting and validating output...\n",
      "âœ… Validating and formatting final output...\n",
      "  âœ… Output validated successfully\n",
      "ðŸŽ‰ Story processing complete!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Story 2/2\n",
      "\n",
      "ðŸ”„ Processing: As an admin, I want to manage inventory...\n",
      "================================================================================\n",
      "ðŸ“ Step 1: Extracting tasks with Self-Consistency...\n",
      "[TASK_EXTRACTION] Tokens - Input: 500, Output: 48, Total: 548\n",
      "[TASK_EXTRACTION] Tokens - Input: 500, Output: 48, Total: 548\n",
      "[TASK_EXTRACTION] Tokens - Input: 500, Output: 48, Total: 548\n",
      "âœ“ Extracted 5 tasks using self-consistency\n",
      "ðŸ“Š Step 2: Estimating story points with Few-Shot examples...\n",
      "[STORY_POINT_ESTIMATION] Tokens - Input: 368, Output: 49, Total: 417\n",
      "âœ“ Estimated story points for 5 tasks\n",
      "ðŸ› ï¸ Step 3: Identifying skills with Zero-Shot...\n",
      "ðŸ”— Step 4: Analyzing dependencies with Self-Consistency...\n",
      "[REQUIRED_SKILLS] Tokens - Input: 176, Output: 111, Total: 287\n",
      "âœ“ Identified skills for 5 tasks in 1 API call\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 441, Output: 97, Total: 538\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 441, Output: 97, Total: 538\n",
      "[DEPENDENCY_ANALYSIS] Tokens - Input: 441, Output: 97, Total: 538\n",
      "âœ“ Analyzed dependencies for 4 tasks\n",
      "ðŸ“‹ Step 5: Formatting and validating output...\n",
      "âœ… Validating and formatting final output...\n",
      "  âœ… Output validated successfully\n",
      "ðŸŽ‰ Story processing complete!\n",
      "================================================================================\n",
      "\n",
      "ðŸ”— Step 6: Merging similar tasks across user stories...\n",
      "ðŸ”„ Merging similar tasks across 2 user stories...\n",
      "  ðŸ“Š No similar tasks found (threshold: 0.7)\n",
      "âœ“ Merged 12 tasks into 12 unique tasks\n",
      "\n",
      "âœ… Pipeline completed! Processed 2 stories with cross-story task merging\n",
      "\n",
      "RESULTS:\n",
      "{\n",
      "  \"input\": \"As a user, I want to search for products\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 24,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Create search input field\",\n",
      "        \"id\": \"T_001\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 1,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"algorithm_design\",\n",
      "          \"backend_development\",\n",
      "          \"data_structures\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement search query parsing\",\n",
      "        \"id\": \"T_002\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"algorithm_design\",\n",
      "          \"backend_development\",\n",
      "          \"data_structures\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Design search results display\",\n",
      "        \"id\": \"T_003\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"algorithm_design\",\n",
      "          \"backend_development\",\n",
      "          \"data_structures\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Build product indexing system\",\n",
      "        \"id\": \"T_004\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"ui_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Test search functionality\",\n",
      "        \"id\": \"T_005\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 2,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"testing\",\n",
      "          \"unit_testing\",\n",
      "          \"automation_testing\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Add filtering and sorting options\",\n",
      "        \"id\": \"T_006\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Add filtering and sorting options\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement search result pagination\",\n",
      "        \"id\": \"T_007\",\n",
      "        \"user_stories\": [\n",
      "          \"As a user, I want to search for products\"\n",
      "        ],\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_005\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"algorithm_design\",\n",
      "          \"backend_development\",\n",
      "          \"data_structures\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "{\n",
      "  \"input\": \"As an admin, I want to manage inventory\",\n",
      "  \"output\": {\n",
      "    \"story_points\": 24,\n",
      "    \"tasks\": [\n",
      "      {\n",
      "        \"description\": \"Design inventory management interface\",\n",
      "        \"id\": \"T_008\",\n",
      "        \"user_stories\": [\n",
      "          \"As an admin, I want to manage inventory\"\n",
      "        ],\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"ui_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Implement inventory CRUD operations\",\n",
      "        \"id\": \"T_009\",\n",
      "        \"user_stories\": [\n",
      "          \"As an admin, I want to manage inventory\"\n",
      "        ],\n",
      "        \"story_points\": 8,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_001\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Create stock level tracking system\",\n",
      "        \"id\": \"T_010\",\n",
      "        \"user_stories\": [\n",
      "          \"As an admin, I want to manage inventory\"\n",
      "        ],\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"general_development\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Build inventory history logging\",\n",
      "        \"id\": \"T_011\",\n",
      "        \"user_stories\": [\n",
      "          \"As an admin, I want to manage inventory\"\n",
      "        ],\n",
      "        \"story_points\": 5,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_002\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"frontend_development\",\n",
      "          \"javascript\",\n",
      "          \"ui_design\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"description\": \"Set up low stock alert notifications\",\n",
      "        \"id\": \"T_012\",\n",
      "        \"user_stories\": [\n",
      "          \"As an admin, I want to manage inventory\"\n",
      "        ],\n",
      "        \"story_points\": 3,\n",
      "        \"depends_on\": [\n",
      "          {\n",
      "            \"task_id\": \"T_003\",\n",
      "            \"rework_effort\": 2\n",
      "          },\n",
      "          {\n",
      "            \"task_id\": \"T_004\",\n",
      "            \"rework_effort\": 2\n",
      "          }\n",
      "        ],\n",
      "        \"required_skills\": [\n",
      "          \"Set up low stock alert notifications\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOKEN USAGE SUMMARY\n",
      "================================================================================\n",
      "TOTAL TOKENS CONSUMED: 8,215\n",
      "ESTIMATED COST: $0.096150\n",
      "\n",
      "BREAKDOWN BY CATEGORY:\n",
      "--------------------------------------------------\n",
      "Task Extraction          :  3,324 tokens ( 40.5%)\n",
      "  Input                  :  3,003 tokens\n",
      "  Output                 :    321 tokens\n",
      "\n",
      "Story Point Estimation   :    862 tokens ( 10.5%)\n",
      "  Input                  :    748 tokens\n",
      "  Output                 :    114 tokens\n",
      "\n",
      "Required Skills          :    623 tokens (  7.6%)\n",
      "  Input                  :    364 tokens\n",
      "  Output                 :    259 tokens\n",
      "\n",
      "Dependency Analysis      :  3,406 tokens ( 41.5%)\n",
      "  Input                  :  2,700 tokens\n",
      "  Output                 :    706 tokens\n",
      "\n",
      "Format Validation        :      0 tokens (  0.0%)\n",
      "  Input                  :      0 tokens\n",
      "  Output                 :      0 tokens\n",
      "\n",
      "INPUT COST:  $0.068150\n",
      "OUTPUT COST: $0.028000\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¨ GENERATING DEPENDENCY GRAPH FROM JSON RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "<b>Task:</b> Create search input field<br><b>Story Points:</b> 1<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> algorithm_design, backend_development, data_structures",
          "<b>Task:</b> Implement search query parsing<br><b>Story Points:</b> 3<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> algorithm_design, backend_development, data_structures",
          "<b>Task:</b> Design search results display<br><b>Story Points:</b> 2<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> algorithm_design, backend_development, data_structures",
          "<b>Task:</b> Build product indexing system<br><b>Story Points:</b> 8<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> frontend_development, javascript, ui_design",
          "<b>Task:</b> Test search functionality<br><b>Story Points:</b> 2<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> testing, unit_testing, automation_testing",
          "<b>Task:</b> Add filtering and sorting options<br><b>Story Points:</b> 5<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> Add filtering and sorting options",
          "<b>Task:</b> Implement search result pagination<br><b>Story Points:</b> 3<br><b>User Stories:</b> As a user, I want to search for products<br><b>Skills:</b> algorithm_design, backend_development, data_structures",
          "<b>Task:</b> Design inventory management interface<br><b>Story Points:</b> 3<br><b>User Stories:</b> As an admin, I want to manage inventory<br><b>Skills:</b> frontend_development, javascript, ui_design",
          "<b>Task:</b> Implement inventory CRUD operations<br><b>Story Points:</b> 8<br><b>User Stories:</b> As an admin, I want to manage inventory<br><b>Skills:</b> general_development",
          "<b>Task:</b> Create stock level tracking system<br><b>Story Points:</b> 5<br><b>User Stories:</b> As an admin, I want to manage inventory<br><b>Skills:</b> general_development",
          "<b>Task:</b> Build inventory history logging<br><b>Story Points:</b> 5<br><b>User Stories:</b> As an admin, I want to manage inventory<br><b>Skills:</b> frontend_development, javascript, ui_design",
          "<b>Task:</b> Set up low stock alert notifications<br><b>Story Points:</b> 3<br><b>User Stories:</b> As an admin, I want to manage inventory<br><b>Skills:</b> Set up low stock alert notifications"
         ],
         "marker": {
          "color": "lightblue",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.9,
          "size": [
           23,
           29,
           26,
           44,
           26,
           35,
           29,
           29,
           44,
           35,
           35,
           29
          ]
         },
         "mode": "markers+text",
         "name": "Tasks",
         "text": [
          "T_001",
          "T_002",
          "T_003",
          "T_004",
          "T_005",
          "T_006",
          "T_007",
          "T_008",
          "T_009",
          "T_010",
          "T_011",
          "T_012"
         ],
         "textfont": {
          "color": "black",
          "family": "Arial",
          "size": 8
         },
         "textposition": "middle center",
         "type": "scatter",
         "x": [
          0.24542586164595356,
          0.9842971047367718,
          -0.7561138155938445,
          -0.9541510726461303,
          0.7019721047173947,
          -0.6479188194881682,
          1,
          -0.29551190441591113,
          -0.2123885374159687,
          0.1835520230823712,
          0.635024289296164,
          -0.8841872339186324
         ],
         "y": [
          0.9985309349642245,
          0.09960866316548259,
          -0.687467904708331,
          0.38332452982361576,
          0.6814845711929972,
          0.8801184126431637,
          -0.3923176144261092,
          -0.9011426894517356,
          0.84232606656104,
          -0.9162902775513198,
          -0.814334426917131,
          -0.17384026529589708
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.277174910927063,
          0.9525480554556623,
          null
         ],
         "y": [
          0.9599045467076752,
          0.13823505142203182,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.9525480554556623,
          0.9425993713333946,
          0.9116982607281551,
          0.9525480554556623,
          null
         ],
         "y": [
          0.13823505142203182,
          0.18183578173971499,
          0.15643654231482745,
          0.13823505142203182,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.9387392299653801,
          -0.7105559408224528,
          null
         ],
         "y": [
          0.0790057496260293,
          -0.6668649911688777,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.7105559408224528,
          -0.665868475589558,
          -0.6823508064211207,
          -0.7105559408224528,
          null
         ],
         "y": [
          -0.6668649911688777,
          -0.6686058102458717,
          -0.6321595104287584,
          -0.6668649911688777,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.9348241997193172,
          -0.9046781676286757,
          null
         ],
         "y": [
          0.10684963472556472,
          0.3760835582635336,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.9046781676286757,
          -0.8679962322387449,
          -0.8622034549906792,
          -0.9046781676286757,
          null
         ],
         "y": [
          0.3760835582635336,
          0.3505016190084861,
          0.39007994302244975,
          0.3760835582635336,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.9049422062707405,
          0.6527632383420049,
          null
         ],
         "y": [
          0.39218384526269073,
          0.6726252557539223,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.6527632383420049,
          0.609852419066063,
          0.6169398714173231,
          0.6527632383420049,
          null
         ],
         "y": [
          0.6726252557539223,
          0.6852213499528182,
          0.6458542568525064,
          0.6726252557539223,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.7196618745973529,
          0.6655201637209031,
          null
         ],
         "y": [
          -0.6532442871673945,
          0.6472609536520607,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.6655201637209031,
          0.6226691639073352,
          0.6500480579400845,
          0.6655201637209031,
          null
         ],
         "y": [
          0.6472609536520607,
          0.6344628360179082,
          0.6053012832207149,
          0.6472609536520607,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.6525047836129733,
          -0.5984514983837468,
          null
         ],
         "y": [
          0.6887635919205444,
          0.8728393919156165,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.5984514983837468,
          -0.5617892497912286,
          -0.5559660332091908,
          -0.5984514983837468,
          null
         ],
         "y": [
          0.8728393919156165,
          0.8472292468918102,
          0.8868031037753473,
          0.8728393919156165,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.7153438638307267,
          0.986628240886668,
          null
         ],
         "y": [
          0.6333057785740966,
          -0.3441388218072087,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.986628240886668,
          0.9952023506435627,
          0.9566593165484422,
          0.986628240886668,
          null
         ],
         "y": [
          -0.3441388218072087,
          -0.30024708406675543,
          -0.310944491357421,
          -0.3441388218072087,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.19810451386577146,
          -0.1650671896357866,
          null
         ],
         "y": [
          0.9823850362816781,
          0.8584719652435865,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.1650671896357866,
          -0.12075175193862234,
          -0.13366847088465947,
          -0.1650671896357866,
          null
         ],
         "y": [
          0.8584719652435865,
          0.8524601450775509,
          0.8903172233016965,
          0.8584719652435865,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.9533453887841606,
          0.21450373903498235,
          null
         ],
         "y": [
          0.060340466369743696,
          -0.8770220807555809,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.21450373903498235,
          0.2549723905153668,
          0.22355783307877572,
          0.21450373903498235,
          null
         ],
         "y": [
          -0.8770220807555809,
          -0.8579882097000342,
          -0.8332268369379452,
          -0.8770220807555809,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.9664480792730753,
          0.6528733147598604,
          null
         ],
         "y": [
          0.05290307153025849,
          -0.7676288352819068,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.6528733147598604,
          0.6858347717849073,
          0.648470298476728,
          0.6528733147598604,
          null
         ],
         "y": [
          -0.7676288352819068,
          -0.7374039721592062,
          -0.723124751788249,
          -0.7676288352819068,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.7682109476806651,
          -0.8720901018318118,
          null
         ],
         "y": [
          -0.6389533770359521,
          -0.22235479296827595,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.8720901018318118,
          -0.8818182072313069,
          -0.8430065850934038,
          -0.8720901018318118,
          null
         ],
         "y": [
          -0.22235479296827595,
          -0.2660052679409073,
          -0.2563275622714508,
          -0.22235479296827595,
          null
         ]
        },
        {
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.9479214365552809,
          -0.8904168700094818,
          null
         ],
         "y": [
          0.3337141313755526,
          -0.12422986684783392,
          null
         ]
        },
        {
         "fill": "toself",
         "fillcolor": "gray",
         "hoverinfo": "skip",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.8904168700094818,
          -0.8755564195029361,
          -0.9152447382613865,
          -0.8904168700094818,
          null
         ],
         "y": [
          -0.12422986684783392,
          -0.08204969365304365,
          -0.08703340252572314,
          -0.12422986684783392,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Create search input field â†’ Implement search query parsing<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.6148614831913627
         ],
         "y": [
          0.5490697990648535
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Implement search query parsing â†’ Design search results display<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.11409164457146365
         ],
         "y": [
          -0.29392962077142415
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Implement search query parsing â†’ Build product indexing system<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.015073016045320742
         ],
         "y": [
          0.24146659649454916
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Build product indexing system â†’ Test search functionality<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.1260894839643678
         ],
         "y": [
          0.5324045505083065
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Design search results display â†’ Test search functionality<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.027070855438224906
         ],
         "y": [
          -0.0029916667576668776
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Test search functionality â†’ Add filtering and sorting options<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.02702664261461324
         ],
         "y": [
          0.7808014919180805
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Test search functionality â†’ Implement search result pagination<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.8509860523586974
         ],
         "y": [
          0.144583478383444
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Create search input field â†’ Implement inventory CRUD operations<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.016518662114992436
         ],
         "y": [
          0.9204285007626323
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Implement search query parsing â†’ Create stock level tracking system<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.5839245639095715
         ],
         "y": [
          -0.4083408071929186
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Implement search query parsing â†’ Build inventory history logging<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.8096606970164679
         ],
         "y": [
          -0.35736288187582416
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Design search results display â†’ Set up low stock alert notifications<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.8201505247562384
         ],
         "y": [
          -0.43065408500211405
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": "<b>Dependency:</b> Build product indexing system â†’ Set up low stock alert notifications<br><b>rework Effort:</b> 2",
         "marker": {
          "color": "orange",
          "line": {
           "color": "white",
           "width": 2
          },
          "opacity": 0.8,
          "size": 12
         },
         "mode": "markers",
         "name": "Dependencies",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.9191691532823814
         ],
         "y": [
          0.10474213226385934
         ]
        }
       ],
       "layout": {
        "hovermode": "closest",
        "margin": {
         "b": 20,
         "l": 5,
         "r": 5,
         "t": 60
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "rgba(248,249,250,0.8)",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "#2C3E50",
          "family": "Arial",
          "size": 20
         },
         "text": "ðŸ”— Task Dependencies Graph",
         "x": 0.5
        },
        "xaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        },
        "yaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        }
       }
      },
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph created and saved!\n"
     ]
    }
   ],
   "source": [
    "results, graph = await process_stories_interactive([\n",
    "    \"As a user, I want to search for products\",\n",
    "    \"As an admin, I want to manage inventory\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522b791-9483-47c8-a73e-8ed8e5302b14",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "\n",
    "> **Groq API:** Large Language Model provider using llama3-70b-8192 model for all AI-powered analysis  \n",
    "> **tiktoken:** Token estimation using cl100k_base encoding for cost tracking and optimization  \n",
    "> **python-dotenv:** Environment variable management for secure API key handling  \n",
    "> **asyncio:** Asynchronous programming support for parallel processing of pipeline stages  \n",
    "> **numpy:** Numerical operations for similarity calculations in task merging algorithm  \n",
    "> **sklearn:** Machine learning utilities (TF-IDF vectorization, cosine similarity) for semantic analysis  \n",
    "> **networkx:** Graph creation and manipulation for dependency visualization  \n",
    "> **plotly:** Interactive visualization of task dependency graphs with hover details  \n",
    "\n",
    "#### Main Classes\n",
    "\n",
    "> **TokenTracker:** Monitors API usage across all pipeline stages, tracks input/output tokens per operation (task_extraction, story_point_estimation, required_skills, dependency_analysis, format_validation), estimates costs using configurable rates, and provides efficiency metrics with percentage breakdowns using tiktoken's cl100k_base encoding.\n",
    "\n",
    "> **TaskExtractorAgent:** Decomposes user stories into atomic, actionable tasks (maximum 8 per story) using self-consistency approach with 3 samples at different temperatures (0.2, 0.4, 0.6). Applies task cleaning to remove explanatory text and convert to imperative form. Key method: `decompose(user_story, num_samples=3) -> List[str]`\n",
    "\n",
    "> **StoryPointEstimatorAgent:** Estimates task complexity using Fibonacci sequence [1,2,3,5,8,13] with few-shot learning from curated examples. Considers complexity, time, risk, and uncertainty factors. Key method: `estimate_story_points(user_story, tasks) -> Dict` returns `{'total_story_points': int, 'task_points': Dict, 'estimated_sum': int}`\n",
    "\n",
    "> **RequiredSkillsAgent:** Maps technical and domain skills to tasks including programming languages (javascript, python), frameworks (react, django), tools (database_design, api_development), and soft skills. Optimized for batch processing of all tasks in single API call. Key method: `identify_skills(user_story, tasks) -> Dict[str, List[str]]`\n",
    "\n",
    "> **DependencyAgent:** Identifies inter-task dependencies using self-consistency approach across 3 samples. Assigns rework effort scoring (1=low, 2=moderate, 3=high rework required). Key method: `analyze_dependencies(user_story, tasks, story_points) -> Dict` returns dependency mappings with prerequisite tasks and effort estimates.\n",
    "\n",
    "> **TaskMergerAgent:** - performs semantic similarity analysis using TF-IDF vectorization (ngram_range=(1,3), max_features=1500) and cosine similarity with configurable threshold (default 0.4). Clusters similar tasks across user stories and merges them intelligently. Key method: `merge_similar_tasks(all_results) -> List[Dict]`\n",
    "\n",
    "> **SimpleGraphVisualizer:** Creates interactive dependency graphs from JSON results using NetworkX and Plotly. Displays task relationships, story points (node size), rework effort (edge annotations), and multi-story task sources. Key method: `create_dependency_graph(pipeline_results) -> plotly.Figure`\n",
    "\n",
    "> **FormatValidator:** Validates output structure ensuring required fields (description, id, user_stories, story_points, depends_on, required_skills) and JSON serialization. Provides error handling and fallback mechanisms. Key method: `validate_and_format(user_story, tasks_data, total_story_points) -> Dict`\n",
    "\n",
    "#### Important Methods\n",
    "\n",
    "> **process_multiple_user_stories_pipeline(user_stories: List[str]):** Main multi-story pipeline orchestrator executing 6-step workflow: (1) Individual story processing, (2) Task extraction with self-consistency, (3) Parallel story points + skills analysis using asyncio.gather(), (4) Dependency analysis, (5) Format validation, (6) Semantic task merging across all stories.\n",
    "\n",
    "> **process_user_story_pipeline(user_story: str):** Single story pipeline executing sequential workflow with parallel processing in steps 2-3 for story points and skills identification, followed by dependency analysis and format validation.\n",
    "\n",
    "> **decompose(user_story: str, num_samples=3):** Extracts actionable tasks using self-consistency approach with multiple LLM samples at varying temperatures. Applies task cleaning, normalization, and consistency voting. Returns maximum 8 imperative-form tasks with parsing resilience for malformed responses.\n",
    "\n",
    "> **merge_similar_tasks(all_results: List[Dict]):** Performs semantic task merging using TF-IDF similarity matrix calculation, task clustering with configurable threshold, and intelligent merging preserving skills, story points, and dependencies. Updates task IDs and rebuilds dependency references across merged tasks.\n",
    "\n",
    "> **estimate_story_points(user_story, tasks):** Uses few-shot learning with domain-specific examples to estimate complexity. Returns `{'total_story_points': int, 'task_points': Dict, 'estimated_sum': int}` using Fibonacci sequence with automatic correction for invalid values and fallback to moderate complexity (3 points).\n",
    "\n",
    "> **identify_skills(user_story, tasks):** Maps technical skills per task in single API call for efficiency. Returns `{'task_description': ['skill1', 'skill2']}` covering programming languages, frameworks, domain expertise, and technical disciplines with fallback to [\"general_development\"].\n",
    "\n",
    "> **analyze_dependencies(user_story, tasks, story_points):** Identifies logical task dependencies using self-consistency across multiple analyses. Returns `{'dependent_task': [{'task_id': 'prerequisite_task', 'rework_effort': 1-3}]}` based on workflow order and technical requirements with consensus voting.\n",
    "\n",
    "> **create_dependency_graph(pipeline_results):** Generates interactive Plotly visualization from JSON results showing task nodes (sized by story points), dependency edges with rework effort indicators, and hover details including merged task sources and required skills.##\n",
    "\n",
    "\n",
    "#### Semantic Similarity and Cross-Story Dependencies\n",
    "\n",
    "**Why Merge Tasks Across User Stories?**\n",
    "\n",
    "**Problem**: Dependencies exist not just within individual user stories, but **across the entire project scope**:\n",
    "* Story A: \"As a user, I want to create an account...\"\n",
    "* Story B: \"As a user, I want to log in to my account...\"\n",
    "* Story C: \"As a user, I want to reset my password...\"\n",
    "\n",
    "**Cross-Story Dependency Chain:** Story A: Create Account â†’ Story B: Login System â†’ Story C: Password Reset\n",
    "\n",
    "**Extended Problem**: Different user stories often describe the same underlying work:\n",
    "* \"As a user, I want to log in...\"\n",
    "* \"As an admin, I want to authenticate...\"\n",
    "* \"As a customer, I want to sign in...\"\n",
    "\n",
    "All require the same **authentication system**.\n",
    "\n",
    "+ **How It Works**\n",
    "\n",
    "1. **Semantic Analysis**: Uses TF-IDF vectorization to convert task descriptions to numerical vectors\n",
    "2. **Similarity Calculation**: Computes cosine similarity between all task pairs\n",
    "3. **Clustering**: Groups tasks above similarity threshold (0.4)\n",
    "4. **Intelligent Merging**:\n",
    "  * Chooses best description from cluster\n",
    "  * Combines all required skills\n",
    "  * Takes maximum story points\n",
    "  * Preserves all source user stories\n",
    "  * Updates all dependencies\n",
    "\n",
    "+ **Benefits**\n",
    "\n",
    "1. **Dependency Clarity**: Shows true task relationships across stories\n",
    "2. **Eliminates Redundant Work**: No duplicate implementation efforts\n",
    "3. **Resource Optimization**: Teams work on unified tasks\n",
    "4. **Consistency**: Single implementation serves multiple user stories\n",
    "5. **Better Planning**: Clear view of actual work required\n",
    "\n",
    "+ **Technical Implementation Details**\n",
    "\n",
    "> **TF-IDF Vectorization Configuration:**\n",
    "> - `ngram_range=(1, 3)`: Captures unigrams, bigrams, and trigrams for phrase-level similarity\n",
    "> - `max_features=1500`: Rich feature space for nuanced similarity detection\n",
    "> - `stop_words='english'`: Removes common words to focus on meaningful terms\n",
    "> - `lowercase=True`: Normalizes text for consistent comparison\n",
    "\n",
    "> **Similarity Threshold Selection:**\n",
    "> - Default: 0.4 (configurable) - balances precision vs. recall\n",
    "> - Lower threshold (0.3): More aggressive merging, risk of false positives\n",
    "> - Higher threshold (0.6): Conservative merging, may miss similar tasks\n",
    "> - Self-tuning based on domain vocabulary and task complexity\n",
    "\n",
    "> **Clustering Algorithm:**\n",
    "> - **Step 1**: Build similarity matrix for all task pairs across stories\n",
    "> - **Step 2**: Identify connected components above threshold\n",
    "> - **Step 3**: Group tasks into clusters using transitive relationships\n",
    "> - **Step 4**: Handle edge cases (single-task clusters, conflicting dependencies)\n",
    "\n",
    "> **Merging Strategy:**\n",
    "> - **Description Selection**: Chooses most comprehensive task description by length and clarity\n",
    "> - **Skill Aggregation**: Union of all required skills from cluster members\n",
    "> - **Story Points**: Maximum points from cluster (assumes most complex scenario)\n",
    "> - **Dependency Resolution**: Maps old task IDs to new merged task IDs throughout dependency graph\n",
    "\n",
    "#### Visualization System\n",
    "\n",
    "+ **SimpleGraphVisualizer Architecture**\n",
    "\n",
    "> **NetworkX Integration:** Creates directed graph (DiGraph) from JSON task results, handles node positioning with spring layout algorithm, manages edge routing for dependency relationships, and provides graph analysis capabilities (shortest paths, cycles, centrality measures).\n",
    "\n",
    "> **Plotly Interactive Features:** Generates web-based interactive visualizations with zoom/pan capabilities, hover tooltips showing detailed task information, clickable elements for task exploration, and responsive design adapting to different screen sizes.\n",
    "\n",
    "+ **Visual Elements and Encoding**\n",
    "\n",
    "> **Node Representation:**\n",
    "> - **Size**: Proportional to story points (`size = 20 + story_points * 3`, capped at 50px)\n",
    "> - **Color**: Light blue (`lightblue`) with white borders for visual clarity\n",
    "> - **Labels**: Task IDs (T_001, T_002) displayed in center with Arial 8pt font\n",
    "> - **Position**: Spring layout algorithm for optimal spacing and readability\n",
    "\n",
    "> **Edge Representation:**\n",
    "> - **Arrows**: Gray lines with triangular arrowheads showing dependency direction\n",
    "> - **Thickness**: 2px width for clear visibility without overwhelming nodes\n",
    "> - **Style**: Solid lines with proper offset from node boundaries to avoid overlap\n",
    "> - **Direction**: Points from prerequisite task to dependent task\n",
    "\n",
    "> **Dependency Indicators:**\n",
    "> - **Orange Circles**: Midpoint markers on dependency edges showing rework effort\n",
    "> - **Size**: 12px diameter with white borders and 80% opacity\n",
    "> - **Interactive**: Hover reveals detailed dependency information\n",
    "> - **Color Coding**: Orange for moderate effort, potential for red/green based on effort level\n",
    "\n",
    "+ **Hover Information System**\n",
    "\n",
    "> **Task Node Hovers:**\n",
    "> ```html\n",
    "> Task: [Full task description]\n",
    "> Story Points: [Numerical value]\n",
    "> User Stories: [Semicolon-separated list of source stories]\n",
    "> Skills: [Comma-separated required skills list]\n",
    "> ```\n",
    "\n",
    "> **Dependency Edge Hovers:**\n",
    "> ```html\n",
    "> Dependency: [Prerequisite Task] â†’ [Dependent Task]\n",
    "> Rework Effort: [1-3 scale with interpretation]\n",
    "> ```\n",
    "\n",
    "+ **Layout Algorithm Details**\n",
    "\n",
    "> **Spring Layout Configuration:**\n",
    "> - `k=3`: Optimal distance between connected nodes for readability\n",
    "> - `iterations=50`: Sufficient iterations for stable positioning\n",
    "> - `seed=42`: Reproducible layouts for consistent visualization\n",
    "> - **Fallback**: Circular layout if spring layout fails (disconnected components)\n",
    "\n",
    "> **Position Optimization:**\n",
    "> - **Node Spacing**: Minimum distance prevents overlap and crowding\n",
    "> - **Edge Routing**: Calculates proper start/end points with boundary offsets\n",
    "> - **Arrowhead Geometry**: Mathematical calculation for proper arrow direction and size\n",
    "> - **Label Positioning**: Centers text within nodes with collision detection\n",
    "\n",
    "+ **Interactive Features**\n",
    "\n",
    "> **Zoom and Pan:**\n",
    "> - Mouse wheel zooming with center-point focus\n",
    "> - Click-and-drag panning for large graphs\n",
    "> - Automatic zoom-to-fit for initial view\n",
    "> - Reset zoom functionality\n",
    "\n",
    "> **Selection and Highlighting:**\n",
    "> - Click nodes to highlight connected dependencies\n",
    "> - Hover effects with visual feedback\n",
    "> - Multi-select capabilities for task comparison\n",
    "> - Search functionality for large task sets\n",
    "\n",
    "> **Export Capabilities:**\n",
    "> - PNG/SVG export for documentation\n",
    "> - HTML export for sharing interactive versions (`dependency_graph.html`)\n",
    "> - JSON export of graph data for further analysis\n",
    "> - Print-friendly layouts with optimized sizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086e6c6-1eed-4636-848c-2fe4482bc15d",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0276b3-2949-4329-bae4-ed26eece56fe",
   "metadata": {},
   "source": [
    "In addition to prompt engineering, this pipeline implements **Parameter-Efficient Fine-Tuning (PEFT)** using **LoRA (Low-Rank Adaptation)** for domain-specific task decomposition optimization.\n",
    "\n",
    "#### Fine-Tuning Types Comparison\n",
    "\n",
    "| Fine-Tuning Type | Parameters Updated | Memory Usage | Training Time | Performance | Use Case |\n",
    "|------------------|-------------------|--------------|---------------|-------------|----------|\n",
    "| **Full Fine-Tuning** | All model parameters (70B+ params) | Very High (>500GB) | Very Long (days/weeks) | Highest | Complete domain adaptation |\n",
    "| **Output Layer Only** | Final classification layer (~1M params) | Low (~2GB) | Short (hours) | Limited | Simple classification tasks |\n",
    "| **LoRA/PEFT** | Low-rank matrices (~0.1% of params) | Medium (~8GB) | Moderate (hours) | High | **Our Implementation** |\n",
    "\n",
    "#### LoRA (Low-Rank Adaptation) Implementation\n",
    "\n",
    "> **Core Concept:** Instead of updating all 70 billion parameters, LoRA adds small trainable matrices that approximate the weight updates through low-rank decomposition: `W = Wâ‚€ + BA`, where B and A are much smaller matrices.\n",
    "\n",
    "> **Mathematical Foundation:**\n",
    "> ```python\n",
    "> # Original weight matrix: W âˆˆ R^(dÃ—k) \n",
    "> # LoRA decomposition: Î”W = BA\n",
    "> # B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k) where r << min(d,k)\n",
    "> # Updated forward pass: h = Wâ‚€x + BAx\n",
    "> ```\n",
    "\n",
    "> **Parameter Efficiency:**\n",
    "> - **Original Model**: 70B parameters\n",
    "> - **LoRA Addition**: ~50M parameters (0.07% of original)\n",
    "> - **Memory Reduction**: 95% less GPU memory required\n",
    "> - **Training Speed**: 10x faster than full fine-tuning\n",
    "\n",
    "#### PEFT (Parameter-Efficient Fine-Tuning) Framework\n",
    "\n",
    "> **Integration Architecture:**\n",
    "> ```python\n",
    "> from peft import LoraConfig, get_peft_model, TaskType\n",
    "> \n",
    "> # LoRA configuration for task decomposition\n",
    "> lora_config = LoraConfig(\n",
    ">     task_type=TaskType.CAUSAL_LM,\n",
    ">     inference_mode=False,\n",
    ">     r=16,                    # Low-rank dimension\n",
    ">     lora_alpha=32,           # Scaling parameter\n",
    ">     lora_dropout=0.1,        # Regularization\n",
    ">     target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "> )\n",
    "> ```\n",
    "\n",
    "> **Target Modules Selection:**\n",
    "> - **Attention Layers**: Query, Key, Value, Output projections\n",
    "> - **Feed-Forward Networks**: Up/down projection layers\n",
    "> - **Layer Normalization**: Optional adaptation for domain shifts\n",
    "> - **Embedding Layers**: Vocabulary adaptation for specialized terms\n",
    "\n",
    "> **Training Process:**\n",
    "> 1. **Base Model Freezing**: Keep llama3-70b weights frozen\n",
    "> 2. **LoRA Injection**: Add trainable low-rank matrices\n",
    "> 3. **Task-Specific Training**: Fine-tune on user story decomposition data\n",
    "> 4. **Adapter Merging**: Combine LoRA weights with base model for inference\n",
    "\n",
    "#### Beam Search Implementation\n",
    "\n",
    "> **Algorithm Enhancement:** Beyond greedy decoding, the system implements beam search for generating multiple candidate task decompositions and selecting optimal combinations.\n",
    "\n",
    "> **Beam Search Configuration:**\n",
    "> ```python\n",
    "> beam_search_params = {\n",
    ">     \"num_beams\": 5,              # Parallel candidate sequences\n",
    ">     \"length_penalty\": 1.2,       # Prefer longer, detailed tasks\n",
    ">     \"diversity_penalty\": 0.5,    # Encourage diverse task types\n",
    ">     \"no_repeat_ngram_size\": 3,   # Avoid repetitive patterns\n",
    ">     \"early_stopping\": True       # Stop when EOS reached\n",
    "> }\n",
    "> ```\n",
    "\n",
    "> **Multi-Objective Optimization:**\n",
    "> - **Task Quality**: Semantic coherence and actionability\n",
    "> - **Coverage**: Complete user story requirement fulfillment\n",
    "> - **Granularity**: Appropriate task size and complexity\n",
    "> - **Dependencies**: Logical prerequisite relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49801c04-7833-4b4c-b050-7e5c5014674c",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "(see the code here \n",
    "https://colab.research.google.com/drive/1MwJPWFDELUK-mZ1Y3brDKgKz8t-wbhg_?usp=sharing )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f020-2b30-49dc-908e-433d91014771",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering vs Fine-Tuning Comparison\n",
    "\n",
    "### Performance Comparison Table\n",
    "\n",
    "| Metric | Prompt Engineering (Multi-Agent) | Fine-Tuning (LoRA + GPT-2) | Winner |\n",
    "|--------|----------------------------------|----------------------------|---------|\n",
    "| **Task Quality** | High - Actionable, technical tasks | Low - Abstract, vague tasks | ðŸ† **Prompt Engineering** |\n",
    "| **Task Specificity** | Excellent - \"Implement search algorithm\", \"Create API endpoints\" | Poor - \"Create a list of items\", \"Identify important items\" | ðŸ† **Prompt Engineering** |\n",
    "| **Dependency Accuracy** | High - Logical prerequisite relationships | Low - Unrealistic dependencies | ðŸ† **Prompt Engineering** |\n",
    "| **Story Point Estimation** | Accurate - Fibonacci scale with reasoning | Generic - Default values (mostly 2) | ðŸ† **Prompt Engineering** |\n",
    "| **Skills Identification** | Comprehensive - Specific tech stack | Basic - Domain categories only | ðŸ† **Prompt Engineering** |\n",
    "| **Output Consistency** | Moderate - Varies across runs | High - Structured format always maintained | ðŸ† **Fine-Tuning** |\n",
    "| **Processing Speed** | Slow - Multiple API calls | Fast - Single inference pass | ðŸ† **Fine-Tuning** |\n",
    "| **Cost Efficiency** | High cost   |  Low cost  | ðŸ† **Fine-Tuning** |\n",
    "| **Scalability** | Limited - API rate limits | Excellent - Local inference | ðŸ† **Fine-Tuning** |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
